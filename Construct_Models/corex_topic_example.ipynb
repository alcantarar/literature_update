{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchored CorEx: Topic Modeling with Minimal Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** [Ryan J. Gallagher](http://ryanjgallagher.github.io/)  \n",
    "\n",
    "**Last updated:** 07/21/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to use the CorEx topic model code. This includes fitting CorEx to your data, examining the topic model output, outputting results, building a hierarchical topic model, and anchoring words to topics.\n",
    "\n",
    "Details of the CorEx topic model and evaluations against unsupervised and semi-supervised variants of LDA can be found in our TACL paper:\n",
    "\n",
    "Gallagher, Ryan J., Kyle Reing, David Kale, and Greg Ver Steeg. \"[Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244).\" *Transactions of the Association for Computational Linguistics (TACL)*, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic# jupyter notebooks will complain matplotlib is being loaded twice\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first load data to run the CorEx topic model. We'll use the 20 Newsgroups dataset, which scikit-learn provides functionality to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Get 20 newsgroups data\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "#https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model assumes input is in the form of a doc-word matrix, where rows are documents and columns are binary counts. We'll vectorize the newsgroups data, take the top 20,000 words, and convert it to a sparse matrix to save on memory usage. Note, we use binary count vectors as input to the CorEx topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 20000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform 20 newsgroup data into a sparse matrix\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=20000, token_pattern='[a-zA-Z-0-9]{3,}', binary=True)\n",
    "doc_word = vectorizer.fit_transform(newsgroups.data)\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our doc-word matrix is 11,314 documents by 20,000 words. Let's get the words that label the columns. We'll need these for outputting readable topics and later for anchoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a final step of preprocessing where we remove all integers from our set of words. This brings is down to 19,038 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 19198)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_digit_inds = [ind for ind,word in enumerate(words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words    = [word for ind,word in enumerate(words) if not word.isdigit()]\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters of the CorEx topic model are:\n",
    "+ **`n_hidden`**: number of topics (\"hidden\" as in \"hidden latent topics\")\n",
    "+ **`words`**: words that label the columns of the doc-word matrix (optional)\n",
    "+ **`docs`**: document labels that label the rows of the doc-word matrix (optional)\n",
    "+ **`max_iter`**: number of iterations to run through the update equations (optional, defaults to 200)\n",
    "+ **`verbose`**:  if `verbose=1`, then CorEx will print the topic TCs with each iteration\n",
    "+ **`seed`**:     random number seed to use for model initialization (optional)\n",
    "\n",
    "We'll train a topic model with 50 topics. (This will take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 50 topics\n",
    "topic_model = ct.Corex(n_hidden=50, words=words, max_iter=50, verbose=False, seed=1)\n",
    "topic_model.fit(doc_word, words=words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CorEx topic model provides functionality for easily accessing the topics. Let's take a look one of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('team', 0.07676807681230795),\n",
       " ('game', 0.06386203097201437),\n",
       " ('season', 0.04679960507057238),\n",
       " ('players', 0.046109605195915386),\n",
       " ('league', 0.04512263966672076),\n",
       " ('play', 0.04188662887690003),\n",
       " ('games', 0.0399178327855832),\n",
       " ('hockey', 0.036703610214971476),\n",
       " ('teams', 0.03645873943647823),\n",
       " ('player', 0.03003272638618156)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_model.get_topics(topic=1, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic words are those with the highest *mutual information* with the topic, rather than those with highest probability within the topic as in LDA. The mutual information with the topic is the number reported in each tuple. Theoretically, mutual information is always positive. If the CorEx output returns a negative mutual information from **`get_topics()`**, then the absolute value of that quantity is the mutual information between the topic and the *absence* of that word.\n",
    "\n",
    "If the column labels have not been specified through **`words`**, then the code will return the column indices for the top words in each topic.\n",
    "\n",
    "We can also retrieve all of the topics at once if we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: archive-name,last-modified,contents,series,dave,jpl,sections,stanford,index,propulsion\n",
      "1: team,game,season,players,league,play,games,hockey,teams,player\n",
      "2: war,armenians,country,military,armenian,killed,turkish,policy,argic,today\n",
      "3: card,windows,disk,drive,software,ram,dos,video,memory,scsi\n",
      "4: god,jesus,christians,christian,bible,christ,religion,church,faith,religious\n",
      "5: people,fact,believe,person,example,saying,claim,reason,agree,mean\n",
      "6: world,history,book,subject,written,holy,books,paul,biblical,historical\n",
      "7: list,include,provides,technical,contains,net,management,misc,library,programming\n",
      "8: key,encryption,clipper,keys,secure,nsa,security,escrow,algorithm,chip\n",
      "9: public,state,states,national,united,president,political,clinton,federal,administration\n",
      "10: say,think,did,said,right,going,let,away,didn,day\n",
      "11: research,information,center,press,published,report,scientific,release,news,march\n",
      "12: gun,rights,police,crime,guns,weapons,court,legal,amendment,criminal\n",
      "13: don,just,like,know,good,really,better,want,thing,doesn\n",
      "14: new,order,sent,present,received,major,return,future,letter,entire\n",
      "15: point,life,situation,understand,sense,known,certainly,view,society,consider\n",
      "16: general,clear,purpose,involved,provide,american,force,necessary,science,action\n",
      "17: car,bike,cars,engine,miles,power,ride,riding,ground,radio\n",
      "18: april,washington,los,york,angeles,city,san,nasa,conference,division\n",
      "19: long,days,left,started,night,took,road,near,close,home\n",
      "20: disease,medical,food,patients,treatment,medicine,hospital,blood,doctors,health\n",
      "21: ftp,files,file,pub,directory,available,anonymous,image,site,format\n",
      "22: including,issue,groups,possible,various,issues,process,specific,related,study\n",
      "23: space,technology,development,department,developed,design,commercial,flight,project,build\n",
      "24: problem,set,help,following,problems,trying,error,message,tried,function\n",
      "25: special,1st,2nd,june,3rd,dec,canada,usa,berkeley,title\n",
      "26: use,using,data,systems,code,user,users,access,uses,applications\n",
      "27: high,low,second,launch,rate,field,higher,range,energy,results\n",
      "28: time,things,different,read,does,times,having,based,called,change\n",
      "29: israel,jews,israeli,jewish,muslim,arab,soldiers,muslims,arabs,peace\n",
      "30: government,law,laws,man,citizens,enforcement,live,act,acts,actions\n",
      "31: year,years,great,total,hit,defense,period,numbers,short,chance\n",
      "32: end,local,requires,allow,ability,break,instead,limited,required,containing\n",
      "33: men,women,place,given,later,woman,taken,dead,young,gave\n",
      "34: number,note,current,open,addition,posted,needed,lines,line,volume\n",
      "35: come,far,important,means,feel,simply,quite,actually,common,unless\n",
      "36: john,independent,held,authors,according,complete,david,accepted,earlier,began\n",
      "37: thanks,advance,program,window,unix,application,graphics,appreciated,server,display\n",
      "38: internet,university,office,international,phone,organization,date,com,fax,org\n",
      "39: large,small,water,parts,usually,designed,areas,additional,placed,heat\n",
      "40: make,real,probably,course,case,making,money,likely,aren,control\n",
      "41: gordon,geb,banks,shameful,chastity,n3jxp,surrender,dsl,skepticism,intellect\n",
      "42: strong,protect,white,prevent,record,americans,carry,kept,legitimate,impossible\n",
      "43: send,edu,computer,address,mail,includes,digital,standard,remote,tape\n",
      "44: sale,price,sell,offer,shipping,condition,buy,interested,asking,prices\n",
      "45: head,face,told,came,turn,doctor,turned,fall,friends,wife\n",
      "46: form,free,position,entirely,leave,applied,percent,explained,permit,intent\n",
      "47: way,questions,answer,ask,question,try,asked,doing,answers,easy\n",
      "48: used,work,old,cost,check,features,replaced,month,older,traffic\n",
      "49: hard,included,original,internal,wide,recently,items,model,transfer,purchase\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic for the newsgroup data tends to be less coherent than expected because of encodings and other oddities in the newsgroups data.  \n",
    "\n",
    "We can also get the column indices instead of the column labels if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12630, 0.1018243939182525),\n",
       " (6589, 0.06468537631732436),\n",
       " (2186, 0.04942563312308274),\n",
       " (12701, 0.040692677481064206),\n",
       " (6350, 0.040426392469674534),\n",
       " (15076, 0.03923854079647752),\n",
       " (3411, 0.03744065192205067),\n",
       " (14028, 0.03679205555091967),\n",
       " (1026, 0.03626135849728878),\n",
       " (10717, 0.03624100527056468)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topics(topic=5, n_words=10, print_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to directly access the topic assignments for each word, they can be accessed through **`cluster`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 48 43 ... 17 17 43]\n",
      "(19198,)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.clusters)\n",
    "print(topic_model.clusters.shape) # m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the topic words, the most probable documents per topic can also be easily accessed. Documents are sorted according to log probabilities which is why the highest probability documents have a score of 0 ($e^0 = 1$) and other documents have negative scores (for example, $e^{-0.5} \\approx 0.6$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10847, 0.0),\n",
       " (9219, 0.0),\n",
       " (6610, 0.0),\n",
       " (9192, 0.0),\n",
       " (3438, 0.0),\n",
       " (9120, 0.0),\n",
       " (144, 0.0),\n",
       " (3410, 0.0),\n",
       " (6664, 0.0),\n",
       " (3380, 0.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_model.get_top_docs(topic=0, n_docs=10, sort_by='log_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorEx is a *discriminative* model, whereas LDA is a *generative* model. This means that while LDA outputs a probability distribution over each document, CorEx instead estimates the probability a document belongs to a topic given that document's words. As a result, the probabilities across topics for a given document do not have to add up to 1. The estimated probabilities of topics for each document can be accessed through **`log_p_y_given_x`** or **`p_y_given_x`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 50)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.p_y_given_x.shape) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a softmax to make a binary determination of which documents belong to each topic. These softmax labels can be accessed through **`labels`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 50)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.labels.shape) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since CorEx does not prescribe a probability distribution of topics over each document, this means that a document could possibly belong to no topics (all 0's across topics in **`labels`**) or all topics (all 1's across topics in **`labels`**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Correlation and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total correlation is the measure which CorEx maximize when constructing the topic model. It can be accessed through **`tc`** and is reported in nats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.68834461501426"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection:** CorEx starts its algorithm with a random initialization, and so different runs can result in different topic models. One way of finding a better topic model is to restart the CorEx algorithm several times and take the run that has the highest TC value (i.e. the run that produces topics that are most informative about the documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall total correlation is the sum of the total correlation per each topic. These can be accessed through **`tcs`**. For an unsupervised CorEx topic model, the topics are always sorted from high to low according to their TC. For an anchored CorEx topic model, the topics are not sorted, and are outputted such that the anchored topics come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tcs.shape # k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.68834461501426\n",
      "41.68834461501426\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(topic_model.tcs))\n",
    "print(topic_model.tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting number of topics:** one way to choose the number of topics is to observe the distribution of TCs for each topic to see how much each additional topic contributes to the overall TC. We should keep adding topics until additional topics do not significantly contribute to the overall TC. This is similar to choosing a cutoff eigenvalue when doing topic modeling via LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the first topic is much more informative than the other topics. Given that we suspect that this topic is picking up on image encodings (as given by \"dsl\" and \"n3jxp\" in the topic) and other boilerplate text (as given by the high TC and lack of coherence of the rest of the topic), we could consider doing additional investigation and preprocessing to help ensure that the CorEx topic model does not pick up on these patterns which are not insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Document TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose total correlation further. The topic correlation is the average of the pointwise total correlations for each individual document. The pointwise total correlations can be accessed through **`log_z`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.log_z.shape # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.094 2.031 1.696 1.587 1.288 1.221 1.047 1.043 1.02  1.014 0.974 0.955 0.936 0.919 0.89  0.886 0.885 0.883 0.875 0.819 0.804 0.772 0.77  0.756 0.751 0.747 0.733 0.728 0.724 0.708 0.699 0.698 0.693\n",
      " 0.689 0.672 0.668 0.65  0.624 0.588 0.569 0.567 0.56  0.481 0.481 0.472 0.456 0.431 0.412 0.387 0.332]\n",
      "[2.094 2.031 1.696 1.587 1.288 1.221 1.047 1.043 1.02  1.014 0.974 0.955 0.936 0.919 0.89  0.886 0.885 0.883 0.875 0.819 0.804 0.772 0.77  0.756 0.751 0.747 0.733 0.728 0.724 0.708 0.699 0.698 0.693\n",
      " 0.689 0.672 0.668 0.65  0.624 0.588 0.569 0.567 0.56  0.481 0.481 0.472 0.456 0.431 0.412 0.387 0.332]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(topic_model.log_z, axis=0))\n",
    "print(topic_model.tcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pointwise total correlations in **`log_z`** represent the correlations within an individual document explained by a particular topic. These correlations have been used to measure how \"surprising\" documents are with respect to given topics (see references below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`labels`** attribute gives the binary topic expressions for each document and each topic. We can use this output as input to another CorEx topic model to get latent representations of the topics themselves. This yields a hierarchical CorEx topic model. Like the first layer of the topic model, one can determine the number of latent variables to add in higher layers through examination of the topic TCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n"
     ]
    }
   ],
   "source": [
    "# Train a second layer to the topic model\n",
    "tm_layer2 = ct.Corex(n_hidden=10)\n",
    "tm_layer2.fit(topic_model.labels);\n",
    "\n",
    "# Train a third layer to the topic model\n",
    "tm_layer3 = ct.Corex(n_hidden=1)\n",
    "tm_layer3.fit(tm_layer2.labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have `graphviz` installed, then you can output visualizations of the hierarchial topic model to your current working directory. One can also create custom visualizations of the hierarchy by properly making use of the **`labels`** attribute of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [options] data_file.csv \n",
      "Assume one document on each line.\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This module implements some visualizations based on CorEx representations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # to create visualizations on a display-less server\n",
    "import pylab\n",
    "import networkx as nx\n",
    "import textwrap\n",
    "import scipy.sparse as ss\n",
    "import sklearn.feature_extraction.text as skt\n",
    "#import cPickle, pickle # neither module is used, and cPickle is not part of Anaconda build, so commented for LF run\n",
    "import corextopic as ct\n",
    "import sys, traceback\n",
    "from time import time\n",
    "import re\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from nltk.stem.snowball import *\n",
    "pattern = '\\\\b[A-Za-z]+\\\\b'\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "class Corex(object):\n",
    "    def vis_rep(corex, data=None, row_label=None, column_label=None, prefix='topics'):\n",
    "        \"\"\"Various visualizations and summary statistics for a one layer representation\"\"\"\n",
    "        if column_label is None:\n",
    "            column_label = list(map(str, range(data.shape[1])))\n",
    "        if row_label is None:\n",
    "            row_label = list(map(str, range(corex.n_samples)))\n",
    "\n",
    "        alpha = corex.alpha\n",
    "\n",
    "        print('Print topics in text file')\n",
    "        output_groups(corex.tcs, alpha, corex.mis, column_label, corex.sign, prefix=prefix)\n",
    "        output_labels(corex.labels, row_label, prefix=prefix)\n",
    "        output_cont_labels(corex.p_y_given_x, row_label, prefix=prefix)\n",
    "        output_strong(corex.tcs, alpha, corex.mis, corex.labels, prefix=prefix)\n",
    "        anomalies(corex.log_z, row_label=row_label, prefix=prefix)\n",
    "        plot_convergence(corex.tc_history, prefix=prefix)\n",
    "        if data is not None:\n",
    "            plot_heatmaps(data, alpha, corex.mis, column_label, corex.p_y_given_x, prefix=prefix)\n",
    "\n",
    "\n",
    "    def vis_hierarchy(corexes, column_label=None, max_edges=100, prefix='topics', n_anchors=0):\n",
    "        \"\"\"Visualize a hierarchy of representations.\"\"\"\n",
    "        if column_label is None:\n",
    "            column_label = list(map(str, range(corexes[0].alpha.shape[1])))\n",
    "\n",
    "        # make l1 label\n",
    "        alpha = corexes[0].alpha\n",
    "        mis = corexes[0].mis\n",
    "        l1_labels = []\n",
    "        annotate = lambda q, s: q if s > 0 else '~' + q\n",
    "        for j in range(corexes[0].n_hidden):\n",
    "            # inds = np.where(alpha[j] * mis[j] > 0)[0]\n",
    "            inds = np.where(alpha[j] >= 1.)[0]\n",
    "            inds = inds[np.argsort(-alpha[j, inds] * mis[j, inds])]\n",
    "            group_number = str('red_') + str(j) if j < n_anchors else str(j)\n",
    "            label = group_number + ':' + ' '.join([annotate(column_label[ind], corexes[0].sign[j,ind]) for ind in inds[:6]])\n",
    "            label = textwrap.fill(label, width=25)\n",
    "            l1_labels.append(label)\n",
    "\n",
    "        # Construct non-tree graph\n",
    "        weights = [corex.alpha.clip(0, 1) * corex.mis for corex in corexes[1:]]\n",
    "        node_weights = [corex.tcs for corex in corexes[1:]]\n",
    "        g = make_graph(weights, node_weights, l1_labels, max_edges=max_edges)\n",
    "\n",
    "        # Display pruned version\n",
    "        h = g.copy()  # trim(g.copy(), max_parents=max_parents, max_children=max_children)\n",
    "        edge2pdf(h, prefix + '/graphs/graph_prune_' + str(max_edges), labels='label', directed=True, makepdf=True)\n",
    "\n",
    "        # Display tree version\n",
    "        tree = g.copy()\n",
    "        tree = trim(tree, max_parents=1, max_children=False)\n",
    "        edge2pdf(tree, prefix + '/graphs/tree', labels='label', directed=True, makepdf=True)\n",
    "\n",
    "        # Output JSON files\n",
    "        try:\n",
    "            import os\n",
    "            #copyfile(os.path.dirname(os.path.realpath(__file__)) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "            print(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "            copyfile(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "        except:\n",
    "            print(\"Couldn't find 'force.html' file for visualizing d3 output\")\n",
    "        import json\n",
    "        from networkx.readwrite import json_graph\n",
    "\n",
    "        mapping = dict([(n, tree.node[n].get('label', str(n))) for n in tree.nodes()])\n",
    "        tree = nx.relabel_nodes(tree, mapping)\n",
    "        json.dump(json_graph.node_link_data(tree), safe_open(prefix + '/graphs/force.json', 'w+'))\n",
    "        json.dump(json_graph.node_link_data(h), safe_open(prefix + '/graphs/force_nontree.json', 'w+'))\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "    def plot_heatmaps(data, alpha, mis, column_label, cont, topk=40, athresh=0.2, prefix=''):\n",
    "        import seaborn as sns\n",
    "        cmap = sns.cubehelix_palette(as_cmap=True, light=.9)\n",
    "        import matplotlib.pyplot as plt\n",
    "        m, nv = mis.shape\n",
    "        for j in range(m):\n",
    "            inds = np.where(np.logical_and(alpha[j] > athresh, mis[j] > 0.))[0]\n",
    "            inds = inds[np.argsort(- alpha[j, inds] * mis[j, inds])][:topk]\n",
    "            if len(inds) >= 2:\n",
    "                plt.clf()\n",
    "                order = np.argsort(cont[:,j])\n",
    "                if type(data) == np.ndarray:\n",
    "                    subdata = data[:, inds][order].T\n",
    "                else:\n",
    "                    # assume sparse\n",
    "                    subdata = data[:, inds].toarray()\n",
    "                    subdata = subdata[order].T\n",
    "                columns = [column_label[i] for i in inds]\n",
    "                fig, ax = plt.subplots(figsize=(20, 10))\n",
    "                sns.heatmap(subdata, vmin=0, vmax=1, cmap=cmap, yticklabels=columns, xticklabels=False, ax=ax, cbar_kws={\"ticks\": [0, 0.5, 1]})\n",
    "                plt.yticks(rotation=0)\n",
    "                filename = '{}/heatmaps/group_num={}.png'.format(prefix, j)\n",
    "                if not os.path.exists(os.path.dirname(filename)):\n",
    "                    os.makedirs(os.path.dirname(filename))\n",
    "                plt.title(\"Latent factor {}\".format(j))\n",
    "                plt.savefig(filename, bbox_inches='tight')\n",
    "                plt.close('all')\n",
    "                #plot_rels(data[:, inds], map(lambda q: column_label[q], inds), colors=cont[:, j],\n",
    "                #          outfile=prefix + '/relationships/group_num=' + str(j), latent=labels[:, j], alpha=0.1)\n",
    "\n",
    "\n",
    "    def make_graph(weights, node_weights, column_label, max_edges=100):\n",
    "        all_edges = np.hstack(list(map(np.ravel, weights)))\n",
    "        max_edges = min(max_edges, len(all_edges))\n",
    "        w_thresh = np.sort(all_edges)[-max_edges]\n",
    "        print('weight threshold is %f for graph with max of %f edges ' % (w_thresh, max_edges))\n",
    "        g = nx.DiGraph()\n",
    "        max_node_weight = max([max(w) for w in node_weights])\n",
    "        for layer, weight in enumerate(weights):\n",
    "            m, n = weight.shape\n",
    "            for j in range(m):\n",
    "                g.add_node((layer + 1, j))\n",
    "                g.node[(layer + 1, j)]['weight'] = 0.3 * node_weights[layer][j] / max_node_weight\n",
    "                for i in range(n):\n",
    "                    if weight[j, i] > w_thresh:\n",
    "                        if weight[j, i] > w_thresh / 2:\n",
    "                            g.add_weighted_edges_from([( (layer, i), (layer + 1, j), 10 * weight[j, i])])\n",
    "                        else:\n",
    "                            g.add_weighted_edges_from([( (layer, i), (layer + 1, j), 0)])\n",
    "\n",
    "        # Label layer 0\n",
    "        for i, lab in enumerate(column_label):\n",
    "            g.add_node((0, i))\n",
    "            g.node[(0, i)]['label'] = lab\n",
    "            g.node[(0, i)]['name'] = lab  # JSON uses this field\n",
    "            g.node[(0, i)]['weight'] = 1\n",
    "        return g\n",
    "\n",
    "\n",
    "    def trim(g, max_parents=False, max_children=False):\n",
    "        for node in g:\n",
    "            if max_parents:\n",
    "                parents = list(g.successors(node))\n",
    "                #weights = [g.edge[node][parent]['weight'] for parent in parents]\n",
    "                weights = [g.adj[node][parent]['weight'] for parent in parents]\n",
    "                for weak_parent in np.argsort(weights)[:-max_parents]:\n",
    "                    g.remove_edge(node, parents[weak_parent])\n",
    "            if max_children:\n",
    "                children = g.predecessors(node)\n",
    "                weights = [g.edge[child][node]['weight'] for child in children]\n",
    "                for weak_child in np.argsort(weights)[:-max_children]:\n",
    "                    g.remove_edge(children[weak_child], node)\n",
    "        return g\n",
    "\n",
    "\n",
    "    def output_groups(tcs, alpha, mis, column_label, direction, thresh=0, prefix=''):\n",
    "        f = safe_open(prefix + '/groups.txt', 'w+')\n",
    "        h = safe_open(prefix + '/topics.txt', 'w+')\n",
    "        m, nv = mis.shape\n",
    "        annotate = lambda q, s: q if s >= 0 else '~' + q\n",
    "        for j in range(m):\n",
    "            f.write('Group num: %d, TC(X;Y_j): %0.3f\\n' % (j, tcs[j]))\n",
    "            # inds = np.where(alpha[j] * mis[j] > thresh)[0]\n",
    "            inds = np.where(alpha[j] >= 1.)[0]\n",
    "            inds = inds[np.argsort(-alpha[j, inds] * mis[j, inds])]\n",
    "            for ind in inds:\n",
    "                f.write(column_label[ind] + u', %0.3f, %0.3f, %0.3f\\n' % (\n",
    "                    mis[j, ind], alpha[j, ind], mis[j, ind] * alpha[j, ind]))\n",
    "            #h.write(unicode(j) + u':' + u','.join([annotate(column_label[ind], direction[j,ind]) for ind in inds[:10]]) + u'\\n')\n",
    "            h.write(str(j) + u':' + u','.join(\n",
    "                [annotate(column_label[ind], direction[j, ind]) for ind in inds[:10]]) + u'\\n')\n",
    "        f.close()\n",
    "        h.close()\n",
    "\n",
    "\n",
    "    def output_labels(labels, row_label, prefix=''):\n",
    "        f = safe_open(prefix + '/labels.txt', 'w+')\n",
    "        ns, m = labels.shape\n",
    "        for l in range(ns):\n",
    "            f.write(row_label[l] + ',' + ','.join(list(map(lambda q: '%d' % q, labels[l, :])))+ '\\n')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def output_cont_labels(p_y_given_x, row_label, prefix=''):\n",
    "        f = safe_open(prefix + '/cont_labels.txt', 'w+')\n",
    "        ns, m = p_y_given_x.shape\n",
    "        for l in range(ns):\n",
    "            f.write(row_label[l] + ',' + ','.join(list(map(lambda q: '{:.10f}'.format(q), np.log(p_y_given_x[l, :])))) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def output_strong(tcs, alpha, mis, labels, prefix=''):\n",
    "        f = safe_open(prefix + '/most_deterministic_groups.txt', 'w+')\n",
    "        m, n = alpha.shape\n",
    "        topk = 5\n",
    "        ixy = np.clip(np.sum(alpha * mis, axis=1) - tcs, 0, np.inf)\n",
    "        hys = np.array([entropy(labels[:, j]) for j in range(m)]).clip(1e-6)\n",
    "        ntcs = [(np.sum(np.sort(alpha[j] * mis[j])[-topk:]) - ixy[j]) / ((topk - 1) * hys[j]) for j in range(m)]\n",
    "\n",
    "        f.write('Group num., NTC\\n')\n",
    "        for j, ntc in sorted(enumerate(ntcs), key=lambda q: -q[1]):\n",
    "            f.write('%d, %0.3f\\n' % (j, ntc))\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def anomalies(log_z, row_label=None, prefix=''):\n",
    "        from scipy.special import erf\n",
    "\n",
    "        ns = log_z.shape[0]\n",
    "        if row_label is None:\n",
    "            row_label = list(map(str, range(ns)))\n",
    "        a_score = np.sum(log_z[:, :], axis=1)\n",
    "        mean, std = np.mean(a_score), np.std(a_score)\n",
    "        a_score = (a_score - mean) / std\n",
    "        percentile = 1. / ns\n",
    "        anomalies = np.where(0.5 * (1 - erf(a_score / np.sqrt(2)) ) < percentile)[0]\n",
    "        f = safe_open(prefix + '/anomalies.txt', 'w+')\n",
    "        for i in anomalies:\n",
    "            f.write(row_label[i] + ', %0.1f\\n' % a_score[i])\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    # Utilities\n",
    "    # IT UTILITIES\n",
    "    def entropy(xsamples):\n",
    "        # sample entropy for one discrete var\n",
    "        xsamples = np.asarray(xsamples)\n",
    "        xsamples = xsamples[xsamples >= 0]  # by def, -1 means missing value\n",
    "        xs = np.unique(xsamples)\n",
    "        ns = len(xsamples)\n",
    "        ps = np.array([float(np.count_nonzero(xsamples == x)) / ns for x in xs])\n",
    "        return -np.sum(ps * np.log(ps))\n",
    "\n",
    "\n",
    "    def safe_open(filename, mode):\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        return codecs.open(filename, mode, \"utf-8\")\n",
    "\n",
    "\n",
    "    # Visualization utilities\n",
    "\n",
    "    def neato(fname, position=None, directed=False):\n",
    "        if directed:\n",
    "            os.system(\n",
    "                \"sfdp \" + fname + \".dot -Tpdf -Earrowhead=none -Nfontsize=16  -GK=2 -Gmaxiter=1000 -Goverlap=False -Gpack=True -Gpackmode=clust -Gsep=0.01 -Gsplines=False -o \" + fname + \"_sfdp.pdf\")\n",
    "            os.system(\n",
    "                \"sfdp \" + fname + \".dot -Tpdf -Earrowhead=none -Nfontsize=16  -GK=2 -Gmaxiter=1000 -Goverlap=False -Gpack=True -Gpackmode=clust -Gsep=0.01 -Gsplines=True -o \" + fname + \"_sfdp_w_splines.pdf\")\n",
    "            return True\n",
    "        if position is None:\n",
    "            os.system(\"neato \" + fname + \".dot -Tpdf -o \" + fname + \".pdf\")\n",
    "            os.system(\"fdp \" + fname + \".dot -Tpdf -o \" + fname + \"fdp.pdf\")\n",
    "        else:\n",
    "            os.system(\"neato \" + fname + \".dot -Tpdf -n -o \" + fname + \".pdf\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def extract_color(label):\n",
    "        import matplotlib\n",
    "\n",
    "        colors = matplotlib.colors.cnames.keys()\n",
    "        parts = label.split('_')\n",
    "        for part in parts:\n",
    "            if part in colors:\n",
    "                parts.remove(part)\n",
    "                return '_'.join(parts), part\n",
    "        return label, 'black'\n",
    "\n",
    "\n",
    "    def edge2pdf(g, filename, threshold=0, position=None, labels=None, connected=True, directed=False, makepdf=True):\n",
    "        #This function will takes list of edges and a filename\n",
    "        #and write a file in .dot format. Readable, eg. by omnigraffle\n",
    "        # OR use \"neato file.dot -Tpng -n -o file.png\"\n",
    "        # The -n option says whether to use included node positions or to generate new ones\n",
    "        # for a grid, positions = [(i%28,i/28) for i in range(784)]\n",
    "        def cnn(node):\n",
    "            #change node names for dot format\n",
    "            if type(node) is tuple or type(node) is list:\n",
    "                #return u'n' + u'_'.join(list(map(unicode, node)))\n",
    "                return u'n' + u'_'.join(list(map(str, node)))\n",
    "            else:\n",
    "                return node\n",
    "\n",
    "        if connected:\n",
    "            touching = list(set(sum([[a, b] for a, b in g.edges()], [])))\n",
    "            g = nx.subgraph(g, touching)\n",
    "            print('non-isolated nodes,edges', len(list(g.nodes())), len(list(g.edges())))\n",
    "        f = safe_open(filename + '.dot', 'w+')\n",
    "        #print('f1->',f)\n",
    "        #print('directed->',directed)\n",
    "        if directed:\n",
    "            f.write(\"strict digraph {\\n\")#.decode(\"utf-8\")\n",
    "            #f.write('strict digraph {')\n",
    "        else:\n",
    "            f.write(\"strict graph {\")\n",
    "        #f.write(\"\\tgraph [overlap=scale];\\n\".encode('utf-8'))\n",
    "        f.write(\"\\tnode [shape=point];\\n\")\n",
    "        for a, b, d in g.edges(data=True):\n",
    "            if 'weight' in d:\n",
    "                if directed:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -> ' + cnn(b) + ' [penwidth=%.2f' % float(\n",
    "                        np.clip(d['weight'], 0, 9)) + '];\\n'))\n",
    "                else:\n",
    "                    if d['weight'] > threshold:\n",
    "                        f.write((\"\\t\" + cnn(a) + ' -- ' + cnn(b) + ' [penwidth=' + str(3 * d['weight']) + '];\\n'))\n",
    "            else:\n",
    "                if directed:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -> ' + cnn(b) + ';\\n'))\n",
    "                else:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -- ' + cnn(b) + ';\\n'))\n",
    "        for n in g.nodes():\n",
    "            if labels is not None:\n",
    "                if type(labels) == dict or type(labels) == list:\n",
    "                    thislabel = labels[n].replace(u'\"', u'\\\\\"')\n",
    "                    lstring = u'label=\"' + thislabel + u'\",shape=none'\n",
    "                elif type(labels) == str:\n",
    "                    #if g.node[n].has_key('label'):\n",
    "                    if 'label' in g.node[n]:\n",
    "                        thislabel = g.node[n][labels].replace(u'\"', u'\\\\\"')\n",
    "                        # combine dupes\n",
    "                        #llist = thislabel.split(',')\n",
    "                        #thislabel = ','.join([l for l in set(llist)])\n",
    "                        thislabel, thiscolor = extract_color(thislabel)\n",
    "                        lstring = u'label=\"%s\",shape=none,fontcolor=\"%s\"' % (thislabel, thiscolor)\n",
    "                    else:\n",
    "                        weight = g.node[n].get('weight', 0.1)\n",
    "                        if n[0] == 1:\n",
    "                            lstring = u'shape=circle,margin=\"0,0\",style=filled,fillcolor=black,fontcolor=white,height=%0.2f,label=\"%d\"' % (\n",
    "                                2 * weight, n[1])\n",
    "                        else:\n",
    "                            lstring = u'shape=point,height=%0.2f' % weight\n",
    "                else:\n",
    "                    lstring = 'label=\"' + str(n) + '\",shape=none'\n",
    "                lstring = str(lstring)\n",
    "            else:\n",
    "                lstring = False\n",
    "            if position is not None:\n",
    "                if position == 'grid':\n",
    "                    position = [(i % 28, 28 - i / 28) for i in range(784)]\n",
    "                posstring = unicode('pos=\"' + str(position[n][0]) + ',' + str(position[n][1]) + '\"')\n",
    "            else:\n",
    "                posstring = False\n",
    "            finalstring = u' [' + u','.join([ts for ts in [posstring, lstring] if ts]) + u']\\n'\n",
    "            #finalstring = u' ['+lstring+u']\\n'\n",
    "            f.write(u'\\t' + cnn(n) + finalstring)\n",
    "        f.write(\"}\")\n",
    "        f.close()\n",
    "        if makepdf:\n",
    "            neato(filename, position=position, directed=directed)\n",
    "        return True\n",
    "\n",
    "\n",
    "    def predictable(out, data, wdict=None, topk=5, outfile='sorted_groups.txt', graphs=False, prefix='', athresh=0.5,\n",
    "                    tvalue=0.1):\n",
    "        alpha, labels, lpygx, mis, lasttc = out[:5]\n",
    "        ns, m = labels.shape\n",
    "        m, nv = mis.shape\n",
    "        hys = [entropy(labels[:, j]) for j in range(m)]\n",
    "        #alpha = np.array([z[2] for z in zs]) # m by nv\n",
    "        nmis = []\n",
    "        ixys = []\n",
    "        for j in range(m):\n",
    "            if hys[j] > 0:\n",
    "                #ixy = np.dot((alpha[j]>0.95).astype(int),mis[j])-lasttc[-1][j]\n",
    "                ixy = max(0., np.dot(alpha[j], mis[j]) - lasttc[-1][j])\n",
    "                ixys.append(ixy)\n",
    "                tcn = (np.sum(np.sort(alpha[j] * mis[j])[-topk:]) - ixy) / ((topk - 1) * hys[j])\n",
    "                nmis.append(tcn)  #ixy) #/hys[j])\n",
    "            else:\n",
    "                ixys.append(0)\n",
    "                nmis.append(0)\n",
    "        f = safe_open(prefix + outfile, 'w+')\n",
    "        print(list(enumerate(np.argsort(-np.array(nmis)))))\n",
    "        print(','.join(list(map(str, list(np.argsort(-np.array(nmis)))))))\n",
    "        for i, top in enumerate(np.argsort(-np.array(nmis))):\n",
    "            f.write('Group num: %d, Score: %0.3f\\n' % (top, nmis[top]))\n",
    "            inds = np.where(alpha[top] > athresh)[0]\n",
    "            inds = inds[np.argsort(-mis[top, inds])]\n",
    "            for ind in inds:\n",
    "                f.write(wdict[ind] + ', %0.3f\\n' % (mis[top, ind] / np.log(2)))\n",
    "            if wdict:\n",
    "                print(','.join(list(map(lambda q: wdict[q], inds))))\n",
    "                print(','.join(list(map(str, inds))))\n",
    "            print(top)\n",
    "            print(nmis[top], ixys[top], hys[top], ixys[top] / hys[top])  #,lasttc[-1][top],hys[top],lasttc[-1][top]/hys[top]\n",
    "            if graphs:\n",
    "                print(inds)\n",
    "                if len(inds) >= 2:\n",
    "                    plot_rels(data[:, inds[:5]], list(map(lambda q: wdict[q], inds[:5])),\n",
    "                              outfile='relationships/' + str(i) + '_group_num=' + str(top), latent=out[1][:, top],\n",
    "                              alpha=tvalue)\n",
    "        f.close()\n",
    "        return nmis\n",
    "\n",
    "\n",
    "    def shorten(s, n=12):\n",
    "        if len(s) > 2 * n:\n",
    "            return s[:n] + '..' + s[-n:]\n",
    "        return s\n",
    "\n",
    "\n",
    "    def plot_convergence(tc_history, prefix=''):\n",
    "        pylab.plot(tc_history)\n",
    "        pylab.xlabel('number of iterations')\n",
    "        filename = prefix + '/convergence.pdf'\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        pylab.savefig(filename)\n",
    "        pylab.close('all')\n",
    "        return True\n",
    "\n",
    "    def chunks(doc, n=100):\n",
    "        \"\"\"Yield successive approximately equal and n-sized chunks from l.\"\"\"\n",
    "        words = doc.split()\n",
    "        if len(words) == 0:\n",
    "            yield ''\n",
    "        n_chunks = len(words) / n  # Round down\n",
    "        if n_chunks == 0:\n",
    "            n_per_chunk = n\n",
    "        else:\n",
    "            n_per_chunk = int(np.ceil(float(len(words)) / n_chunks))  # round up\n",
    "        for i in xrange(0, len(words), n_per_chunk):\n",
    "            yield ' '.join(words[i:i+n])\n",
    "\n",
    "\n",
    "    # Utilities to construct generalized binary bag of words matrices\n",
    "\n",
    "\n",
    "    def av_bbow(docs, n=100):\n",
    "        \"\"\"Average binary bag of words if we take chunks of a doc of size n\"\"\"\n",
    "        proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "        proc.fit(docs)\n",
    "        n_doc, n_words = len(docs), len(proc.vocabulary_)\n",
    "        mat = ss.lil_matrix((n_doc, n_words))\n",
    "        for l, doc in enumerate(docs):\n",
    "            subdocs = chunks(doc, n=n)\n",
    "            mat[l] = (proc.transform(subdocs) > 0).mean(axis=0).A.ravel()\n",
    "        return mat.asformat('csr'), proc\n",
    "\n",
    "\n",
    "    def bow(docs):\n",
    "        \"\"\"Standard bag of words\"\"\"\n",
    "        proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "        return proc.fit_transform(docs), proc\n",
    "\n",
    "\n",
    "    def all_bbow(docs, n=100):\n",
    "        \"\"\"Split each document into a subdocuments of size n, and return as binary BOW\"\"\"\n",
    "        proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "        proc.fit(docs)\n",
    "        ids = []\n",
    "        for l, doc in enumerate(docs):\n",
    "            subdocs = chunks(doc, n=n)\n",
    "            submat = (proc.transform(subdocs) > 0)\n",
    "            if l == 0:\n",
    "              mat = submat\n",
    "            else:\n",
    "              mat = ss.vstack([mat, submat])\n",
    "            ids += [l]*submat.shape[0]\n",
    "        return mat.asformat('csr'), proc, ids\n",
    "\n",
    "\n",
    "    def file_to_array(filename, stemming=False, strategy=2, words_per_doc=100, n_words=10000):\n",
    "        pattern = '\\\\b[A-Za-z]+\\\\b'\n",
    "        stemmer = SnowballStemmer('english')\n",
    "\n",
    "        with open(filename, 'rU') as input_file:\n",
    "            docs = []\n",
    "            for line in input_file:\n",
    "                if stemming:\n",
    "                    docs.append(' '.join([stemmer.stem(w) for w in re.findall(pattern, line)]))\n",
    "                else:\n",
    "                    docs.append(' '.join([w for w in re.findall(pattern, line)]))\n",
    "        print('processing file')\n",
    "\n",
    "        if strategy == 1:\n",
    "            X, proc = av_bbow(docs, n=words_per_doc)\n",
    "        elif strategy == 2:\n",
    "            X, proc, ids = all_bbow(docs, n=words_per_doc)\n",
    "        else:\n",
    "            X, proc = bow(docs)\n",
    "\n",
    "        var_order = np.argsort(-X.sum(axis=0).A1)[:n_words]\n",
    "        X = X[:, var_order]\n",
    "\n",
    "        #Dictionary\n",
    "        ivd = {v: k for k, v in proc.vocabulary_.items()}\n",
    "        words = [ivd[v] for v in var_order]\n",
    "        return X, words\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        # Command line interface\n",
    "        # Sample commands:\n",
    "        # python vis_topic.py tests/data/twenty.txt --n_words=2000 --layers=20,3,1 -v --edges=50 -o test_output\n",
    "        from optparse import OptionParser, OptionGroup\n",
    "\n",
    "\n",
    "        parser = OptionParser(usage=\"usage: %prog [options] data_file.csv \\n\"\n",
    "                                    \"Assume one document on each line.\")\n",
    "\n",
    "        group = OptionGroup(parser, \"Options\")\n",
    "        group.add_option(\"-n\", \"--n_words\",\n",
    "                         action=\"store\", dest=\"n_words\", type=\"int\", default=10000,\n",
    "                         help=\"Maximum number of words to include in dictionary.\")\n",
    "        group.add_option(\"-l\", \"--layers\", dest=\"layers\", type=\"string\", default=\"2,1\",\n",
    "                         help=\"Specify number of units at each layer: 5,3,1 has \"\n",
    "                              \"5 units at layer 1, 3 at layer 2, and 1 at layer 3\")\n",
    "        group.add_option(\"-t\", \"--strategy\", dest=\"strategy\", type=\"int\", default=0,\n",
    "                         help=\"Specify the strategy for handling non-binary count data.\\n\"\n",
    "                              \"0. Naive binarization. This will be good for documents of similar length and especially\"\n",
    "                              \"short documents.\\n\"\n",
    "                              \"1. Average binary bag of words. We split documents into chunks, compute the binary \"\n",
    "                              \"bag of words for each documents and then average. This implicitly weights all documents\"\n",
    "                              \"equally.\\n\"\n",
    "                              \"2. All binary bag of words. Split documents into chunks and consider each chunk as its\"\n",
    "                              \"own binary bag of words documents. This changes the number of documents so it may take\"\n",
    "                              \"some work to match the ids back, if desired.\\n\"\n",
    "                              \"3. Fractional counts. This converts counts into a fraction of the background rate, with 1 as\"\n",
    "                              \"the max. Short documents tend to stay binary and words in long documents are weighted\"\n",
    "                              \"according to their frequency with respect to background in the corpus.\")\n",
    "        group.add_option(\"-o\", \"--output\",\n",
    "                         action=\"store\", dest=\"output\", type=\"string\", default=\"topic_output\",\n",
    "                         help=\"A directory to put all output files.\")\n",
    "        group.add_option(\"-s\", \"--stemming\",\n",
    "                         action=\"store_false\", dest=\"stemming\", default=True,\n",
    "                         help=\"Use a stemmer on words.\")\n",
    "        group.add_option(\"-v\", \"--verbose\",\n",
    "                         action=\"store_true\", dest=\"verbose\", default=False,\n",
    "                         help=\"Print rich outputs while running.\")\n",
    "        group.add_option(\"-w\", \"--words_per_doc\",\n",
    "                         action=\"store\", dest=\"words_per_doc\", type=\"int\", default=300,\n",
    "                         help=\"If using all_bbow or av_bbow, this specifies the number of words each \"\n",
    "                              \"to split documents into.\")\n",
    "        group.add_option(\"-e\", \"--edges\",\n",
    "                         action=\"store\", dest=\"max_edges\", type=\"int\", default=1000,\n",
    "                         help=\"Show at most this many edges in graphs.\")\n",
    "        group.add_option(\"-q\", \"--regraph\",\n",
    "                         action=\"store_true\", dest=\"regraph\", default=False,\n",
    "                         help=\"Don't re-run corex, just re-generate outputs (with number of edges changed).\")\n",
    "        parser.add_option_group(group)\n",
    "\n",
    "        (options, args) = parser.parse_args()\n",
    "        if not len(args) == 1:\n",
    "            print(\"Run with '-h' option for usage help.\")\n",
    "            sys.exit()\n",
    "\n",
    "        layers = list(map(int, options.layers.split(',')))\n",
    "        if layers[-1] != 1:\n",
    "            layers.append(1)  # Last layer has one unit for convenience so that graph is fully connected.\n",
    "\n",
    "        #Load data from text file\n",
    "        print('reading file')\n",
    "        X, words = file_to_array(args[0], stemming=options.stemming, strategy=options.strategy,\n",
    "                                 words_per_doc=options.words_per_doc, n_words=options.n_words)\n",
    "        # cPickle.dump(words, open(options.prefix + '/dictionary.dat', 'w'))  # TODO: output dictionary\n",
    "\n",
    "        # Run CorEx on data\n",
    "        if options.verbose:\n",
    "            np.set_printoptions(precision=3, suppress=True)  # For legible output from numpy\n",
    "            print('\\nData summary: X has %d rows and %d columns' % X.shape)\n",
    "            print('Variable names are: ' + ','.join(words))\n",
    "            print('Getting CorEx results')\n",
    "        if options.strategy == 3:\n",
    "            count = 'fraction'\n",
    "        else:\n",
    "            count = 'binarize'  # Strategies 1 and 2 already produce counts <= 1 and are not affected by this choice.\n",
    "        if not options.regraph:\n",
    "            for l, layer in enumerate(layers):\n",
    "                if options.verbose:\n",
    "                    print(\"Layer \", l)\n",
    "                if l == 0:\n",
    "                    t0 = time()\n",
    "                    corexes = [ct.Corex(n_hidden=layer, verbose=options.verbose, count=count).fit(X)]\n",
    "                    print('Time for first layer: %0.2f' % (time() - t0))\n",
    "                else:\n",
    "                    X_prev = np.matrix(corexes[-1].labels)\n",
    "                    corexes.append(ct.Corex(n_hidden=layer, verbose=options.verbose).fit(X_prev))\n",
    "            for l, corex in enumerate(corexes):\n",
    "                # The learned model can be loaded again using ct.Corex().load(filename)\n",
    "                print('TC at layer %d is: %0.3f' % (l, corex.tc))\n",
    "                corex.save(options.output + '/layer_' + str(l) + '.dat')\n",
    "        else:\n",
    "            corexes = [ct.Corex().load(options.output + '/layer_' + str(l) + '.dat') for l in range(len(layers))]\n",
    "\n",
    "\n",
    "        # This line outputs plots showing relationships at the first layer\n",
    "        vis_rep(corexes[0], data=X, column_label=words, prefix=options.output)\n",
    "        # This line outputs a hierarchical networks structure in a .dot file in the \"graphs\" folder\n",
    "        # And it tries to compile the dot file into a pdf using the command line utility sfdp (part of graphviz)\n",
    "        vis_hierarchy(corexes, column_label=words, max_edges=options.max_edges, prefix=options.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryanalcantara/Buff Drive/Biomech_Lit_Up/literature_update/Construct_Models/tests/d3_files/force.html topic-model-example/graphs/force.html\n"
     ]
    }
   ],
   "source": [
    "# prefix = 'topic-model-example'\n",
    "print(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight threshold is 0.000000 for graph with max of 200.000000 edges \n",
      "non-isolated nodes,edges 60 76\n",
      "non-isolated nodes,edges 59 58\n",
      "/Users/ryanalcantara/Buff Drive/Biomech_Lit_Up/literature_update/Construct_Models/tests/d3_files/force.html topic-model-example/graphs/force.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'topic-model-example/graphs/graph_prune_200.dot.pdf'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import unicode\n",
    "vis_hierarchy([topic_model, tm_layer2, tm_layer3], column_label=words, max_edges=200, prefix='topic-model-example')\n",
    "\n",
    "from graphviz import Source\n",
    "path = 'topic-model-example/graphs/graph_prune_200.dot'\n",
    "s = Source.from_file(path)\n",
    "s.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchoring for Semi-Supervised Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchored CorEx is an extension of CorEx that allows the \"anchoring\" of words to topics. When anchoring a word to a topic, CorEx is trying to maximize the mutual information between that word and the anchored topic. So, anchoring provides a way to guide the topic model towards specific subsets of words that the user would like to explore.  \n",
    "\n",
    "The anchoring mechanism is flexible, and so there are many possibilities of anchoring. We explored the following types of anchoring in our TACL paper:\n",
    "\n",
    "1. Anchoring a single set of words to a single topic. This can help promote a topic that did not naturally emerge when running an unsupervised instance of the CorEx topic model. For example, one might anchor words like \"snow,\" \"cold,\" and \"avalanche\" to a topic if one suspects there should be a snow avalanche topic within a set of disaster relief articles.\n",
    "\n",
    "2. Anchoring single sets of words to multiple topics. This can help find different aspects of a topic that may be discussed in several different contexts. For example, one might anchor \"protest\" to three topics and \"riot\" to three other topics to understand different framings that arise from tweets about political protests.\n",
    "\n",
    "3. Anchoring different sets of words to multiple topics. This can help enforce topic separability if there appear to be chimera topics. For example, one might anchor \"mountain,\" \"Bernese,\" and \"dog\" to one topic and \"mountain,\" \"rocky,\" and \"colorado\" to another topic to help separate topics that merge discussion of Bernese Mountain Dogs and the Rocky Mountains.\n",
    "\n",
    "\n",
    "We'll demonstrate how to anchor words to the the CorEx topic model and how to develop other anchoring strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('archive-name', 0.03041865845654593),\n",
       " ('last-modified', 0.021862379241347728),\n",
       " ('contents', 0.012221520483933474),\n",
       " ('series', 0.011907408313657811),\n",
       " ('dave', 0.010942577165063716),\n",
       " ('jpl', 0.010240237124075112),\n",
       " ('sections', 0.0101389765658582),\n",
       " ('stanford', 0.009257505720168558),\n",
       " ('index', 0.008781701567886728),\n",
       " ('propulsion', 0.008203861704857577)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anchor one word to the first topic\n",
    "anchor_words = ['nasa']\n",
    "topic_model.get_topics(topic=0, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor the word 'nasa' to the first topic\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2) #50 topics\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This anchors the single word \"nasa\" to the first topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: nasa,space,gov,orbit,last-modified,shuttle,moon,lunar,launch,earth\n"
     ]
    }
   ],
   "source": [
    "topic_words,_ = zip(*anchored_topic_model.get_topics(topic=0))\n",
    "print('0: ' + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can anchor multiple groups of words to multiple topics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor 'nasa' and 'space' to first topic, 'sports' and 'stadium' to second topic, so on...\n",
    "anchor_words = [['nasa', 'space'], ['sports', 'stadium'], ['politics', 'government'], ['love', 'hope']]\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: space,nasa,orbit,moon,shuttle,launch,gov,earth,lunar,ames\n",
      "1: sports,stadium,april,san,city,los,york,washington,angeles,center\n",
      "2: government,politics,state,rights,law,war,country,military,public,security\n",
      "3: hope,love,helps,relates,virile,tatoos,sustaining,whosoever,weird,allegory\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in the above topic model, topics will no longer be sorted according to descending TC. Instead, the first topic will be the one with \"nasa\" and \"space\" anchored to it, the second topic will be the one with \"sports\" and \"stadium\" anchored to it, and so on.  \n",
    "\n",
    "Observe, the topic with \"love\" and \"hope\" anchored to it is less interpretable than the other three topics. This could be a sign that there is not a good topic around these two words, and one should consider if it is appropriate to anchor around them.\n",
    "\n",
    "We can continue to develop even more involved anchoring strategies. Here we anchor \"nasa\" by itself, as well as in two other topics each with \"politics\" and \"news\" to find different aspects around the word \"nasa\". We also create a fourth anchoring of \"war\" to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor with single words and groups of words\n",
    "anchor_words = ['nasa', ['nasa', 'politics'], ['nasa', 'news'], 'war']\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: nasa,space,orbit,launch,shuttle,moon,earth,lunar,satellite,commercial\n",
      "1: nasa,politics,research,gov,science,scientific,institute,organization,studies,providing\n",
      "2: news,nasa,insisting,edwards,hal,llnl,cso,cfv,nodak,admin\n",
      "3: war,israel,armenians,armenian,israeli,jews,soldiers,military,killed,history\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you do not specify the column labels through `words`, then you can still anchor by specifying the column indices of the features you wish to anchor on. You may also specify anchors using a mix of strings and indices if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing anchor strength:** the anchor strength controls how much weight CorEx puts towards maximizing the mutual information between the anchor words and their respective topics. Anchor strength should always be set at a value *greater than* 1, since setting anchor strength between 0 and 1 only recovers the unsupervised CorEx objective. Empirically, setting anchor strength from 1.5-3 seems to nudge the topic model towards the anchor words. Setting anchor strength greater than 5 is strongly enforcing that the CorEx topic model find a topic associated with the anchor words.\n",
    "\n",
    "We encourage users to experiment with the anchor strength and determine what values are best for their needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`vis_topic`** module provides support for outputting topics and visualizations of the CorEx topic model. The code below creates a results direcory named \"twenty\" in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vt.vis_rep(topic_model, column_label=words, prefix='twenty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TACL paper details the theory of the CorEx topic model, its sparsity optimization, anchoring via the information bottleneck, comparisons to LDA, and anchoring experiments. The two papers from Greg Ver Steeg and Aram Galstyan develop the CorEx theory in general and provide further motivation and details of the underlying CorEx mechanisms. Hodas et al. demonstrated early CorEx topic model results and investigated an application of pointwise total correlations to quantify \"surprising\" documents.\n",
    "\n",
    "1. [Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244), Gallagher et al., TACL 2017.\n",
    "\n",
    "2. [Discovering Structure in High-Dimensional Data Through Correlation Explanation](https://arxiv.org/abs/1406.1222), Ver Steeg and Galstyan, NIPS 2014. \n",
    "\n",
    "3. [Maximally Informative Hierarchical Representions of High-Dimensional Data](https://arxiv.org/abs/1410.7404), Ver Steeg and Galstyan, AISTATS 2015.\n",
    "\n",
    "4. [Disentangling the Lexicons of Disaster Response in Twitter](https://dl.acm.org/citation.cfm?id=2741728), Hodas et al., WWW 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
