{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchored CorEx: Topic Modeling with Minimal Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** [Ryan J. Gallagher](http://ryanjgallagher.github.io/)  \n",
    "\n",
    "**Last updated:** 07/21/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to use the CorEx topic model code. This includes fitting CorEx to your data, examining the topic model output, outputting results, building a hierarchical topic model, and anchoring words to topics.\n",
    "\n",
    "Details of the CorEx topic model and evaluations against unsupervised and semi-supervised variants of LDA can be found in our TACL paper:\n",
    "\n",
    "Gallagher, Ryan J., Kyle Reing, David Kale, and Greg Ver Steeg. \"[Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244).\" *Transactions of the Association for Computational Linguistics (TACL)*, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic# jupyter notebooks will complain matplotlib is being loaded twice\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first load data to run the CorEx topic model. We'll use the 20 Newsgroups dataset, which scikit-learn provides functionality to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\n",
      "BONE                              1957\n",
      "CARDIOVASCULAR/CARDIOPULMONARY    1164\n",
      "CELLULAR/SUBCELLULAR              1217\n",
      "COMPARATIVE                       1602\n",
      "DENTAL/ORAL/FACIAL                1346\n",
      "ERGONOMICS                         502\n",
      "EVOLUTION/ANTHROPOLOGY             998\n",
      "GAIT/LOCOMOTION                   3184\n",
      "JOINT/CARTILAGE                   1429\n",
      "METHODS                           1440\n",
      "MODELING                          1240\n",
      "MUSCLE                             705\n",
      "NEURAL                            1705\n",
      "ORTHOPAEDICS/SPINE                2286\n",
      "ORTHOPAEDICS/SURGERY              3056\n",
      "PROSTHETICS/ORTHOTICS              554\n",
      "REHABILITATION                    1176\n",
      "ROBOTICS                          1059\n",
      "SPORT/EXERCISE                    2810\n",
      "TENDON/LIGAMENT                   1551\n",
      "TISSUE/BIOMATERIAL                2117\n",
      "TRAUMA/IMPACTTESTING               690\n",
      "VETERINARY/AGRICULTURAL            702\n",
      "VISUAL/VESTIBULAR/EYE              777\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/RYANDATA_filt.csv')\n",
    "data.columns = ['V0', 'topic', 'authors','title','journal','year','vol_issue','doi','abstract']\n",
    "print(data.groupby('topic')['title'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAIT/LOCOMOTION                   2547\n",
       "ORTHOPAEDICS/SURGERY              2445\n",
       "SPORT/EXERCISE                    2248\n",
       "ORTHOPAEDICS/SPINE                1829\n",
       "TISSUE/BIOMATERIAL                1693\n",
       "BONE                              1565\n",
       "NEURAL                            1364\n",
       "COMPARATIVE                       1281\n",
       "TENDON/LIGAMENT                   1241\n",
       "METHODS                           1152\n",
       "JOINT/CARTILAGE                   1143\n",
       "DENTAL/ORAL/FACIAL                1077\n",
       "MODELING                           992\n",
       "CELLULAR/SUBCELLULAR               974\n",
       "REHABILITATION                     941\n",
       "CARDIOVASCULAR/CARDIOPULMONARY     931\n",
       "ROBOTICS                           847\n",
       "EVOLUTION/ANTHROPOLOGY             798\n",
       "VISUAL/VESTIBULAR/EYE              622\n",
       "MUSCLE                             564\n",
       "VETERINARY/AGRICULTURAL            562\n",
       "TRAUMA/IMPACTTESTING               552\n",
       "PROSTHETICS/ORTHOTICS              443\n",
       "ERGONOMICS                         402\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data keeping distribution\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits= 1, \n",
    "                             test_size = 0.2, \n",
    "                             random_state = 0)\n",
    "\n",
    "for train_idx, test_idx in sss.split(data['title'],data['topic']):\n",
    "    X_train, X_test = data['title'][train_idx], data['title'][test_idx]\n",
    "    y_train, y_test = data['topic'][train_idx], data['topic'][test_idx]\n",
    "\n",
    "\n",
    "y_train.value_counts() #same distribution as original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "Proximal radius fracture morphology following axial force impact: a biomechanical evaluation of fracture patterns\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "proximal radius fracture morphology following axial force impact biomechanical evaluation fracture patterns \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8793</th>\n",
       "      <td>overcoming limitations harmonic ratio reliable...</td>\n",
       "      <td>GAIT/LOCOMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6411</th>\n",
       "      <td>influence different abutment designs biomechan...</td>\n",
       "      <td>DENTAL/ORAL/FACIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002</th>\n",
       "      <td>content validation clinical assessment instrum...</td>\n",
       "      <td>REHABILITATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35014</th>\n",
       "      <td>neural control adaptive neural forward models ...</td>\n",
       "      <td>ROBOTICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16312</th>\n",
       "      <td>dual developmental origin spinal cerebrospinal...</td>\n",
       "      <td>NEURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21506</th>\n",
       "      <td>remplissage versus latarjet engaging hill sach...</td>\n",
       "      <td>ORTHOPAEDICS/SURGERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>aquantitative image analysis cellular cytoskel...</td>\n",
       "      <td>CELLULAR/SUBCELLULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34341</th>\n",
       "      <td>gait trajectory rolling planning control hexap...</td>\n",
       "      <td>ROBOTICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11238</th>\n",
       "      <td>functional properties chondrocytes articular c...</td>\n",
       "      <td>JOINT/CARTILAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4816</th>\n",
       "      <td>effect foot posture capacity apply free moment...</td>\n",
       "      <td>COMPARATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27340</th>\n",
       "      <td>differences kinematics baseball swing hitters ...</td>\n",
       "      <td>SPORT/EXERCISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>sclerostin antibody increases callus size stre...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31008</th>\n",
       "      <td>coculture human mesenchymal stem cells articul...</td>\n",
       "      <td>TISSUE/BIOMATERIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>mechanical response human subclavian iliac art...</td>\n",
       "      <td>CARDIOVASCULAR/CARDIOPULMONARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>statistical analysis inter individual variatio...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28648</th>\n",
       "      <td>relative motion tendon limbs loop tendon graft</td>\n",
       "      <td>TENDON/LIGAMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8844</th>\n",
       "      <td>comparison muscle synergies running different ...</td>\n",
       "      <td>GAIT/LOCOMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6559</th>\n",
       "      <td>push bond strength evaluation glass fiber post...</td>\n",
       "      <td>DENTAL/ORAL/FACIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17356</th>\n",
       "      <td>sensitivity dimensional hindlimb joint kinemat...</td>\n",
       "      <td>NEURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32393</th>\n",
       "      <td>vitro comparison suture techniques anastomosis...</td>\n",
       "      <td>VETERINARY/AGRICULTURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17779</th>\n",
       "      <td>intramedullary nail integrated cephalocervical...</td>\n",
       "      <td>ORTHOPAEDICS/SPINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>regional left ventricular myocardial contracti...</td>\n",
       "      <td>CARDIOVASCULAR/CARDIOPULMONARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>circle method helps reliable cortical thicknes...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>combined treatment alendronate drynaria rhizom...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8472</th>\n",
       "      <td>study association gait variability physical ac...</td>\n",
       "      <td>GAIT/LOCOMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>neuromuscular efficiency stand movement women ...</td>\n",
       "      <td>JOINT/CARTILAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>movement deviation profile measure distance no...</td>\n",
       "      <td>GAIT/LOCOMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31104</th>\n",
       "      <td>characterizing endogenous time course controll...</td>\n",
       "      <td>TRAUMA/IMPACTTESTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17691</th>\n",
       "      <td>preventing pseudoarthrosis proximal junctional...</td>\n",
       "      <td>ORTHOPAEDICS/SPINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>effect active pedaling combined electrical sti...</td>\n",
       "      <td>NEURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32736</th>\n",
       "      <td>vivo early corneal biomechanical changes corne...</td>\n",
       "      <td>VISUAL/VESTIBULAR/EYE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12860</th>\n",
       "      <td>dimensional shear wave imaging based field las...</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18295</th>\n",
       "      <td>biomechanical fixation properties cortical ver...</td>\n",
       "      <td>ORTHOPAEDICS/SPINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11440</th>\n",
       "      <td>biomechanical viscoelastic properties differen...</td>\n",
       "      <td>JOINT/CARTILAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>biomechanics race walking literature overview ...</td>\n",
       "      <td>GAIT/LOCOMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>effect spinal level loading conditions product...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9490</th>\n",
       "      <td>decoding sensorimotor rhythms robotic assisted...</td>\n",
       "      <td>GAIT/LOCOMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33917</th>\n",
       "      <td>modeling skeletal traits functions upper body ...</td>\n",
       "      <td>EVOLUTION/ANTHROPOLOGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>verification dimensional finite element method...</td>\n",
       "      <td>CARDIOVASCULAR/CARDIOPULMONARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32555</th>\n",
       "      <td>predicting refractive outcome small incision l...</td>\n",
       "      <td>VISUAL/VESTIBULAR/EYE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29907</th>\n",
       "      <td>protein engineered scaffolds vitro culture pri...</td>\n",
       "      <td>TISSUE/BIOMATERIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>extracting accurate strain measurements bone m...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22492</th>\n",
       "      <td>primary results kienbock disease treated ballo...</td>\n",
       "      <td>ORTHOPAEDICS/SURGERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13861</th>\n",
       "      <td>optimization split keyboard design touchscreen...</td>\n",
       "      <td>MODELING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>femoroplasty injectable resorbable calcium pho...</td>\n",
       "      <td>BONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16618</th>\n",
       "      <td>dissociation locomotor cerebellar deficits mur...</td>\n",
       "      <td>NEURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22691</th>\n",
       "      <td>mechanical stress tensioned wires direct indir...</td>\n",
       "      <td>ORTHOPAEDICS/SURGERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6224</th>\n",
       "      <td>mechanical equilibrium forces moments applied ...</td>\n",
       "      <td>DENTAL/ORAL/FACIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26494</th>\n",
       "      <td>trunk bend twist coordination affected pain st...</td>\n",
       "      <td>SPORT/EXERCISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15922</th>\n",
       "      <td>consistent visuomotor adaptations generalizati...</td>\n",
       "      <td>NEURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>influence delivered radiant exposure values bo...</td>\n",
       "      <td>DENTAL/ORAL/FACIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>attenuation spinal cord ischemia reperfusion i...</td>\n",
       "      <td>CARDIOVASCULAR/CARDIOPULMONARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23437</th>\n",
       "      <td>walking gait asymmetries months following ante...</td>\n",
       "      <td>REHABILITATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4718</th>\n",
       "      <td>enclosure utilisation activity budgets disable...</td>\n",
       "      <td>COMPARATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29438</th>\n",
       "      <td>porous scaffold internal architecture design b...</td>\n",
       "      <td>TISSUE/BIOMATERIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22161</th>\n",
       "      <td>comparison gait pathology outcomes meniscal pr...</td>\n",
       "      <td>ORTHOPAEDICS/SURGERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>related changes locomotor performance reveal s...</td>\n",
       "      <td>COMPARATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15283</th>\n",
       "      <td>vivo multiscale spatially dependent biomechani...</td>\n",
       "      <td>MUSCLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34506</th>\n",
       "      <td>adaptive energy efficient walking hexapod robo...</td>\n",
       "      <td>ROBOTICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>comparison multiple linear regression artifici...</td>\n",
       "      <td>MODELING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28213 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "8793   overcoming limitations harmonic ratio reliable...   \n",
       "6411   influence different abutment designs biomechan...   \n",
       "24002  content validation clinical assessment instrum...   \n",
       "35014  neural control adaptive neural forward models ...   \n",
       "16312  dual developmental origin spinal cerebrospinal...   \n",
       "21506  remplissage versus latarjet engaging hill sach...   \n",
       "3450   aquantitative image analysis cellular cytoskel...   \n",
       "34341  gait trajectory rolling planning control hexap...   \n",
       "11238  functional properties chondrocytes articular c...   \n",
       "4816   effect foot posture capacity apply free moment...   \n",
       "27340  differences kinematics baseball swing hitters ...   \n",
       "596    sclerostin antibody increases callus size stre...   \n",
       "31008  coculture human mesenchymal stem cells articul...   \n",
       "2095   mechanical response human subclavian iliac art...   \n",
       "548    statistical analysis inter individual variatio...   \n",
       "28648     relative motion tendon limbs loop tendon graft   \n",
       "8844   comparison muscle synergies running different ...   \n",
       "6559   push bond strength evaluation glass fiber post...   \n",
       "17356  sensitivity dimensional hindlimb joint kinemat...   \n",
       "32393  vitro comparison suture techniques anastomosis...   \n",
       "17779  intramedullary nail integrated cephalocervical...   \n",
       "3120   regional left ventricular myocardial contracti...   \n",
       "75     circle method helps reliable cortical thicknes...   \n",
       "124    combined treatment alendronate drynaria rhizom...   \n",
       "8472   study association gait variability physical ac...   \n",
       "12335  neuromuscular efficiency stand movement women ...   \n",
       "10789  movement deviation profile measure distance no...   \n",
       "31104  characterizing endogenous time course controll...   \n",
       "17691  preventing pseudoarthrosis proximal junctional...   \n",
       "16997  effect active pedaling combined electrical sti...   \n",
       "...                                                  ...   \n",
       "32736  vivo early corneal biomechanical changes corne...   \n",
       "12860  dimensional shear wave imaging based field las...   \n",
       "18295  biomechanical fixation properties cortical ver...   \n",
       "11440  biomechanical viscoelastic properties differen...   \n",
       "9990   biomechanics race walking literature overview ...   \n",
       "1816   effect spinal level loading conditions product...   \n",
       "9490   decoding sensorimotor rhythms robotic assisted...   \n",
       "33917  modeling skeletal traits functions upper body ...   \n",
       "3112   verification dimensional finite element method...   \n",
       "32555  predicting refractive outcome small incision l...   \n",
       "29907  protein engineered scaffolds vitro culture pri...   \n",
       "1137   extracting accurate strain measurements bone m...   \n",
       "22492  primary results kienbock disease treated ballo...   \n",
       "13861  optimization split keyboard design touchscreen...   \n",
       "1928   femoroplasty injectable resorbable calcium pho...   \n",
       "16618  dissociation locomotor cerebellar deficits mur...   \n",
       "22691  mechanical stress tensioned wires direct indir...   \n",
       "6224   mechanical equilibrium forces moments applied ...   \n",
       "26494  trunk bend twist coordination affected pain st...   \n",
       "15922  consistent visuomotor adaptations generalizati...   \n",
       "6665   influence delivered radiant exposure values bo...   \n",
       "2982   attenuation spinal cord ischemia reperfusion i...   \n",
       "23437  walking gait asymmetries months following ante...   \n",
       "4718   enclosure utilisation activity budgets disable...   \n",
       "29438  porous scaffold internal architecture design b...   \n",
       "22161  comparison gait pathology outcomes meniscal pr...   \n",
       "4789   related changes locomotor performance reveal s...   \n",
       "15283  vivo multiscale spatially dependent biomechani...   \n",
       "34506  adaptive energy efficient walking hexapod robo...   \n",
       "14985  comparison multiple linear regression artifici...   \n",
       "\n",
       "                                topic  \n",
       "8793                  GAIT/LOCOMOTION  \n",
       "6411               DENTAL/ORAL/FACIAL  \n",
       "24002                  REHABILITATION  \n",
       "35014                        ROBOTICS  \n",
       "16312                          NEURAL  \n",
       "21506            ORTHOPAEDICS/SURGERY  \n",
       "3450             CELLULAR/SUBCELLULAR  \n",
       "34341                        ROBOTICS  \n",
       "11238                 JOINT/CARTILAGE  \n",
       "4816                      COMPARATIVE  \n",
       "27340                  SPORT/EXERCISE  \n",
       "596                              BONE  \n",
       "31008              TISSUE/BIOMATERIAL  \n",
       "2095   CARDIOVASCULAR/CARDIOPULMONARY  \n",
       "548                              BONE  \n",
       "28648                 TENDON/LIGAMENT  \n",
       "8844                  GAIT/LOCOMOTION  \n",
       "6559               DENTAL/ORAL/FACIAL  \n",
       "17356                          NEURAL  \n",
       "32393         VETERINARY/AGRICULTURAL  \n",
       "17779              ORTHOPAEDICS/SPINE  \n",
       "3120   CARDIOVASCULAR/CARDIOPULMONARY  \n",
       "75                               BONE  \n",
       "124                              BONE  \n",
       "8472                  GAIT/LOCOMOTION  \n",
       "12335                 JOINT/CARTILAGE  \n",
       "10789                 GAIT/LOCOMOTION  \n",
       "31104            TRAUMA/IMPACTTESTING  \n",
       "17691              ORTHOPAEDICS/SPINE  \n",
       "16997                          NEURAL  \n",
       "...                               ...  \n",
       "32736           VISUAL/VESTIBULAR/EYE  \n",
       "12860                         METHODS  \n",
       "18295              ORTHOPAEDICS/SPINE  \n",
       "11440                 JOINT/CARTILAGE  \n",
       "9990                  GAIT/LOCOMOTION  \n",
       "1816                             BONE  \n",
       "9490                  GAIT/LOCOMOTION  \n",
       "33917          EVOLUTION/ANTHROPOLOGY  \n",
       "3112   CARDIOVASCULAR/CARDIOPULMONARY  \n",
       "32555           VISUAL/VESTIBULAR/EYE  \n",
       "29907              TISSUE/BIOMATERIAL  \n",
       "1137                             BONE  \n",
       "22492            ORTHOPAEDICS/SURGERY  \n",
       "13861                        MODELING  \n",
       "1928                             BONE  \n",
       "16618                          NEURAL  \n",
       "22691            ORTHOPAEDICS/SURGERY  \n",
       "6224               DENTAL/ORAL/FACIAL  \n",
       "26494                  SPORT/EXERCISE  \n",
       "15922                          NEURAL  \n",
       "6665               DENTAL/ORAL/FACIAL  \n",
       "2982   CARDIOVASCULAR/CARDIOPULMONARY  \n",
       "23437                  REHABILITATION  \n",
       "4718                      COMPARATIVE  \n",
       "29438              TISSUE/BIOMATERIAL  \n",
       "22161            ORTHOPAEDICS/SURGERY  \n",
       "4789                      COMPARATIVE  \n",
       "15283                          MUSCLE  \n",
       "34506                        ROBOTICS  \n",
       "14985                        MODELING  \n",
       "\n",
       "[28213 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "#tokenize, lemmatized, stemmed\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "#             result.append(lemmatize_stemming(token))\n",
    "            result.append(token)\n",
    "    result = ' '.join(result)        \n",
    "    return result\n",
    "\n",
    "doc_sample = X_train[0]\n",
    "print('Original document: ')\n",
    "words = []\n",
    "for doc in [doc_sample]:\n",
    "    print(doc)\n",
    "print('\\nTokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample),'\\n')\n",
    "\n",
    "X_train_proc = X_train.map(preprocess)\n",
    "\n",
    "train_data = pd.DataFrame({'title':X_train_proc, 'topic':y_train})\n",
    "train_data\n",
    "                      \n",
    "          \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model assumes input is in the form of a doc-word matrix, where rows are documents and columns are binary counts. We'll vectorize the newsgroups data, take the top 20,000 words, and convert it to a sparse matrix to save on memory usage. Note, we use binary count vectors as input to the CorEx topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28213, 20000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform title data into sparse matrix\n",
    "#additional stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['biomechanics','biomechanical','locomotor','locomotion','study'])\n",
    "#CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=3,\n",
    "                            max_df=0.5,\n",
    "                            max_features= 20000,\n",
    "                            ngram_range=(1,3),\n",
    "                            strip_accents='unicode',\n",
    "                            lowercase=True,\n",
    "                            analyzer='word',\n",
    "                            stop_words=stop_words,\n",
    "                            token_pattern= '[a-zA-Z-0-9]{3,}' #TRY BINARY = TRUE\n",
    "                           )\n",
    "doc_word = vectorizer.fit_transform(train_data['title'])\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "doc_word.shape # n_docs x m_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get common words for each topic\n",
    "n_top_topics = 3\n",
    "title_subset = []\n",
    "top_words = []\n",
    "freq_list = []\n",
    "words_freq = []\n",
    "for i in train_data.groupby('topic'):\n",
    "    title_subset.append(i)\n",
    "for a in title_subset:  \n",
    "    a # all titles in a given topic\n",
    "    bag_of_words = vectorizer.transform(a[1]['title'])\n",
    "    sum_words = bag_of_words.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    words_freq = words_freq[:n_top_topics]\n",
    "    for item in list(zip(*words_freq))[0]:\n",
    "        freq_list.append(item)\n",
    "\n",
    "freq_list = [freq_list[i:i + n_top_topics] for i in range(0, len(freq_list), n_top_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bone', 'fracture', 'model']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_list = pd.DataFrame({'top_words':freq_list})\n",
    "anchor_list.iloc[0]['top_words'] #will use this to anchor model later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our doc-word matrix is 11,314 documents by 20,000 words. Let's get the words that label the columns. We'll need these for outputting readable topics and later for anchoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters of the CorEx topic model are:\n",
    "+ **`n_hidden`**: number of topics (\"hidden\" as in \"hidden latent topics\")\n",
    "+ **`words`**: words that label the columns of the doc-word matrix (optional)\n",
    "+ **`docs`**: document labels that label the rows of the doc-word matrix (optional)\n",
    "+ **`max_iter`**: number of iterations to run through the update equations (optional, defaults to 200)\n",
    "+ **`verbose`**:  if `verbose=1`, then CorEx will print the topic TCs with each iteration\n",
    "+ **`seed`**:     random number seed to use for model initialization (optional)\n",
    "\n",
    "We'll train a topic model with 50 topics. (This will take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 50 topics\n",
    "n_topics = 27\n",
    "topic_model = ct.Corex(n_hidden=n_topics, \n",
    "                       words=words, \n",
    "                       max_iter=200, \n",
    "                       verbose=False, \n",
    "                       seed=1, \n",
    "                       docs = train_data['title'])\n",
    "topic_model.fit(doc_word, words=words,);\n",
    "print('model fit with',n_topics,'topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CorEx topic model provides functionality for easily accessing the topics. Let's take a look one of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ligament', 0.12565747836980273),\n",
       " ('cruciate', 0.09301666969080115),\n",
       " ('cruciate ligament', 0.0909861274986143),\n",
       " ('anterior cruciate', 0.07989143170087362),\n",
       " ('anterior cruciate ligament', 0.07959001712899003),\n",
       " ('anterior', 0.062028957848339686),\n",
       " ('ligament reconstruction', 0.05385770791021663),\n",
       " ('reconstruction', 0.049339688676797264),\n",
       " ('cruciate ligament reconstruction', 0.04290597857508636),\n",
       " ('bundle', 0.012307634420431373)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_model.get_topics(topic=0, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic words are those with the highest *mutual information* with the topic, rather than those with highest probability within the topic as in LDA. The mutual information with the topic is the number reported in each tuple. Theoretically, mutual information is always positive. If the CorEx output returns a negative mutual information from **`get_topics()`**, then the absolute value of that quantity is the mutual information between the topic and the *absence* of that word.\n",
    "\n",
    "If the column labels have not been specified through **`words`**, then the code will return the column indices for the top words in each topic.\n",
    "\n",
    "We can also retrieve all of the topics at once if we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ligament,cruciate,cruciate ligament,anterior cruciate,anterior cruciate ligament,anterior,ligament reconstruction,reconstruction,cruciate ligament reconstruction,bundle\n",
      "1: fixation,fractures,screw,plate,pedicle,locking,suture,screws,screw fixation,distal\n",
      "2: cord,spinal cord,spinal,cord injury,spinal cord injury,injury,stem,stem cells,cells,articular cartilage\n",
      "3: finite,element,finite element,element analysis,finite element analysis,lumbar,spine,total,arthroplasty,cervical\n",
      "4: gait,walking,lower,limb,running,kinematics,lower limb,knee,foot,ankle\n",
      "5: implant,implants,dental,stress,stress distribution,defect,titanium,distribution,dental implants,porous\n",
      "6: rotator,cuff,rotator cuff,parkinson,parkinson disease,reaction,ground,palsy,cerebral palsy,ground reaction\n",
      "7: specific,aortic,abdominal,patient specific,patient,wall,cancer,abdominal aortic,aneurysm,subject\n",
      "8: bone,cortical bone,marrow,femoral,bone marrow,cortical,density,tendon bone,mineral,femoral neck\n",
      "9: evolution,morphology,implications,feeding,primates,forelimb,shape,species,obstacle,evolutionary\n",
      "10: healing,randomized,trial,randomized controlled,controlled,controlled trial,fracture healing,randomized controlled trial,bone healing,cross linking\n",
      "11: training,risk,jump,physical,female,landing,athletes,risk factors,factors,sectional\n",
      "12: review,systematic,systematic review,shoulder,meta,meta analysis,shoulder arthroplasty,review meta,systematic review meta,literature\n",
      "13: cell,stimulation,stem cell,optical coherence,coherence,bone loss,optical,electrical,stromal,differentiation\n",
      "14: robot,control,robotic,design,robots,powered,inspired,legged,humanoid,biped\n",
      "15: osteoarthritis,term,pain,knee osteoarthritis,achilles,long term,achilles tendon,long,patients,tendon\n",
      "16: corneal,measurement,inertial,sensor,based,intraocular,intraocular pressure,corneal properties,method,glaucoma\n",
      "17: head,players,time,stroke,motor,real,impact,football,real time,nerve\n",
      "18: rats,treadmill,induced,exercise,mice,neurons,mouse,ovariectomized,treadmill walking,mouse model\n",
      "19: resonance,computed,image,magnetic resonance,tomography,imaging,trabecular,computed tomography,magnetic,trabecular bone\n",
      "20: mechanical,properties,human,mechanical properties,properties human,meniscus,tensile,structural,viscoelastic,elastic\n",
      "21: model,tissue,computational,collagen,rabbit,soft,soft tissue,numerical,experimental,fiber\n",
      "22: muscle,body,muscle activity,activation,range,muscle activation,skeletal muscle,range motion,activity,swimming\n",
      "23: disc,intervertebral,intervertebral disc,valve,heart,degeneration,animal,left,disc degeneration,animal model\n",
      "24: clinical,flexion,contact,outcomes,year,patellar,medial,patellofemoral,follow,prospective\n",
      "25: comparison,tears,lateral,technique,cement,radial,comparison different,wedge,glenohumeral,surgical\n",
      "26: different,healthy,visual,work,postural,associated,effect different,healthy subjects,sitting,evaluation different\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic for the newsgroup data tends to be less coherent than expected because of encodings and other oddities in the newsgroups data.  \n",
    "\n",
    "We can also get the column indices instead of the column labels if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8978, 0.05328271639472043),\n",
       " (9013, 0.027634555214009572),\n",
       " (4509, 0.019933710042152365),\n",
       " (16900, 0.019347739250012),\n",
       " (16904, 0.014464403463662854),\n",
       " (4381, 0.011488413745435036),\n",
       " (18144, 0.011181573806631597),\n",
       " (5178, 0.010635286835666334),\n",
       " (4513, 0.009754625693297927),\n",
       " (13193, 0.008446275292428968)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topics(topic=5, n_words=10, print_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to directly access the topic assignments for each word, they can be accessed through **`cluster`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 7 7 ... 1 5 5]\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.clusters)\n",
    "print(topic_model.clusters.shape) # m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the topic words, the most probable documents per topic can also be easily accessed. Documents are sorted according to log probabilities which is why the highest probability documents have a score of 0 ($e^0 = 1$) and other documents have negative scores (for example, $e^{-0.5} \\approx 0.6$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\n",
      "\n",
      "Papers for topic 0\n",
      "20496 anterolateral ligament reconstruction required reconstructed knees associated injury anterolateral structures robotic analysis rotational knee stability \n",
      "\n",
      "18980 wrist finger flexor muscles olive baboons papio anubis \n",
      "\n",
      "1506 strategy enhance artificial ligament graft bone tunnel \n",
      "\n",
      "906 differences incidence anterior cruciate ligament medial collateral ligament meniscal injuries collegiate high school sports \n",
      "\n",
      "24796 gait strategy patients ehlers danlos syndrome hypermobility type syndrome \n",
      "\n",
      "26284 comparison graft length changes knee motion different anatomic single bundle anterior cruciate ligament reconstruction approaches biomechanical study \n",
      "\n",
      "20130 prevention anterior cruciate ligament injuries sports systematic review risk factors male athletes \n",
      "\n",
      "7552 vivo length patterns medial collateral ligament stance phase gait \n",
      "\n",
      "27404 vitro comparison cortical cortico cancellous femoral suspension devices anterior cruciate ligament reconstruction implications mobilization \n",
      "\n",
      "27408 biomechanical comparison dynamic condylar screw locking compression plate fixation unstable distal femoral fractures vitro study \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_n = 0\n",
    "indx = topic_model.get_top_docs(topic=topic_n, n_docs=10, sort_by='log_prob')\n",
    "print(\"\\nPapers for topic\",topic_n)\n",
    "for i in indx:\n",
    "    print(i[0],train_data['title'].iloc[i[0]],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorEx is a *discriminative* model, whereas LDA is a *generative* model. This means that while LDA outputs a probability distribution over each document, CorEx instead estimates the probability a document belongs to a topic given that document's words. As a result, the probabilities across topics for a given document do not have to add up to 1. The estimated probabilities of topics for each document can be accessed through **`log_p_y_given_x`** or **`p_y_given_x`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28213, 27)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.p_y_given_x.shape) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a softmax to make a binary determination of which documents belong to each topic. These softmax labels can be accessed through **`labels`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28213, 27)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.labels.shape) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.273797</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.052003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.059198</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.998752</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.065485</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>0.011509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.024435</td>\n",
       "      <td>0.035609</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.011538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.821368</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.015760</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.074238</td>\n",
       "      <td>0.011396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.096989</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.011354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.994823</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.422736</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.011417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.851093</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.005913</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>0.011608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.443976</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536492</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007270</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.046867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.109781</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.036529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.011388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.043235</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.535348</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.074948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.039811</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.011438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.069590</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.051253</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.074216</td>\n",
       "      <td>0.166529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.031820</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.093502</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.164870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060854</td>\n",
       "      <td>0.787944</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.011336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.088998</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.011459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.011452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.073948</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.011331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.011447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.005395</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.061095</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.011490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.173506</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.033217</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.011619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.108610</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.036984</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.986944</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.011449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.454803</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.011506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28183</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28184</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.867006</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>0.011471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28185</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.659502</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.011376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28186</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.166313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28187</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.011463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28188</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28189</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28190</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.054084</td>\n",
       "      <td>0.042715</td>\n",
       "      <td>0.406204</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28191</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.254114</td>\n",
       "      <td>0.969641</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.011321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.128158</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.063798</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.011394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28193</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.044265</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.011425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28194</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.026719</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.182114</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.053851</td>\n",
       "      <td>0.011365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28195</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.011512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28196</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.028408</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>0.011470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28197</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.793219</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.021537</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.837390</td>\n",
       "      <td>0.011432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28198</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.082436</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>0.057674</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.426072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28199</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.048910</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.114264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28200</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>0.084923</td>\n",
       "      <td>0.004893</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.011099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28201</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.991031</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.011379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28202</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.010547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034890</td>\n",
       "      <td>0.055994</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.028469</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.038979</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.742840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28203</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017611</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.058602</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.175751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28204</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.044855</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.114929</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>0.011462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28205</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.951328</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.011499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28206</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.996393</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.011434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28207</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.012536</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.105431</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.011405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28208</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.019511</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.081725</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.123440</td>\n",
       "      <td>0.459529</td>\n",
       "      <td>0.011507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28209</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.227302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027140</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.607586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28210</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.100059</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.010847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28211</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019829</td>\n",
       "      <td>0.072324</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.035609</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.006928</td>\n",
       "      <td>0.011575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28212</th>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.074142</td>\n",
       "      <td>0.011562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28213 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.000061  0.000191  0.000098  0.000176  0.999999  0.002217  0.001612   \n",
       "1      0.000061  0.000191  0.000098  0.999999  0.003139  0.999999  0.001612   \n",
       "2      0.000061  0.000191  0.000098  0.000176  0.998752  0.000427  0.044767   \n",
       "3      0.000061  0.000191  0.000098  0.000176  0.060692  0.000427  0.001612   \n",
       "4      0.000061  0.000191  0.003638  0.000176  0.012982  0.000427  0.001612   \n",
       "5      0.000061  0.999999  0.000098  0.000176  0.003139  0.011356  0.999999   \n",
       "6      0.000061  0.000191  0.000098  0.001049  0.000365  0.000427  0.001612   \n",
       "7      0.000061  0.000191  0.000098  0.000176  0.096989  0.000427  0.001612   \n",
       "8      0.000061  0.000191  0.994823  0.000176  0.003139  0.000427  0.001612   \n",
       "9      0.000163  0.000191  0.000098  0.000176  0.999999  0.000427  0.851093   \n",
       "10     0.010005  0.000191  0.000098  0.000176  0.443976  0.000427  0.001612   \n",
       "11     0.000061  0.109781  0.000258  0.000176  0.003139  0.000427  0.001612   \n",
       "12     0.000061  0.000191  0.999999  0.000176  0.003139  0.000427  0.001612   \n",
       "13     0.000061  0.000258  0.000098  0.000176  0.003139  0.000427  0.001612   \n",
       "14     0.000061  0.000001  0.000098  0.001049  0.014149  0.000427  0.001612   \n",
       "15     0.069590  0.001887  0.000098  0.000176  0.051253  0.000427  0.001612   \n",
       "16     0.000061  0.000191  0.000098  0.000176  0.999999  0.000427  0.999999   \n",
       "17     0.000061  0.005084  0.000098  0.000176  0.018763  0.000427  0.001612   \n",
       "18     0.000061  0.000191  0.999999  0.004700  0.999999  0.000427  0.001612   \n",
       "19     0.000061  0.999999  0.000339  0.000176  0.000365  0.000427  0.001612   \n",
       "20     0.000061  0.999999  0.000098  0.000619  0.003139  0.000427  0.001612   \n",
       "21     0.000061  0.000191  0.000098  0.999999  0.003139  0.007983  0.001612   \n",
       "22     0.000061  0.002533  0.000098  0.000176  0.037713  0.000427  0.001612   \n",
       "23     0.000061  0.999999  0.000098  0.000176  0.003139  0.000427  0.001612   \n",
       "24     0.000061  0.000191  0.000098  0.000176  0.999999  0.000427  0.001612   \n",
       "25     0.000061  0.000191  0.000098  0.000176  0.999999  0.000427  0.001612   \n",
       "26     0.000061  0.000191  0.000098  0.000176  0.173506  0.000427  0.001612   \n",
       "27     0.000061  0.000191  0.026648  0.000176  0.003139  0.000427  0.001612   \n",
       "28     0.999999  0.986944  0.000098  0.007368  0.003139  0.000427  0.001612   \n",
       "29     0.000061  0.000191  0.003638  0.000176  0.003139  0.000427  0.001612   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28183  0.000061  0.000018  0.000098  0.000176  0.003139  0.000427  0.001612   \n",
       "28184  0.000061  0.000191  0.000098  0.000791  0.003139  0.999999  0.001612   \n",
       "28185  0.000061  0.999999  0.000098  0.999999  0.000365  0.000427  0.001612   \n",
       "28186  0.000061  0.999999  0.999999  0.041923  0.003139  0.000427  0.001612   \n",
       "28187  0.000061  0.000001  0.000098  0.000176  0.999999  0.000427  0.001612   \n",
       "28188  0.000061  0.999999  0.999999  0.001484  0.003139  0.000427  0.001612   \n",
       "28189  0.000061  0.000191  0.000619  0.000176  0.060692  0.002407  0.001612   \n",
       "28190  0.000061  0.000001  0.000098  0.000176  0.999989  0.000427  0.001612   \n",
       "28191  0.000061  0.000191  0.000098  0.999999  0.000001  0.000427  0.001612   \n",
       "28192  0.000061  0.000191  0.000098  0.000176  0.003139  0.000427  0.999999   \n",
       "28193  0.000061  0.000191  0.999994  0.000176  0.000365  0.002293  0.001612   \n",
       "28194  0.000061  0.000191  0.000098  0.000675  0.000008  0.026719  0.001612   \n",
       "28195  0.000061  0.000191  0.000098  0.000176  0.003139  0.002293  0.027000   \n",
       "28196  0.000061  0.000638  0.000098  0.000176  0.003139  0.000427  0.028408   \n",
       "28197  0.000061  0.999999  0.000098  0.000176  0.003139  0.793219  0.001612   \n",
       "28198  0.000061  0.000191  0.000098  0.000176  0.000001  0.000427  0.001612   \n",
       "28199  0.000061  0.999967  0.000098  0.000363  0.003139  0.048910  0.001612   \n",
       "28200  0.000061  0.000191  0.000098  0.006774  0.015748  0.999999  0.999999   \n",
       "28201  0.000061  0.000191  0.000098  0.000176  0.991031  0.000427  0.006566   \n",
       "28202  0.000061  0.000001  0.000098  0.000176  0.003139  0.000427  0.001612   \n",
       "28203  0.000061  0.000191  0.000098  0.000176  0.003139  0.000450  0.001612   \n",
       "28204  0.000061  0.000191  0.999999  0.000176  0.003139  0.000427  0.999999   \n",
       "28205  0.999999  0.000021  0.000098  0.000176  0.999999  0.000427  0.001612   \n",
       "28206  0.000061  0.000191  0.000098  0.000176  0.003139  0.000427  0.001612   \n",
       "28207  0.000061  0.000191  0.002221  0.000176  0.012536  0.242800  0.001612   \n",
       "28208  0.000061  0.000191  0.000098  0.000176  0.999999  0.000427  0.001612   \n",
       "28209  0.000061  0.000018  0.999999  0.000176  0.003139  0.000427  0.999999   \n",
       "28210  0.000061  0.000001  0.000098  0.000176  0.000001  0.003150  0.001612   \n",
       "28211  0.000061  0.000191  0.000098  0.000176  0.060692  0.000427  0.001612   \n",
       "28212  0.001985  0.000005  0.000098  0.000176  0.003139  0.000427  0.001612   \n",
       "\n",
       "             7         8         9   ...        17        18        19  \\\n",
       "0      0.000969  0.000491  0.001293  ...  0.002953  0.006247  0.000943   \n",
       "1      0.000969  0.000491  0.004326  ...  0.002953  0.006248  0.000944   \n",
       "2      0.000969  0.000491  0.001294  ...  0.002953  0.006247  0.000944   \n",
       "3      0.000969  0.000491  0.001294  ...  0.002953  0.033220  0.006647   \n",
       "4      0.000969  0.000491  0.999999  ...  0.002953  0.821368  0.000944   \n",
       "5      0.000969  0.999999  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "6      0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.999999   \n",
       "7      0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "8      0.059000  0.000491  0.005063  ...  0.002953  0.006248  0.422736   \n",
       "9      0.000969  0.000491  0.999999  ...  0.002953  0.006248  0.017903   \n",
       "10     0.000969  0.000491  0.001294  ...  0.536492  0.006248  0.000944   \n",
       "11     0.000969  0.999999  0.012021  ...  0.064103  0.029789  0.000944   \n",
       "12     0.000969  0.004471  0.005063  ...  0.002953  0.023858  0.000944   \n",
       "13     0.043235  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "14     0.000969  0.999999  0.074948  ...  0.002953  0.006248  0.000944   \n",
       "15     0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "16     0.999997  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "17     0.000969  0.031820  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "18     0.000969  0.000491  0.005063  ...  0.060854  0.787944  0.000944   \n",
       "19     0.088998  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "20     0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "21     0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "22     0.000969  0.999644  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "23     0.000969  0.023242  0.000015  ...  0.002953  0.999999  0.000944   \n",
       "24     0.000969  0.000491  0.001294  ...  0.002953  0.006247  0.000944   \n",
       "25     0.999999  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "26     0.000969  0.000491  0.001294  ...  0.002953  0.033217  0.000944   \n",
       "27     0.000969  0.011060  0.001294  ...  0.999999  0.108610  0.000944   \n",
       "28     0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "29     0.000969  0.000491  0.000015  ...  0.002953  0.999999  0.000944   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28183  0.000969  0.000491  0.005215  ...  0.002952  0.006248  0.000945   \n",
       "28184  0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.999999   \n",
       "28185  0.000969  0.999998  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28186  0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28187  0.000969  0.000491  0.010710  ...  0.002953  0.006208  0.000944   \n",
       "28188  0.000969  0.000491  0.000256  ...  0.002953  0.006248  0.000944   \n",
       "28189  0.000969  0.000491  0.001294  ...  0.002953  0.999999  0.000944   \n",
       "28190  0.000969  0.000491  0.999997  ...  0.002953  0.006248  0.000944   \n",
       "28191  0.999999  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28192  0.999999  0.000491  0.001294  ...  0.002953  0.006247  0.000944   \n",
       "28193  0.000969  0.003785  0.001294  ...  0.002953  0.044265  0.000944   \n",
       "28194  0.000969  0.182114  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28195  0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28196  0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28197  0.000969  0.999999  0.001294  ...  0.002953  0.006250  0.021537   \n",
       "28198  0.000969  0.000491  0.001294  ...  0.002953  0.082436  0.000944   \n",
       "28199  0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28200  0.000970  0.000491  0.001294  ...  0.002953  0.006241  0.999999   \n",
       "28201  0.000969  0.000491  0.001294  ...  0.002953  0.006230  0.000944   \n",
       "28202  0.000969  0.003422  0.010547  ...  0.034890  0.055994  0.000944   \n",
       "28203  0.000969  0.000491  0.001294  ...  0.017611  0.006248  0.000944   \n",
       "28204  0.044855  0.000491  0.001294  ...  0.002953  0.999999  0.000944   \n",
       "28205  0.999999  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28206  0.000969  0.000491  0.001294  ...  0.002953  0.006248  0.000944   \n",
       "28207  0.000969  0.000491  0.076412  ...  0.002953  0.006248  0.000944   \n",
       "28208  0.000969  0.003984  0.001294  ...  0.002953  0.019511  0.000944   \n",
       "28209  0.999999  0.000491  0.227302  ...  0.027140  0.006248  0.000944   \n",
       "28210  0.000969  0.000491  0.001294  ...  0.002952  0.006248  0.000945   \n",
       "28211  0.000969  0.000491  0.001294  ...  0.019829  0.072324  0.000944   \n",
       "28212  0.000969  0.000490  0.001294  ...  0.002953  0.033208  0.000944   \n",
       "\n",
       "             20        21        22        23        24        25        26  \n",
       "0      0.005894  0.273797  0.004897  0.001529  0.005258  0.005929  0.052003  \n",
       "1      0.059198  0.007335  0.004896  0.001529  0.005369  0.006436  0.999999  \n",
       "2      0.005917  0.065485  0.004896  0.012090  0.999999  0.006423  0.011509  \n",
       "3      0.005918  0.024435  0.035609  0.001529  0.005363  0.006405  0.011538  \n",
       "4      0.005916  0.007335  0.015760  0.001529  0.005369  0.006436  0.011462  \n",
       "5      0.005916  0.007335  0.004896  0.001529  0.005369  0.074238  0.011396  \n",
       "6      0.005916  0.007335  0.004896  0.001529  0.005369  0.006439  0.011461  \n",
       "7      0.005916  0.057811  0.004895  0.001529  0.005369  0.006427  0.011354  \n",
       "8      0.999999  0.007335  0.004896  0.001529  0.005369  0.006435  0.011417  \n",
       "9      0.005913  0.007332  0.195478  0.001529  0.005364  0.006376  0.011608  \n",
       "10     0.005916  0.007270  0.004896  0.001529  0.005370  0.006411  0.046867  \n",
       "11     0.000009  0.057687  0.004896  0.001529  0.005369  0.006436  0.011463  \n",
       "12     0.999999  0.007335  0.004896  0.036529  0.005369  0.006433  0.011388  \n",
       "13     0.999999  0.007335  0.004896  0.001529  0.535348  0.006436  0.011455  \n",
       "14     0.005917  0.007335  0.039811  0.001529  0.005367  0.006446  0.011438  \n",
       "15     0.005916  0.007335  0.068848  0.001529  0.005370  0.006428  0.011436  \n",
       "16     0.005916  0.007335  0.215001  0.001529  0.005369  0.074216  0.166529  \n",
       "17     0.005916  0.999999  0.093502  0.001529  0.005369  0.006466  0.164870  \n",
       "18     0.005916  0.007335  0.014569  0.001529  0.005369  0.006434  0.011336  \n",
       "19     0.999999  0.007335  0.004896  0.001529  0.005369  0.999999  0.011459  \n",
       "20     0.005916  0.000001  0.004895  0.001529  0.005370  0.006439  0.011452  \n",
       "21     0.073948  0.057687  0.004896  0.999999  0.005369  0.006436  0.011461  \n",
       "22     0.005916  0.007335  0.004896  0.001529  0.005351  0.006441  0.011331  \n",
       "23     0.005916  0.007335  0.004895  0.001529  0.005370  0.006439  0.011447  \n",
       "24     0.005909  0.007292  0.033535  0.001528  0.005395  0.006398  0.072524  \n",
       "25     0.005916  0.007335  0.061095  0.001529  0.005372  0.006433  0.011490  \n",
       "26     0.005907  0.999994  0.015797  0.001529  0.005385  0.006431  0.011619  \n",
       "27     0.036984  0.007335  0.004896  0.001529  0.005369  0.006436  0.011459  \n",
       "28     0.005916  0.007335  0.004896  0.001529  0.005369  0.006446  0.011449  \n",
       "29     0.005916  0.007334  0.454803  0.001529  0.005368  0.006383  0.011506  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "28183  0.020762  0.007342  0.004894  0.001531  0.005378  0.006450  0.011051  \n",
       "28184  0.867006  0.007335  0.004896  0.001529  0.005368  0.006432  0.011471  \n",
       "28185  0.659502  0.057687  0.004896  0.001529  0.005369  0.006473  0.011376  \n",
       "28186  0.999999  0.007335  0.004895  0.001529  0.005374  0.006559  0.166313  \n",
       "28187  0.005916  0.007335  0.004896  0.001529  0.005369  0.006435  0.011463  \n",
       "28188  0.006002  0.999999  0.028470  0.001529  0.005369  0.006462  0.999999  \n",
       "28189  0.005916  0.057811  0.004895  0.001529  0.005369  0.006427  0.011364  \n",
       "28190  0.054084  0.042715  0.406204  0.001529  0.005369  0.006436  0.011465  \n",
       "28191  0.254114  0.969641  0.004896  0.001529  0.005368  0.006413  0.011321  \n",
       "28192  0.128158  0.007337  0.015773  0.001529  0.063798  0.006442  0.011394  \n",
       "28193  0.005916  0.007344  0.004895  0.018324  0.005368  0.006433  0.011425  \n",
       "28194  0.005945  0.007336  0.004896  0.001529  0.005369  0.053851  0.011365  \n",
       "28195  0.005916  0.007335  0.004895  0.001527  0.005404  0.999999  0.011512  \n",
       "28196  0.005916  0.007335  0.004896  0.001529  0.005368  0.006432  0.011470  \n",
       "28197  0.005916  0.007335  0.004896  0.001529  0.005369  0.837390  0.011432  \n",
       "28198  0.005915  0.057674  0.004896  0.001529  0.005372  0.006409  0.426072  \n",
       "28199  0.999999  0.007334  0.004896  0.001529  0.005370  0.006444  0.114264  \n",
       "28200  0.323574  0.084923  0.004893  0.004069  0.005346  0.006329  0.011099  \n",
       "28201  0.005916  0.006978  0.004896  0.013771  0.005370  0.006430  0.011379  \n",
       "28202  0.005916  0.007331  0.028469  0.001529  0.038979  0.006389  0.742840  \n",
       "28203  0.058602  0.999999  0.004896  0.001529  0.005369  0.006435  0.175751  \n",
       "28204  0.005916  0.007335  0.114929  0.001529  0.005369  0.006436  0.011462  \n",
       "28205  0.005916  0.007347  0.004896  0.001529  0.951328  0.006440  0.011499  \n",
       "28206  0.005916  0.007335  0.996393  0.001529  0.005369  0.006435  0.011434  \n",
       "28207  0.999999  0.105431  0.004896  0.001529  0.005368  0.006434  0.011405  \n",
       "28208  0.081725  0.007334  0.004895  0.001529  0.123440  0.459529  0.011507  \n",
       "28209  0.005923  0.007330  0.999999  0.001528  0.005361  0.006375  0.607586  \n",
       "28210  0.100059  0.007343  0.999999  0.001531  0.005370  0.032540  0.010847  \n",
       "28211  0.005918  0.007334  0.035609  0.001529  0.005362  0.006928  0.011575  \n",
       "28212  0.000005  0.999999  0.004896  0.001529  0.032941  0.074142  0.011562  \n",
       "\n",
       "[28213 rows x 27 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex_pred_data = pd.DataFrame(topic_model.p_y_given_x)\n",
    "corex_pred_data #title rows, column topic numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since CorEx does not prescribe a probability distribution of topics over each document, this means that a document could possibly belong to no topics (all 0's across topics in **`labels`**) or all topics (all 1's across topics in **`labels`**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Correlation and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total correlation is the measure which CorEx maximize when constructing the topic model. It can be accessed through **`tc`** and is reported in nats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.734384738435843"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection:** CorEx starts its algorithm with a random initialization, and so different runs can result in different topic models. One way of finding a better topic model is to restart the CorEx algorithm several times and take the run that has the highest TC value (i.e. the run that produces topics that are most informative about the documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall total correlation is the sum of the total correlation per each topic. These can be accessed through **`tcs`**. For an unsupervised CorEx topic model, the topics are always sorted from high to low according to their TC. For an anchored CorEx topic model, the topics are not sorted, and are outputted such that the anchored topics come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tcs.shape # k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.734384738435843\n",
      "10.734384738435843\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(topic_model.tcs))\n",
    "print(topic_model.tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting number of topics:** one way to choose the number of topics is to observe the distribution of TCs for each topic to see how much each additional topic contributes to the overall TC. We should keep adding topics until additional topics do not significantly contribute to the overall TC. This is similar to choosing a cutoff eigenvalue when doing topic modeling via LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the first topic is much more informative than the other topics. Given that we suspect that this topic is picking up on image encodings (as given by \"dsl\" and \"n3jxp\" in the topic) and other boilerplate text (as given by the high TC and lack of coherence of the rest of the topic), we could consider doing additional investigation and preprocessing to help ensure that the CorEx topic model does not pick up on these patterns which are not insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Document TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose total correlation further. The topic correlation is the average of the pointwise total correlations for each individual document. The pointwise total correlations can be accessed through **`log_z`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28213, 27)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.log_z.shape # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89898803 0.85852308 0.85643471 0.84136796 0.59416987 0.42346793\n",
      " 0.4200857  0.40726644 0.39585665 0.36759709 0.35938127 0.35783007\n",
      " 0.34417091 0.33393502 0.32090076 0.32050596 0.31306224 0.2979806\n",
      " 0.29788815 0.29276047 0.25131951 0.24903485 0.24767016 0.24749841\n",
      " 0.18888326 0.14522183 0.1025838 ]\n",
      "[0.89898803 0.85852308 0.85643471 0.84136796 0.59416987 0.42346793\n",
      " 0.4200857  0.40726644 0.39585665 0.36759709 0.35938127 0.35783007\n",
      " 0.34417091 0.33393502 0.32090076 0.32050596 0.31306224 0.2979806\n",
      " 0.29788815 0.29276047 0.25131951 0.24903485 0.24767016 0.24749841\n",
      " 0.18888326 0.14522183 0.1025838 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(topic_model.log_z, axis=0))\n",
    "print(topic_model.tcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pointwise total correlations in **`log_z`** represent the correlations within an individual document explained by a particular topic. These correlations have been used to measure how \"surprising\" documents are with respect to given topics (see references below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`labels`** attribute gives the binary topic expressions for each document and each topic. We can use this output as input to another CorEx topic model to get latent representations of the topics themselves. This yields a hierarchical CorEx topic model. Like the first layer of the topic model, one can determine the number of latent variables to add in higher layers through examination of the topic TCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n"
     ]
    }
   ],
   "source": [
    "# Train a second layer to the topic model\n",
    "tm_layer2 = ct.Corex(n_hidden=10)\n",
    "tm_layer2.fit(topic_model.labels);\n",
    "\n",
    "# Train a third layer to the topic model\n",
    "tm_layer3 = ct.Corex(n_hidden=1)\n",
    "tm_layer3.fit(tm_layer2.labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have `graphviz` installed, then you can output visualizations of the hierarchial topic model to your current working directory. One can also create custom visualizations of the hierarchy by properly making use of the **`labels`** attribute of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [options] data_file.csv \n",
      "Assume one document on each line.\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This module implements some visualizations based on CorEx representations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # to create visualizations on a display-less server\n",
    "import pylab\n",
    "import networkx as nx\n",
    "import textwrap\n",
    "import scipy.sparse as ss\n",
    "import sklearn.feature_extraction.text as skt\n",
    "#import cPickle, pickle # neither module is used, and cPickle is not part of Anaconda build, so commented for LF run\n",
    "import corextopic as ct\n",
    "import sys, traceback\n",
    "from time import time\n",
    "import re\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from nltk.stem.snowball import *\n",
    "pattern = '\\\\b[A-Za-z]+\\\\b'\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "class Corex(object):\n",
    "    def vis_rep(corex, data=None, row_label=None, column_label=None, prefix='topics'):\n",
    "        \"\"\"Various visualizations and summary statistics for a one layer representation\"\"\"\n",
    "        if column_label is None:\n",
    "            column_label = list(map(str, range(data.shape[1])))\n",
    "        if row_label is None:\n",
    "            row_label = list(map(str, range(corex.n_samples)))\n",
    "\n",
    "        alpha = corex.alpha\n",
    "\n",
    "        print('Print topics in text file')\n",
    "        output_groups(corex.tcs, alpha, corex.mis, column_label, corex.sign, prefix=prefix)\n",
    "        output_labels(corex.labels, row_label, prefix=prefix)\n",
    "        output_cont_labels(corex.p_y_given_x, row_label, prefix=prefix)\n",
    "        output_strong(corex.tcs, alpha, corex.mis, corex.labels, prefix=prefix)\n",
    "        anomalies(corex.log_z, row_label=row_label, prefix=prefix)\n",
    "        plot_convergence(corex.tc_history, prefix=prefix)\n",
    "        if data is not None:\n",
    "            plot_heatmaps(data, alpha, corex.mis, column_label, corex.p_y_given_x, prefix=prefix)\n",
    "\n",
    "\n",
    "    def vis_hierarchy(corexes, column_label=None, max_edges=100, prefix='topics', n_anchors=0):\n",
    "        \"\"\"Visualize a hierarchy of representations.\"\"\"\n",
    "        if column_label is None:\n",
    "            column_label = list(map(str, range(corexes[0].alpha.shape[1])))\n",
    "\n",
    "        # make l1 label\n",
    "        alpha = corexes[0].alpha\n",
    "        mis = corexes[0].mis\n",
    "        l1_labels = []\n",
    "        annotate = lambda q, s: q if s > 0 else '~' + q\n",
    "        for j in range(corexes[0].n_hidden):\n",
    "            # inds = np.where(alpha[j] * mis[j] > 0)[0]\n",
    "            inds = np.where(alpha[j] >= 1.)[0]\n",
    "            inds = inds[np.argsort(-alpha[j, inds] * mis[j, inds])]\n",
    "            group_number = str('red_') + str(j) if j < n_anchors else str(j)\n",
    "            label = group_number + ':' + ' '.join([annotate(column_label[ind], corexes[0].sign[j,ind]) for ind in inds[:6]])\n",
    "            label = textwrap.fill(label, width=25)\n",
    "            l1_labels.append(label)\n",
    "\n",
    "        # Construct non-tree graph\n",
    "        weights = [corex.alpha.clip(0, 1) * corex.mis for corex in corexes[1:]]\n",
    "        node_weights = [corex.tcs for corex in corexes[1:]]\n",
    "        g = make_graph(weights, node_weights, l1_labels, max_edges=max_edges)\n",
    "\n",
    "        # Display pruned version\n",
    "        h = g.copy()  # trim(g.copy(), max_parents=max_parents, max_children=max_children)\n",
    "        edge2pdf(h, prefix + '/graphs/graph_prune_' + str(max_edges), labels='label', directed=True, makepdf=True)\n",
    "\n",
    "        # Display tree version\n",
    "        tree = g.copy()\n",
    "        tree = trim(tree, max_parents=1, max_children=False)\n",
    "        edge2pdf(tree, prefix + '/graphs/tree', labels='label', directed=True, makepdf=True)\n",
    "\n",
    "        # Output JSON files\n",
    "        try:\n",
    "            import os\n",
    "            #copyfile(os.path.dirname(os.path.realpath(__file__)) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "            print(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "            copyfile(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "        except:\n",
    "            print(\"Couldn't find 'force.html' file for visualizing d3 output\")\n",
    "        import json\n",
    "        from networkx.readwrite import json_graph\n",
    "\n",
    "        mapping = dict([(n, tree.node[n].get('label', str(n))) for n in tree.nodes()])\n",
    "        tree = nx.relabel_nodes(tree, mapping)\n",
    "        json.dump(json_graph.node_link_data(tree), safe_open(prefix + '/graphs/force.json', 'w+'))\n",
    "        json.dump(json_graph.node_link_data(h), safe_open(prefix + '/graphs/force_nontree.json', 'w+'))\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "    def plot_heatmaps(data, alpha, mis, column_label, cont, topk=40, athresh=0.2, prefix=''):\n",
    "        import seaborn as sns\n",
    "        cmap = sns.cubehelix_palette(as_cmap=True, light=.9)\n",
    "        import matplotlib.pyplot as plt\n",
    "        m, nv = mis.shape\n",
    "        for j in range(m):\n",
    "            inds = np.where(np.logical_and(alpha[j] > athresh, mis[j] > 0.))[0]\n",
    "            inds = inds[np.argsort(- alpha[j, inds] * mis[j, inds])][:topk]\n",
    "            if len(inds) >= 2:\n",
    "                plt.clf()\n",
    "                order = np.argsort(cont[:,j])\n",
    "                if type(data) == np.ndarray:\n",
    "                    subdata = data[:, inds][order].T\n",
    "                else:\n",
    "                    # assume sparse\n",
    "                    subdata = data[:, inds].toarray()\n",
    "                    subdata = subdata[order].T\n",
    "                columns = [column_label[i] for i in inds]\n",
    "                fig, ax = plt.subplots(figsize=(20, 10))\n",
    "                sns.heatmap(subdata, vmin=0, vmax=1, cmap=cmap, yticklabels=columns, xticklabels=False, ax=ax, cbar_kws={\"ticks\": [0, 0.5, 1]})\n",
    "                plt.yticks(rotation=0)\n",
    "                filename = '{}/heatmaps/group_num={}.png'.format(prefix, j)\n",
    "                if not os.path.exists(os.path.dirname(filename)):\n",
    "                    os.makedirs(os.path.dirname(filename))\n",
    "                plt.title(\"Latent factor {}\".format(j))\n",
    "                plt.savefig(filename, bbox_inches='tight')\n",
    "                plt.close('all')\n",
    "                #plot_rels(data[:, inds], map(lambda q: column_label[q], inds), colors=cont[:, j],\n",
    "                #          outfile=prefix + '/relationships/group_num=' + str(j), latent=labels[:, j], alpha=0.1)\n",
    "\n",
    "\n",
    "    def make_graph(weights, node_weights, column_label, max_edges=100):\n",
    "        all_edges = np.hstack(list(map(np.ravel, weights)))\n",
    "        max_edges = min(max_edges, len(all_edges))\n",
    "        w_thresh = np.sort(all_edges)[-max_edges]\n",
    "        print('weight threshold is %f for graph with max of %f edges ' % (w_thresh, max_edges))\n",
    "        g = nx.DiGraph()\n",
    "        max_node_weight = max([max(w) for w in node_weights])\n",
    "        for layer, weight in enumerate(weights):\n",
    "            m, n = weight.shape\n",
    "            for j in range(m):\n",
    "                g.add_node((layer + 1, j))\n",
    "                g.node[(layer + 1, j)]['weight'] = 0.3 * node_weights[layer][j] / max_node_weight\n",
    "                for i in range(n):\n",
    "                    if weight[j, i] > w_thresh:\n",
    "                        if weight[j, i] > w_thresh / 2:\n",
    "                            g.add_weighted_edges_from([( (layer, i), (layer + 1, j), 10 * weight[j, i])])\n",
    "                        else:\n",
    "                            g.add_weighted_edges_from([( (layer, i), (layer + 1, j), 0)])\n",
    "\n",
    "        # Label layer 0\n",
    "        for i, lab in enumerate(column_label):\n",
    "            g.add_node((0, i))\n",
    "            g.node[(0, i)]['label'] = lab\n",
    "            g.node[(0, i)]['name'] = lab  # JSON uses this field\n",
    "            g.node[(0, i)]['weight'] = 1\n",
    "        return g\n",
    "\n",
    "\n",
    "    def trim(g, max_parents=False, max_children=False):\n",
    "        for node in g:\n",
    "            if max_parents:\n",
    "                parents = list(g.successors(node))\n",
    "                #weights = [g.edge[node][parent]['weight'] for parent in parents]\n",
    "                weights = [g.adj[node][parent]['weight'] for parent in parents]\n",
    "                for weak_parent in np.argsort(weights)[:-max_parents]:\n",
    "                    g.remove_edge(node, parents[weak_parent])\n",
    "            if max_children:\n",
    "                children = g.predecessors(node)\n",
    "                weights = [g.edge[child][node]['weight'] for child in children]\n",
    "                for weak_child in np.argsort(weights)[:-max_children]:\n",
    "                    g.remove_edge(children[weak_child], node)\n",
    "        return g\n",
    "\n",
    "\n",
    "    def output_groups(tcs, alpha, mis, column_label, direction, thresh=0, prefix=''):\n",
    "        f = safe_open(prefix + '/groups.txt', 'w+')\n",
    "        h = safe_open(prefix + '/topics.txt', 'w+')\n",
    "        m, nv = mis.shape\n",
    "        annotate = lambda q, s: q if s >= 0 else '~' + q\n",
    "        for j in range(m):\n",
    "            f.write('Group num: %d, TC(X;Y_j): %0.3f\\n' % (j, tcs[j]))\n",
    "            # inds = np.where(alpha[j] * mis[j] > thresh)[0]\n",
    "            inds = np.where(alpha[j] >= 1.)[0]\n",
    "            inds = inds[np.argsort(-alpha[j, inds] * mis[j, inds])]\n",
    "            for ind in inds:\n",
    "                f.write(column_label[ind] + u', %0.3f, %0.3f, %0.3f\\n' % (\n",
    "                    mis[j, ind], alpha[j, ind], mis[j, ind] * alpha[j, ind]))\n",
    "            #h.write(unicode(j) + u':' + u','.join([annotate(column_label[ind], direction[j,ind]) for ind in inds[:10]]) + u'\\n')\n",
    "            h.write(str(j) + u':' + u','.join(\n",
    "                [annotate(column_label[ind], direction[j, ind]) for ind in inds[:10]]) + u'\\n')\n",
    "        f.close()\n",
    "        h.close()\n",
    "\n",
    "\n",
    "    def output_labels(labels, row_label, prefix=''):\n",
    "        f = safe_open(prefix + '/labels.txt', 'w+')\n",
    "        ns, m = labels.shape\n",
    "        for l in range(ns):\n",
    "            f.write(row_label[l] + ',' + ','.join(list(map(lambda q: '%d' % q, labels[l, :])))+ '\\n')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def output_cont_labels(p_y_given_x, row_label, prefix=''):\n",
    "        f = safe_open(prefix + '/cont_labels.txt', 'w+')\n",
    "        ns, m = p_y_given_x.shape\n",
    "        for l in range(ns):\n",
    "            f.write(row_label[l] + ',' + ','.join(list(map(lambda q: '{:.10f}'.format(q), np.log(p_y_given_x[l, :])))) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def output_strong(tcs, alpha, mis, labels, prefix=''):\n",
    "        f = safe_open(prefix + '/most_deterministic_groups.txt', 'w+')\n",
    "        m, n = alpha.shape\n",
    "        topk = 5\n",
    "        ixy = np.clip(np.sum(alpha * mis, axis=1) - tcs, 0, np.inf)\n",
    "        hys = np.array([entropy(labels[:, j]) for j in range(m)]).clip(1e-6)\n",
    "        ntcs = [(np.sum(np.sort(alpha[j] * mis[j])[-topk:]) - ixy[j]) / ((topk - 1) * hys[j]) for j in range(m)]\n",
    "\n",
    "        f.write('Group num., NTC\\n')\n",
    "        for j, ntc in sorted(enumerate(ntcs), key=lambda q: -q[1]):\n",
    "            f.write('%d, %0.3f\\n' % (j, ntc))\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def anomalies(log_z, row_label=None, prefix=''):\n",
    "        from scipy.special import erf\n",
    "\n",
    "        ns = log_z.shape[0]\n",
    "        if row_label is None:\n",
    "            row_label = list(map(str, range(ns)))\n",
    "        a_score = np.sum(log_z[:, :], axis=1)\n",
    "        mean, std = np.mean(a_score), np.std(a_score)\n",
    "        a_score = (a_score - mean) / std\n",
    "        percentile = 1. / ns\n",
    "        anomalies = np.where(0.5 * (1 - erf(a_score / np.sqrt(2)) ) < percentile)[0]\n",
    "        f = safe_open(prefix + '/anomalies.txt', 'w+')\n",
    "        for i in anomalies:\n",
    "            f.write(row_label[i] + ', %0.1f\\n' % a_score[i])\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    # Utilities\n",
    "    # IT UTILITIES\n",
    "    def entropy(xsamples):\n",
    "        # sample entropy for one discrete var\n",
    "        xsamples = np.asarray(xsamples)\n",
    "        xsamples = xsamples[xsamples >= 0]  # by def, -1 means missing value\n",
    "        xs = np.unique(xsamples)\n",
    "        ns = len(xsamples)\n",
    "        ps = np.array([float(np.count_nonzero(xsamples == x)) / ns for x in xs])\n",
    "        return -np.sum(ps * np.log(ps))\n",
    "\n",
    "\n",
    "    def safe_open(filename, mode):\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        return codecs.open(filename, mode, \"utf-8\")\n",
    "\n",
    "\n",
    "    # Visualization utilities\n",
    "\n",
    "    def neato(fname, position=None, directed=False):\n",
    "        if directed:\n",
    "            os.system(\n",
    "                \"sfdp \" + fname + \".dot -Tpdf -Earrowhead=none -Nfontsize=16  -GK=2 -Gmaxiter=1000 -Goverlap=False -Gpack=True -Gpackmode=clust -Gsep=0.01 -Gsplines=False -o \" + fname + \"_sfdp.pdf\")\n",
    "            os.system(\n",
    "                \"sfdp \" + fname + \".dot -Tpdf -Earrowhead=none -Nfontsize=16  -GK=2 -Gmaxiter=1000 -Goverlap=False -Gpack=True -Gpackmode=clust -Gsep=0.01 -Gsplines=True -o \" + fname + \"_sfdp_w_splines.pdf\")\n",
    "            return True\n",
    "        if position is None:\n",
    "            os.system(\"neato \" + fname + \".dot -Tpdf -o \" + fname + \".pdf\")\n",
    "            os.system(\"fdp \" + fname + \".dot -Tpdf -o \" + fname + \"fdp.pdf\")\n",
    "        else:\n",
    "            os.system(\"neato \" + fname + \".dot -Tpdf -n -o \" + fname + \".pdf\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def extract_color(label):\n",
    "        import matplotlib\n",
    "\n",
    "        colors = matplotlib.colors.cnames.keys()\n",
    "        parts = label.split('_')\n",
    "        for part in parts:\n",
    "            if part in colors:\n",
    "                parts.remove(part)\n",
    "                return '_'.join(parts), part\n",
    "        return label, 'black'\n",
    "\n",
    "\n",
    "    def edge2pdf(g, filename, threshold=0, position=None, labels=None, connected=True, directed=False, makepdf=True):\n",
    "        #This function will takes list of edges and a filename\n",
    "        #and write a file in .dot format. Readable, eg. by omnigraffle\n",
    "        # OR use \"neato file.dot -Tpng -n -o file.png\"\n",
    "        # The -n option says whether to use included node positions or to generate new ones\n",
    "        # for a grid, positions = [(i%28,i/28) for i in range(784)]\n",
    "        def cnn(node):\n",
    "            #change node names for dot format\n",
    "            if type(node) is tuple or type(node) is list:\n",
    "                #return u'n' + u'_'.join(list(map(unicode, node)))\n",
    "                return u'n' + u'_'.join(list(map(str, node)))\n",
    "            else:\n",
    "                return node\n",
    "\n",
    "        if connected:\n",
    "            touching = list(set(sum([[a, b] for a, b in g.edges()], [])))\n",
    "            g = nx.subgraph(g, touching)\n",
    "            print('non-isolated nodes,edges', len(list(g.nodes())), len(list(g.edges())))\n",
    "        f = safe_open(filename + '.dot', 'w+')\n",
    "        #print('f1->',f)\n",
    "        #print('directed->',directed)\n",
    "        if directed:\n",
    "            f.write(\"strict digraph {\\n\")#.decode(\"utf-8\")\n",
    "            #f.write('strict digraph {')\n",
    "        else:\n",
    "            f.write(\"strict graph {\")\n",
    "        #f.write(\"\\tgraph [overlap=scale];\\n\".encode('utf-8'))\n",
    "        f.write(\"\\tnode [shape=point];\\n\")\n",
    "        for a, b, d in g.edges(data=True):\n",
    "            if 'weight' in d:\n",
    "                if directed:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -> ' + cnn(b) + ' [penwidth=%.2f' % float(\n",
    "                        np.clip(d['weight'], 0, 9)) + '];\\n'))\n",
    "                else:\n",
    "                    if d['weight'] > threshold:\n",
    "                        f.write((\"\\t\" + cnn(a) + ' -- ' + cnn(b) + ' [penwidth=' + str(3 * d['weight']) + '];\\n'))\n",
    "            else:\n",
    "                if directed:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -> ' + cnn(b) + ';\\n'))\n",
    "                else:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -- ' + cnn(b) + ';\\n'))\n",
    "        for n in g.nodes():\n",
    "            if labels is not None:\n",
    "                if type(labels) == dict or type(labels) == list:\n",
    "                    thislabel = labels[n].replace(u'\"', u'\\\\\"')\n",
    "                    lstring = u'label=\"' + thislabel + u'\",shape=none'\n",
    "                elif type(labels) == str:\n",
    "                    #if g.node[n].has_key('label'):\n",
    "                    if 'label' in g.node[n]:\n",
    "                        thislabel = g.node[n][labels].replace(u'\"', u'\\\\\"')\n",
    "                        # combine dupes\n",
    "                        #llist = thislabel.split(',')\n",
    "                        #thislabel = ','.join([l for l in set(llist)])\n",
    "                        thislabel, thiscolor = extract_color(thislabel)\n",
    "                        lstring = u'label=\"%s\",shape=none,fontcolor=\"%s\"' % (thislabel, thiscolor)\n",
    "                    else:\n",
    "                        weight = g.node[n].get('weight', 0.1)\n",
    "                        if n[0] == 1:\n",
    "                            lstring = u'shape=circle,margin=\"0,0\",style=filled,fillcolor=black,fontcolor=white,height=%0.2f,label=\"%d\"' % (\n",
    "                                2 * weight, n[1])\n",
    "                        else:\n",
    "                            lstring = u'shape=point,height=%0.2f' % weight\n",
    "                else:\n",
    "                    lstring = 'label=\"' + str(n) + '\",shape=none'\n",
    "                lstring = str(lstring)\n",
    "            else:\n",
    "                lstring = False\n",
    "            if position is not None:\n",
    "                if position == 'grid':\n",
    "                    position = [(i % 28, 28 - i / 28) for i in range(784)]\n",
    "                posstring = unicode('pos=\"' + str(position[n][0]) + ',' + str(position[n][1]) + '\"')\n",
    "            else:\n",
    "                posstring = False\n",
    "            finalstring = u' [' + u','.join([ts for ts in [posstring, lstring] if ts]) + u']\\n'\n",
    "            #finalstring = u' ['+lstring+u']\\n'\n",
    "            f.write(u'\\t' + cnn(n) + finalstring)\n",
    "        f.write(\"}\")\n",
    "        f.close()\n",
    "        if makepdf:\n",
    "            neato(filename, position=position, directed=directed)\n",
    "        return True\n",
    "\n",
    "\n",
    "    def predictable(out, data, wdict=None, topk=5, outfile='sorted_groups.txt', graphs=False, prefix='', athresh=0.5,\n",
    "                    tvalue=0.1):\n",
    "        alpha, labels, lpygx, mis, lasttc = out[:5]\n",
    "        ns, m = labels.shape\n",
    "        m, nv = mis.shape\n",
    "        hys = [entropy(labels[:, j]) for j in range(m)]\n",
    "        #alpha = np.array([z[2] for z in zs]) # m by nv\n",
    "        nmis = []\n",
    "        ixys = []\n",
    "        for j in range(m):\n",
    "            if hys[j] > 0:\n",
    "                #ixy = np.dot((alpha[j]>0.95).astype(int),mis[j])-lasttc[-1][j]\n",
    "                ixy = max(0., np.dot(alpha[j], mis[j]) - lasttc[-1][j])\n",
    "                ixys.append(ixy)\n",
    "                tcn = (np.sum(np.sort(alpha[j] * mis[j])[-topk:]) - ixy) / ((topk - 1) * hys[j])\n",
    "                nmis.append(tcn)  #ixy) #/hys[j])\n",
    "            else:\n",
    "                ixys.append(0)\n",
    "                nmis.append(0)\n",
    "        f = safe_open(prefix + outfile, 'w+')\n",
    "        print(list(enumerate(np.argsort(-np.array(nmis)))))\n",
    "        print(','.join(list(map(str, list(np.argsort(-np.array(nmis)))))))\n",
    "        for i, top in enumerate(np.argsort(-np.array(nmis))):\n",
    "            f.write('Group num: %d, Score: %0.3f\\n' % (top, nmis[top]))\n",
    "            inds = np.where(alpha[top] > athresh)[0]\n",
    "            inds = inds[np.argsort(-mis[top, inds])]\n",
    "            for ind in inds:\n",
    "                f.write(wdict[ind] + ', %0.3f\\n' % (mis[top, ind] / np.log(2)))\n",
    "            if wdict:\n",
    "                print(','.join(list(map(lambda q: wdict[q], inds))))\n",
    "                print(','.join(list(map(str, inds))))\n",
    "            print(top)\n",
    "            print(nmis[top], ixys[top], hys[top], ixys[top] / hys[top])  #,lasttc[-1][top],hys[top],lasttc[-1][top]/hys[top]\n",
    "            if graphs:\n",
    "                print(inds)\n",
    "                if len(inds) >= 2:\n",
    "                    plot_rels(data[:, inds[:5]], list(map(lambda q: wdict[q], inds[:5])),\n",
    "                              outfile='relationships/' + str(i) + '_group_num=' + str(top), latent=out[1][:, top],\n",
    "                              alpha=tvalue)\n",
    "        f.close()\n",
    "        return nmis\n",
    "\n",
    "\n",
    "    def shorten(s, n=12):\n",
    "        if len(s) > 2 * n:\n",
    "            return s[:n] + '..' + s[-n:]\n",
    "        return s\n",
    "\n",
    "\n",
    "    def plot_convergence(tc_history, prefix=''):\n",
    "        pylab.plot(tc_history)\n",
    "        pylab.xlabel('number of iterations')\n",
    "        filename = prefix + '/convergence.pdf'\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        pylab.savefig(filename)\n",
    "        pylab.close('all')\n",
    "        return True\n",
    "\n",
    "    def chunks(doc, n=100):\n",
    "        \"\"\"Yield successive approximately equal and n-sized chunks from l.\"\"\"\n",
    "        words = doc.split()\n",
    "        if len(words) == 0:\n",
    "            yield ''\n",
    "        n_chunks = len(words) / n  # Round down\n",
    "        if n_chunks == 0:\n",
    "            n_per_chunk = n\n",
    "        else:\n",
    "            n_per_chunk = int(np.ceil(float(len(words)) / n_chunks))  # round up\n",
    "        for i in xrange(0, len(words), n_per_chunk):\n",
    "            yield ' '.join(words[i:i+n])\n",
    "\n",
    "\n",
    "    # Utilities to construct generalized binary bag of words matrices\n",
    "\n",
    "\n",
    "    def av_bbow(docs, n=100):\n",
    "        \"\"\"Average binary bag of words if we take chunks of a doc of size n\"\"\"\n",
    "        proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "        proc.fit(docs)\n",
    "        n_doc, n_words = len(docs), len(proc.vocabulary_)\n",
    "        mat = ss.lil_matrix((n_doc, n_words))\n",
    "        for l, doc in enumerate(docs):\n",
    "            subdocs = chunks(doc, n=n)\n",
    "            mat[l] = (proc.transform(subdocs) > 0).mean(axis=0).A.ravel()\n",
    "        return mat.asformat('csr'), proc\n",
    "\n",
    "\n",
    "    def bow(docs):\n",
    "        \"\"\"Standard bag of words\"\"\"\n",
    "        proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "        return proc.fit_transform(docs), proc\n",
    "\n",
    "\n",
    "    def all_bbow(docs, n=100):\n",
    "        \"\"\"Split each document into a subdocuments of size n, and return as binary BOW\"\"\"\n",
    "        proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "        proc.fit(docs)\n",
    "        ids = []\n",
    "        for l, doc in enumerate(docs):\n",
    "            subdocs = chunks(doc, n=n)\n",
    "            submat = (proc.transform(subdocs) > 0)\n",
    "            if l == 0:\n",
    "              mat = submat\n",
    "            else:\n",
    "              mat = ss.vstack([mat, submat])\n",
    "            ids += [l]*submat.shape[0]\n",
    "        return mat.asformat('csr'), proc, ids\n",
    "\n",
    "\n",
    "    def file_to_array(filename, stemming=False, strategy=2, words_per_doc=100, n_words=10000):\n",
    "        pattern = '\\\\b[A-Za-z]+\\\\b'\n",
    "        stemmer = SnowballStemmer('english')\n",
    "\n",
    "        with open(filename, 'rU') as input_file:\n",
    "            docs = []\n",
    "            for line in input_file:\n",
    "                if stemming:\n",
    "                    docs.append(' '.join([stemmer.stem(w) for w in re.findall(pattern, line)]))\n",
    "                else:\n",
    "                    docs.append(' '.join([w for w in re.findall(pattern, line)]))\n",
    "        print('processing file')\n",
    "\n",
    "        if strategy == 1:\n",
    "            X, proc = av_bbow(docs, n=words_per_doc)\n",
    "        elif strategy == 2:\n",
    "            X, proc, ids = all_bbow(docs, n=words_per_doc)\n",
    "        else:\n",
    "            X, proc = bow(docs)\n",
    "\n",
    "        var_order = np.argsort(-X.sum(axis=0).A1)[:n_words]\n",
    "        X = X[:, var_order]\n",
    "\n",
    "        #Dictionary\n",
    "        ivd = {v: k for k, v in proc.vocabulary_.items()}\n",
    "        words = [ivd[v] for v in var_order]\n",
    "        return X, words\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        # Command line interface\n",
    "        # Sample commands:\n",
    "        # python vis_topic.py tests/data/twenty.txt --n_words=2000 --layers=20,3,1 -v --edges=50 -o test_output\n",
    "        from optparse import OptionParser, OptionGroup\n",
    "\n",
    "\n",
    "        parser = OptionParser(usage=\"usage: %prog [options] data_file.csv \\n\"\n",
    "                                    \"Assume one document on each line.\")\n",
    "\n",
    "        group = OptionGroup(parser, \"Options\")\n",
    "        group.add_option(\"-n\", \"--n_words\",\n",
    "                         action=\"store\", dest=\"n_words\", type=\"int\", default=10000,\n",
    "                         help=\"Maximum number of words to include in dictionary.\")\n",
    "        group.add_option(\"-l\", \"--layers\", dest=\"layers\", type=\"string\", default=\"2,1\",\n",
    "                         help=\"Specify number of units at each layer: 5,3,1 has \"\n",
    "                              \"5 units at layer 1, 3 at layer 2, and 1 at layer 3\")\n",
    "        group.add_option(\"-t\", \"--strategy\", dest=\"strategy\", type=\"int\", default=0,\n",
    "                         help=\"Specify the strategy for handling non-binary count data.\\n\"\n",
    "                              \"0. Naive binarization. This will be good for documents of similar length and especially\"\n",
    "                              \"short documents.\\n\"\n",
    "                              \"1. Average binary bag of words. We split documents into chunks, compute the binary \"\n",
    "                              \"bag of words for each documents and then average. This implicitly weights all documents\"\n",
    "                              \"equally.\\n\"\n",
    "                              \"2. All binary bag of words. Split documents into chunks and consider each chunk as its\"\n",
    "                              \"own binary bag of words documents. This changes the number of documents so it may take\"\n",
    "                              \"some work to match the ids back, if desired.\\n\"\n",
    "                              \"3. Fractional counts. This converts counts into a fraction of the background rate, with 1 as\"\n",
    "                              \"the max. Short documents tend to stay binary and words in long documents are weighted\"\n",
    "                              \"according to their frequency with respect to background in the corpus.\")\n",
    "        group.add_option(\"-o\", \"--output\",\n",
    "                         action=\"store\", dest=\"output\", type=\"string\", default=\"topic_output\",\n",
    "                         help=\"A directory to put all output files.\")\n",
    "        group.add_option(\"-s\", \"--stemming\",\n",
    "                         action=\"store_false\", dest=\"stemming\", default=True,\n",
    "                         help=\"Use a stemmer on words.\")\n",
    "        group.add_option(\"-v\", \"--verbose\",\n",
    "                         action=\"store_true\", dest=\"verbose\", default=False,\n",
    "                         help=\"Print rich outputs while running.\")\n",
    "        group.add_option(\"-w\", \"--words_per_doc\",\n",
    "                         action=\"store\", dest=\"words_per_doc\", type=\"int\", default=300,\n",
    "                         help=\"If using all_bbow or av_bbow, this specifies the number of words each \"\n",
    "                              \"to split documents into.\")\n",
    "        group.add_option(\"-e\", \"--edges\",\n",
    "                         action=\"store\", dest=\"max_edges\", type=\"int\", default=1000,\n",
    "                         help=\"Show at most this many edges in graphs.\")\n",
    "        group.add_option(\"-q\", \"--regraph\",\n",
    "                         action=\"store_true\", dest=\"regraph\", default=False,\n",
    "                         help=\"Don't re-run corex, just re-generate outputs (with number of edges changed).\")\n",
    "        parser.add_option_group(group)\n",
    "\n",
    "        (options, args) = parser.parse_args()\n",
    "        if not len(args) == 1:\n",
    "            print(\"Run with '-h' option for usage help.\")\n",
    "            sys.exit()\n",
    "\n",
    "        layers = list(map(int, options.layers.split(',')))\n",
    "        if layers[-1] != 1:\n",
    "            layers.append(1)  # Last layer has one unit for convenience so that graph is fully connected.\n",
    "\n",
    "        #Load data from text file\n",
    "        print('reading file')\n",
    "        X, words = file_to_array(args[0], stemming=options.stemming, strategy=options.strategy,\n",
    "                                 words_per_doc=options.words_per_doc, n_words=options.n_words)\n",
    "        # cPickle.dump(words, open(options.prefix + '/dictionary.dat', 'w'))  # TODO: output dictionary\n",
    "\n",
    "        # Run CorEx on data\n",
    "        if options.verbose:\n",
    "            np.set_printoptions(precision=3, suppress=True)  # For legible output from numpy\n",
    "            print('\\nData summary: X has %d rows and %d columns' % X.shape)\n",
    "            print('Variable names are: ' + ','.join(words))\n",
    "            print('Getting CorEx results')\n",
    "        if options.strategy == 3:\n",
    "            count = 'fraction'\n",
    "        else:\n",
    "            count = 'binarize'  # Strategies 1 and 2 already produce counts <= 1 and are not affected by this choice.\n",
    "        if not options.regraph:\n",
    "            for l, layer in enumerate(layers):\n",
    "                if options.verbose:\n",
    "                    print(\"Layer \", l)\n",
    "                if l == 0:\n",
    "                    t0 = time()\n",
    "                    corexes = [ct.Corex(n_hidden=layer, verbose=options.verbose, count=count).fit(X)]\n",
    "                    print('Time for first layer: %0.2f' % (time() - t0))\n",
    "                else:\n",
    "                    X_prev = np.matrix(corexes[-1].labels)\n",
    "                    corexes.append(ct.Corex(n_hidden=layer, verbose=options.verbose).fit(X_prev))\n",
    "            for l, corex in enumerate(corexes):\n",
    "                # The learned model can be loaded again using ct.Corex().load(filename)\n",
    "                print('TC at layer %d is: %0.3f' % (l, corex.tc))\n",
    "                corex.save(options.output + '/layer_' + str(l) + '.dat')\n",
    "        else:\n",
    "            corexes = [ct.Corex().load(options.output + '/layer_' + str(l) + '.dat') for l in range(len(layers))]\n",
    "\n",
    "\n",
    "        # This line outputs plots showing relationships at the first layer\n",
    "        vis_rep(corexes[0], data=X, column_label=words, prefix=options.output)\n",
    "        # This line outputs a hierarchical networks structure in a .dot file in the \"graphs\" folder\n",
    "        # And it tries to compile the dot file into a pdf using the command line utility sfdp (part of graphviz)\n",
    "        vis_hierarchy(corexes, column_label=words, max_edges=options.max_edges, prefix=options.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [options] data_file.csv \n",
      "Assume one document on each line.\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This module implements some visualizations based on CorEx representations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # to create visualizations on a display-less server\n",
    "import pylab\n",
    "import networkx as nx\n",
    "import textwrap\n",
    "import scipy.sparse as ss\n",
    "import sklearn.feature_extraction.text as skt\n",
    "#import cPickle, pickle # neither module is used, and cPickle is not part of Anaconda build, so commented for LF run\n",
    "import corextopic as ct\n",
    "import sys, traceback\n",
    "from time import time\n",
    "import re\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from nltk.stem.snowball import *\n",
    "pattern = '\\\\b[A-Za-z]+\\\\b'\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "\n",
    "def vis_rep(corex, data=None, row_label=None, column_label=None, prefix='topics'):\n",
    "    \"\"\"Various visualizations and summary statistics for a one layer representation\"\"\"\n",
    "    if column_label is None:\n",
    "        column_label = list(map(str, range(data.shape[1])))\n",
    "    if row_label is None:\n",
    "        row_label = list(map(str, range(corex.n_samples)))\n",
    "\n",
    "    alpha = corex.alpha\n",
    "\n",
    "    print('Print topics in text file')\n",
    "    output_groups(corex.tcs, alpha, corex.mis, column_label, corex.sign, prefix=prefix)\n",
    "    output_labels(corex.labels, row_label, prefix=prefix)\n",
    "    output_cont_labels(corex.p_y_given_x, row_label, prefix=prefix)\n",
    "    output_strong(corex.tcs, alpha, corex.mis, corex.labels, prefix=prefix)\n",
    "    anomalies(corex.log_z, row_label=row_label, prefix=prefix)\n",
    "    plot_convergence(corex.tc_history, prefix=prefix)\n",
    "    if data is not None:\n",
    "        plot_heatmaps(data, alpha, corex.mis, column_label, corex.p_y_given_x, prefix=prefix)\n",
    "\n",
    "\n",
    "def vis_hierarchy(corexes, column_label=None, max_edges=100, prefix='topics', n_anchors=0):\n",
    "    \"\"\"Visualize a hierarchy of representations.\"\"\"\n",
    "    if column_label is None:\n",
    "        column_label = list(map(str, range(corexes[0].alpha.shape[1])))\n",
    "\n",
    "    # make l1 label\n",
    "    alpha = corexes[0].alpha\n",
    "    mis = corexes[0].mis\n",
    "    l1_labels = []\n",
    "    annotate = lambda q, s: q if s > 0 else '~' + q\n",
    "    for j in range(corexes[0].n_hidden):\n",
    "        # inds = np.where(alpha[j] * mis[j] > 0)[0]\n",
    "        inds = np.where(alpha[j] >= 1.)[0]\n",
    "        inds = inds[np.argsort(-alpha[j, inds] * mis[j, inds])]\n",
    "        group_number = str('red_') + str(j) if j < n_anchors else str(j)\n",
    "        label = group_number + ':' + ' '.join([annotate(column_label[ind], corexes[0].sign[j,ind]) for ind in inds[:6]])\n",
    "        label = textwrap.fill(label, width=25)\n",
    "        l1_labels.append(label)\n",
    "\n",
    "    # Construct non-tree graph\n",
    "    weights = [corex.alpha.clip(0, 1) * corex.mis for corex in corexes[1:]]\n",
    "    node_weights = [corex.tcs for corex in corexes[1:]]\n",
    "    g = make_graph(weights, node_weights, l1_labels, max_edges=max_edges)\n",
    "\n",
    "    # Display pruned version\n",
    "    h = g.copy()  # trim(g.copy(), max_parents=max_parents, max_children=max_children)\n",
    "    edge2pdf(h, prefix + '/graphs/graph_prune_' + str(max_edges), labels='label', directed=True, makepdf=True)\n",
    "\n",
    "    # Display tree version\n",
    "    tree = g.copy()\n",
    "    tree = trim(tree, max_parents=1, max_children=False)\n",
    "    edge2pdf(tree, prefix + '/graphs/tree', labels='label', directed=True, makepdf=True)\n",
    "\n",
    "    # Output JSON files\n",
    "    try:\n",
    "        import os\n",
    "        #copyfile(os.path.dirname(os.path.realpath(__file__)) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "        print(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "        copyfile(os.path.dirname(os.path.realpath('tests')) + '/tests/d3_files/force.html', prefix + '/graphs/force.html')\n",
    "    except:\n",
    "        print(\"Couldn't find 'force.html' file for visualizing d3 output\")\n",
    "    import json\n",
    "    from networkx.readwrite import json_graph\n",
    "\n",
    "    mapping = dict([(n, tree.node[n].get('label', str(n))) for n in tree.nodes()])\n",
    "    tree = nx.relabel_nodes(tree, mapping)\n",
    "    json.dump(json_graph.node_link_data(tree), safe_open(prefix + '/graphs/force.json', 'w+'))\n",
    "    json.dump(json_graph.node_link_data(h), safe_open(prefix + '/graphs/force_nontree.json', 'w+'))\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_heatmaps(data, alpha, mis, column_label, cont, topk=40, athresh=0.2, prefix=''):\n",
    "    import seaborn as sns\n",
    "    cmap = sns.cubehelix_palette(as_cmap=True, light=.9)\n",
    "    import matplotlib.pyplot as plt\n",
    "    m, nv = mis.shape\n",
    "    for j in range(m):\n",
    "        inds = np.where(np.logical_and(alpha[j] > athresh, mis[j] > 0.))[0]\n",
    "        inds = inds[np.argsort(- alpha[j, inds] * mis[j, inds])][:topk]\n",
    "        if len(inds) >= 2:\n",
    "            plt.clf()\n",
    "            order = np.argsort(cont[:,j])\n",
    "            if type(data) == np.ndarray:\n",
    "                subdata = data[:, inds][order].T\n",
    "            else:\n",
    "                # assume sparse\n",
    "                subdata = data[:, inds].toarray()\n",
    "                subdata = subdata[order].T\n",
    "            columns = [column_label[i] for i in inds]\n",
    "            fig, ax = plt.subplots(figsize=(20, 10))\n",
    "            sns.heatmap(subdata, vmin=0, vmax=1, cmap=cmap, yticklabels=columns, xticklabels=False, ax=ax, cbar_kws={\"ticks\": [0, 0.5, 1]})\n",
    "            plt.yticks(rotation=0)\n",
    "            filename = '{}/heatmaps/group_num={}.png'.format(prefix, j)\n",
    "            if not os.path.exists(os.path.dirname(filename)):\n",
    "                os.makedirs(os.path.dirname(filename))\n",
    "            plt.title(\"Latent factor {}\".format(j))\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.close('all')\n",
    "            #plot_rels(data[:, inds], map(lambda q: column_label[q], inds), colors=cont[:, j],\n",
    "            #          outfile=prefix + '/relationships/group_num=' + str(j), latent=labels[:, j], alpha=0.1)\n",
    "\n",
    "\n",
    "def make_graph(weights, node_weights, column_label, max_edges=100):\n",
    "    all_edges = np.hstack(list(map(np.ravel, weights)))\n",
    "    max_edges = min(max_edges, len(all_edges))\n",
    "    w_thresh = np.sort(all_edges)[-max_edges]\n",
    "    print('weight threshold is %f for graph with max of %f edges ' % (w_thresh, max_edges))\n",
    "    g = nx.DiGraph()\n",
    "    max_node_weight = max([max(w) for w in node_weights])\n",
    "    for layer, weight in enumerate(weights):\n",
    "        m, n = weight.shape\n",
    "        for j in range(m):\n",
    "            g.add_node((layer + 1, j))\n",
    "            g.node[(layer + 1, j)]['weight'] = 0.3 * node_weights[layer][j] / max_node_weight\n",
    "            for i in range(n):\n",
    "                if weight[j, i] > w_thresh:\n",
    "                    if weight[j, i] > w_thresh / 2:\n",
    "                        g.add_weighted_edges_from([( (layer, i), (layer + 1, j), 10 * weight[j, i])])\n",
    "                    else:\n",
    "                        g.add_weighted_edges_from([( (layer, i), (layer + 1, j), 0)])\n",
    "\n",
    "    # Label layer 0\n",
    "    for i, lab in enumerate(column_label):\n",
    "        g.add_node((0, i))\n",
    "        g.node[(0, i)]['label'] = lab\n",
    "        g.node[(0, i)]['name'] = lab  # JSON uses this field\n",
    "        g.node[(0, i)]['weight'] = 1\n",
    "    return g\n",
    "\n",
    "\n",
    "def trim(g, max_parents=False, max_children=False):\n",
    "    for node in g:\n",
    "        if max_parents:\n",
    "            parents = list(g.successors(node))\n",
    "            #weights = [g.edge[node][parent]['weight'] for parent in parents]\n",
    "            weights = [g.adj[node][parent]['weight'] for parent in parents]\n",
    "            for weak_parent in np.argsort(weights)[:-max_parents]:\n",
    "                g.remove_edge(node, parents[weak_parent])\n",
    "        if max_children:\n",
    "            children = g.predecessors(node)\n",
    "            weights = [g.edge[child][node]['weight'] for child in children]\n",
    "            for weak_child in np.argsort(weights)[:-max_children]:\n",
    "                g.remove_edge(children[weak_child], node)\n",
    "    return g\n",
    "\n",
    "\n",
    "def output_groups(tcs, alpha, mis, column_label, direction, thresh=0, prefix=''):\n",
    "    f = safe_open(prefix + '/groups.txt', 'w+')\n",
    "    h = safe_open(prefix + '/topics.txt', 'w+')\n",
    "    m, nv = mis.shape\n",
    "    annotate = lambda q, s: q if s >= 0 else '~' + q\n",
    "    for j in range(m):\n",
    "        f.write('Group num: %d, TC(X;Y_j): %0.3f\\n' % (j, tcs[j]))\n",
    "        # inds = np.where(alpha[j] * mis[j] > thresh)[0]\n",
    "        inds = np.where(alpha[j] >= 1.)[0]\n",
    "        inds = inds[np.argsort(-alpha[j, inds] * mis[j, inds])]\n",
    "        for ind in inds:\n",
    "            f.write(column_label[ind] + u', %0.3f, %0.3f, %0.3f\\n' % (\n",
    "                mis[j, ind], alpha[j, ind], mis[j, ind] * alpha[j, ind]))\n",
    "        #h.write(unicode(j) + u':' + u','.join([annotate(column_label[ind], direction[j,ind]) for ind in inds[:10]]) + u'\\n')\n",
    "        h.write(str(j) + u':' + u','.join(\n",
    "            [annotate(column_label[ind], direction[j, ind]) for ind in inds[:10]]) + u'\\n')\n",
    "    f.close()\n",
    "    h.close()\n",
    "\n",
    "\n",
    "def output_labels(labels, row_label, prefix=''):\n",
    "    f = safe_open(prefix + '/labels.txt', 'w+')\n",
    "    ns, m = labels.shape\n",
    "    for l in range(ns):\n",
    "        f.write(row_label[l] + ',' + ','.join(list(map(lambda q: '%d' % q, labels[l, :])))+ '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def output_cont_labels(p_y_given_x, row_label, prefix=''):\n",
    "    f = safe_open(prefix + '/cont_labels.txt', 'w+')\n",
    "    ns, m = p_y_given_x.shape\n",
    "    for l in range(ns):\n",
    "        f.write(row_label[l] + ',' + ','.join(list(map(lambda q: '{:.10f}'.format(q), np.log(p_y_given_x[l, :])))) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def output_strong(tcs, alpha, mis, labels, prefix=''):\n",
    "    f = safe_open(prefix + '/most_deterministic_groups.txt', 'w+')\n",
    "    m, n = alpha.shape\n",
    "    topk = 5\n",
    "    ixy = np.clip(np.sum(alpha * mis, axis=1) - tcs, 0, np.inf)\n",
    "    hys = np.array([entropy(labels[:, j]) for j in range(m)]).clip(1e-6)\n",
    "    ntcs = [(np.sum(np.sort(alpha[j] * mis[j])[-topk:]) - ixy[j]) / ((topk - 1) * hys[j]) for j in range(m)]\n",
    "\n",
    "    f.write('Group num., NTC\\n')\n",
    "    for j, ntc in sorted(enumerate(ntcs), key=lambda q: -q[1]):\n",
    "        f.write('%d, %0.3f\\n' % (j, ntc))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def anomalies(log_z, row_label=None, prefix=''):\n",
    "    from scipy.special import erf\n",
    "\n",
    "    ns = log_z.shape[0]\n",
    "    if row_label is None:\n",
    "        row_label = list(map(str, range(ns)))\n",
    "    a_score = np.sum(log_z[:, :], axis=1)\n",
    "    mean, std = np.mean(a_score), np.std(a_score)\n",
    "    a_score = (a_score - mean) / std\n",
    "    percentile = 1. / ns\n",
    "    anomalies = np.where(0.5 * (1 - erf(a_score / np.sqrt(2)) ) < percentile)[0]\n",
    "    f = safe_open(prefix + '/anomalies.txt', 'w+')\n",
    "    for i in anomalies:\n",
    "        f.write(row_label[i] + ', %0.1f\\n' % a_score[i])\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# Utilities\n",
    "# IT UTILITIES\n",
    "def entropy(xsamples):\n",
    "    # sample entropy for one discrete var\n",
    "    xsamples = np.asarray(xsamples)\n",
    "    xsamples = xsamples[xsamples >= 0]  # by def, -1 means missing value\n",
    "    xs = np.unique(xsamples)\n",
    "    ns = len(xsamples)\n",
    "    ps = np.array([float(np.count_nonzero(xsamples == x)) / ns for x in xs])\n",
    "    return -np.sum(ps * np.log(ps))\n",
    "\n",
    "\n",
    "def safe_open(filename, mode):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    return codecs.open(filename, mode, \"utf-8\")\n",
    "\n",
    "\n",
    "# Visualization utilities\n",
    "\n",
    "def neato(fname, position=None, directed=False):\n",
    "    if directed:\n",
    "        os.system(\n",
    "            \"sfdp \" + fname + \".dot -Tpdf -Earrowhead=none -Nfontsize=16  -GK=2 -Gmaxiter=1000 -Goverlap=False -Gpack=True -Gpackmode=clust -Gsep=0.01 -Gsplines=False -o \" + fname + \"_sfdp.pdf\")\n",
    "        os.system(\n",
    "            \"sfdp \" + fname + \".dot -Tpdf -Earrowhead=none -Nfontsize=16  -GK=2 -Gmaxiter=1000 -Goverlap=False -Gpack=True -Gpackmode=clust -Gsep=0.01 -Gsplines=True -o \" + fname + \"_sfdp_w_splines.pdf\")\n",
    "        return True\n",
    "    if position is None:\n",
    "        os.system(\"neato \" + fname + \".dot -Tpdf -o \" + fname + \".pdf\")\n",
    "        os.system(\"fdp \" + fname + \".dot -Tpdf -o \" + fname + \"fdp.pdf\")\n",
    "    else:\n",
    "        os.system(\"neato \" + fname + \".dot -Tpdf -n -o \" + fname + \".pdf\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_color(label):\n",
    "    import matplotlib\n",
    "\n",
    "    colors = matplotlib.colors.cnames.keys()\n",
    "    parts = label.split('_')\n",
    "    for part in parts:\n",
    "        if part in colors:\n",
    "            parts.remove(part)\n",
    "            return '_'.join(parts), part\n",
    "    return label, 'black'\n",
    "\n",
    "\n",
    "def edge2pdf(g, filename, threshold=0, position=None, labels=None, connected=True, directed=False, makepdf=True):\n",
    "    #This function will takes list of edges and a filename\n",
    "    #and write a file in .dot format. Readable, eg. by omnigraffle\n",
    "    # OR use \"neato file.dot -Tpng -n -o file.png\"\n",
    "    # The -n option says whether to use included node positions or to generate new ones\n",
    "    # for a grid, positions = [(i%28,i/28) for i in range(784)]\n",
    "    def cnn(node):\n",
    "        #change node names for dot format\n",
    "        if type(node) is tuple or type(node) is list:\n",
    "            #return u'n' + u'_'.join(list(map(unicode, node)))\n",
    "            return u'n' + u'_'.join(list(map(str, node)))\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    if connected:\n",
    "        touching = list(set(sum([[a, b] for a, b in g.edges()], [])))\n",
    "        g = nx.subgraph(g, touching)\n",
    "        print('non-isolated nodes,edges', len(list(g.nodes())), len(list(g.edges())))\n",
    "    f = safe_open(filename + '.dot', 'w+')\n",
    "    #print('f1->',f)\n",
    "    #print('directed->',directed)\n",
    "    if directed:\n",
    "        f.write(\"strict digraph {\\n\")#.decode(\"utf-8\")\n",
    "        #f.write('strict digraph {')\n",
    "    else:\n",
    "        f.write(\"strict graph {\")\n",
    "    #f.write(\"\\tgraph [overlap=scale];\\n\".encode('utf-8'))\n",
    "    f.write(\"\\tnode [shape=point];\\n\")\n",
    "    for a, b, d in g.edges(data=True):\n",
    "        if 'weight' in d:\n",
    "            if directed:\n",
    "                f.write((\"\\t\" + cnn(a) + ' -> ' + cnn(b) + ' [penwidth=%.2f' % float(\n",
    "                    np.clip(d['weight'], 0, 9)) + '];\\n'))\n",
    "            else:\n",
    "                if d['weight'] > threshold:\n",
    "                    f.write((\"\\t\" + cnn(a) + ' -- ' + cnn(b) + ' [penwidth=' + str(3 * d['weight']) + '];\\n'))\n",
    "        else:\n",
    "            if directed:\n",
    "                f.write((\"\\t\" + cnn(a) + ' -> ' + cnn(b) + ';\\n'))\n",
    "            else:\n",
    "                f.write((\"\\t\" + cnn(a) + ' -- ' + cnn(b) + ';\\n'))\n",
    "    for n in g.nodes():\n",
    "        if labels is not None:\n",
    "            if type(labels) == dict or type(labels) == list:\n",
    "                thislabel = labels[n].replace(u'\"', u'\\\\\"')\n",
    "                lstring = u'label=\"' + thislabel + u'\",shape=none'\n",
    "            elif type(labels) == str:\n",
    "                #if g.node[n].has_key('label'):\n",
    "                if 'label' in g.node[n]:\n",
    "                    thislabel = g.node[n][labels].replace(u'\"', u'\\\\\"')\n",
    "                    # combine dupes\n",
    "                    #llist = thislabel.split(',')\n",
    "                    #thislabel = ','.join([l for l in set(llist)])\n",
    "                    thislabel, thiscolor = extract_color(thislabel)\n",
    "                    lstring = u'label=\"%s\",shape=none,fontcolor=\"%s\"' % (thislabel, thiscolor)\n",
    "                else:\n",
    "                    weight = g.node[n].get('weight', 0.1)\n",
    "                    if n[0] == 1:\n",
    "                        lstring = u'shape=circle,margin=\"0,0\",style=filled,fillcolor=black,fontcolor=white,height=%0.2f,label=\"%d\"' % (\n",
    "                            2 * weight, n[1])\n",
    "                    else:\n",
    "                        lstring = u'shape=point,height=%0.2f' % weight\n",
    "            else:\n",
    "                lstring = 'label=\"' + str(n) + '\",shape=none'\n",
    "            lstring = str(lstring)\n",
    "        else:\n",
    "            lstring = False\n",
    "        if position is not None:\n",
    "            if position == 'grid':\n",
    "                position = [(i % 28, 28 - i / 28) for i in range(784)]\n",
    "            posstring = unicode('pos=\"' + str(position[n][0]) + ',' + str(position[n][1]) + '\"')\n",
    "        else:\n",
    "            posstring = False\n",
    "        finalstring = u' [' + u','.join([ts for ts in [posstring, lstring] if ts]) + u']\\n'\n",
    "        #finalstring = u' ['+lstring+u']\\n'\n",
    "        f.write(u'\\t' + cnn(n) + finalstring)\n",
    "    f.write(\"}\")\n",
    "    f.close()\n",
    "    if makepdf:\n",
    "        neato(filename, position=position, directed=directed)\n",
    "    return True\n",
    "\n",
    "\n",
    "def predictable(out, data, wdict=None, topk=5, outfile='sorted_groups.txt', graphs=False, prefix='', athresh=0.5,\n",
    "                tvalue=0.1):\n",
    "    alpha, labels, lpygx, mis, lasttc = out[:5]\n",
    "    ns, m = labels.shape\n",
    "    m, nv = mis.shape\n",
    "    hys = [entropy(labels[:, j]) for j in range(m)]\n",
    "    #alpha = np.array([z[2] for z in zs]) # m by nv\n",
    "    nmis = []\n",
    "    ixys = []\n",
    "    for j in range(m):\n",
    "        if hys[j] > 0:\n",
    "            #ixy = np.dot((alpha[j]>0.95).astype(int),mis[j])-lasttc[-1][j]\n",
    "            ixy = max(0., np.dot(alpha[j], mis[j]) - lasttc[-1][j])\n",
    "            ixys.append(ixy)\n",
    "            tcn = (np.sum(np.sort(alpha[j] * mis[j])[-topk:]) - ixy) / ((topk - 1) * hys[j])\n",
    "            nmis.append(tcn)  #ixy) #/hys[j])\n",
    "        else:\n",
    "            ixys.append(0)\n",
    "            nmis.append(0)\n",
    "    f = safe_open(prefix + outfile, 'w+')\n",
    "    print(list(enumerate(np.argsort(-np.array(nmis)))))\n",
    "    print(','.join(list(map(str, list(np.argsort(-np.array(nmis)))))))\n",
    "    for i, top in enumerate(np.argsort(-np.array(nmis))):\n",
    "        f.write('Group num: %d, Score: %0.3f\\n' % (top, nmis[top]))\n",
    "        inds = np.where(alpha[top] > athresh)[0]\n",
    "        inds = inds[np.argsort(-mis[top, inds])]\n",
    "        for ind in inds:\n",
    "            f.write(wdict[ind] + ', %0.3f\\n' % (mis[top, ind] / np.log(2)))\n",
    "        if wdict:\n",
    "            print(','.join(list(map(lambda q: wdict[q], inds))))\n",
    "            print(','.join(list(map(str, inds))))\n",
    "        print(top)\n",
    "        print(nmis[top], ixys[top], hys[top], ixys[top] / hys[top])  #,lasttc[-1][top],hys[top],lasttc[-1][top]/hys[top]\n",
    "        if graphs:\n",
    "            print(inds)\n",
    "            if len(inds) >= 2:\n",
    "                plot_rels(data[:, inds[:5]], list(map(lambda q: wdict[q], inds[:5])),\n",
    "                          outfile='relationships/' + str(i) + '_group_num=' + str(top), latent=out[1][:, top],\n",
    "                          alpha=tvalue)\n",
    "    f.close()\n",
    "    return nmis\n",
    "\n",
    "\n",
    "def shorten(s, n=12):\n",
    "    if len(s) > 2 * n:\n",
    "        return s[:n] + '..' + s[-n:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def plot_convergence(tc_history, prefix=''):\n",
    "    pylab.plot(tc_history)\n",
    "    pylab.xlabel('number of iterations')\n",
    "    filename = prefix + '/convergence.pdf'\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    pylab.savefig(filename)\n",
    "    pylab.close('all')\n",
    "    return True\n",
    "\n",
    "def chunks(doc, n=100):\n",
    "    \"\"\"Yield successive approximately equal and n-sized chunks from l.\"\"\"\n",
    "    words = doc.split()\n",
    "    if len(words) == 0:\n",
    "        yield ''\n",
    "    n_chunks = len(words) / n  # Round down\n",
    "    if n_chunks == 0:\n",
    "        n_per_chunk = n\n",
    "    else:\n",
    "        n_per_chunk = int(np.ceil(float(len(words)) / n_chunks))  # round up\n",
    "    for i in xrange(0, len(words), n_per_chunk):\n",
    "        yield ' '.join(words[i:i+n])\n",
    "\n",
    "\n",
    "# Utilities to construct generalized binary bag of words matrices\n",
    "\n",
    "\n",
    "def av_bbow(docs, n=100):\n",
    "    \"\"\"Average binary bag of words if we take chunks of a doc of size n\"\"\"\n",
    "    proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "    proc.fit(docs)\n",
    "    n_doc, n_words = len(docs), len(proc.vocabulary_)\n",
    "    mat = ss.lil_matrix((n_doc, n_words))\n",
    "    for l, doc in enumerate(docs):\n",
    "        subdocs = chunks(doc, n=n)\n",
    "        mat[l] = (proc.transform(subdocs) > 0).mean(axis=0).A.ravel()\n",
    "    return mat.asformat('csr'), proc\n",
    "\n",
    "\n",
    "def bow(docs):\n",
    "    \"\"\"Standard bag of words\"\"\"\n",
    "    proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "    return proc.fit_transform(docs), proc\n",
    "\n",
    "\n",
    "def all_bbow(docs, n=100):\n",
    "    \"\"\"Split each document into a subdocuments of size n, and return as binary BOW\"\"\"\n",
    "    proc = skt.CountVectorizer(token_pattern=pattern)\n",
    "    proc.fit(docs)\n",
    "    ids = []\n",
    "    for l, doc in enumerate(docs):\n",
    "        subdocs = chunks(doc, n=n)\n",
    "        submat = (proc.transform(subdocs) > 0)\n",
    "        if l == 0:\n",
    "          mat = submat\n",
    "        else:\n",
    "          mat = ss.vstack([mat, submat])\n",
    "        ids += [l]*submat.shape[0]\n",
    "    return mat.asformat('csr'), proc, ids\n",
    "\n",
    "\n",
    "def file_to_array(filename, stemming=False, strategy=2, words_per_doc=100, n_words=10000):\n",
    "    pattern = '\\\\b[A-Za-z]+\\\\b'\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    with open(filename, 'rU') as input_file:\n",
    "        docs = []\n",
    "        for line in input_file:\n",
    "            if stemming:\n",
    "                docs.append(' '.join([stemmer.stem(w) for w in re.findall(pattern, line)]))\n",
    "            else:\n",
    "                docs.append(' '.join([w for w in re.findall(pattern, line)]))\n",
    "    print('processing file')\n",
    "\n",
    "    if strategy == 1:\n",
    "        X, proc = av_bbow(docs, n=words_per_doc)\n",
    "    elif strategy == 2:\n",
    "        X, proc, ids = all_bbow(docs, n=words_per_doc)\n",
    "    else:\n",
    "        X, proc = bow(docs)\n",
    "\n",
    "    var_order = np.argsort(-X.sum(axis=0).A1)[:n_words]\n",
    "    X = X[:, var_order]\n",
    "\n",
    "    #Dictionary\n",
    "    ivd = {v: k for k, v in proc.vocabulary_.items()}\n",
    "    words = [ivd[v] for v in var_order]\n",
    "    return X, words\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Command line interface\n",
    "    # Sample commands:\n",
    "    # python vis_topic.py tests/data/twenty.txt --n_words=2000 --layers=20,3,1 -v --edges=50 -o test_output\n",
    "    from optparse import OptionParser, OptionGroup\n",
    "\n",
    "\n",
    "    parser = OptionParser(usage=\"usage: %prog [options] data_file.csv \\n\"\n",
    "                                \"Assume one document on each line.\")\n",
    "\n",
    "    group = OptionGroup(parser, \"Options\")\n",
    "    group.add_option(\"-n\", \"--n_words\",\n",
    "                     action=\"store\", dest=\"n_words\", type=\"int\", default=10000,\n",
    "                     help=\"Maximum number of words to include in dictionary.\")\n",
    "    group.add_option(\"-l\", \"--layers\", dest=\"layers\", type=\"string\", default=\"2,1\",\n",
    "                     help=\"Specify number of units at each layer: 5,3,1 has \"\n",
    "                          \"5 units at layer 1, 3 at layer 2, and 1 at layer 3\")\n",
    "    group.add_option(\"-t\", \"--strategy\", dest=\"strategy\", type=\"int\", default=0,\n",
    "                     help=\"Specify the strategy for handling non-binary count data.\\n\"\n",
    "                          \"0. Naive binarization. This will be good for documents of similar length and especially\"\n",
    "                          \"short documents.\\n\"\n",
    "                          \"1. Average binary bag of words. We split documents into chunks, compute the binary \"\n",
    "                          \"bag of words for each documents and then average. This implicitly weights all documents\"\n",
    "                          \"equally.\\n\"\n",
    "                          \"2. All binary bag of words. Split documents into chunks and consider each chunk as its\"\n",
    "                          \"own binary bag of words documents. This changes the number of documents so it may take\"\n",
    "                          \"some work to match the ids back, if desired.\\n\"\n",
    "                          \"3. Fractional counts. This converts counts into a fraction of the background rate, with 1 as\"\n",
    "                          \"the max. Short documents tend to stay binary and words in long documents are weighted\"\n",
    "                          \"according to their frequency with respect to background in the corpus.\")\n",
    "    group.add_option(\"-o\", \"--output\",\n",
    "                     action=\"store\", dest=\"output\", type=\"string\", default=\"topic_output\",\n",
    "                     help=\"A directory to put all output files.\")\n",
    "    group.add_option(\"-s\", \"--stemming\",\n",
    "                     action=\"store_false\", dest=\"stemming\", default=True,\n",
    "                     help=\"Use a stemmer on words.\")\n",
    "    group.add_option(\"-v\", \"--verbose\",\n",
    "                     action=\"store_true\", dest=\"verbose\", default=False,\n",
    "                     help=\"Print rich outputs while running.\")\n",
    "    group.add_option(\"-w\", \"--words_per_doc\",\n",
    "                     action=\"store\", dest=\"words_per_doc\", type=\"int\", default=300,\n",
    "                     help=\"If using all_bbow or av_bbow, this specifies the number of words each \"\n",
    "                          \"to split documents into.\")\n",
    "    group.add_option(\"-e\", \"--edges\",\n",
    "                     action=\"store\", dest=\"max_edges\", type=\"int\", default=1000,\n",
    "                     help=\"Show at most this many edges in graphs.\")\n",
    "    group.add_option(\"-q\", \"--regraph\",\n",
    "                     action=\"store_true\", dest=\"regraph\", default=False,\n",
    "                     help=\"Don't re-run corex, just re-generate outputs (with number of edges changed).\")\n",
    "    parser.add_option_group(group)\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    if not len(args) == 1:\n",
    "        print(\"Run with '-h' option for usage help.\")\n",
    "        sys.exit()\n",
    "\n",
    "    layers = list(map(int, options.layers.split(',')))\n",
    "    if layers[-1] != 1:\n",
    "        layers.append(1)  # Last layer has one unit for convenience so that graph is fully connected.\n",
    "\n",
    "    #Load data from text file\n",
    "    print('reading file')\n",
    "    X, words = file_to_array(args[0], stemming=options.stemming, strategy=options.strategy,\n",
    "                             words_per_doc=options.words_per_doc, n_words=options.n_words)\n",
    "    # cPickle.dump(words, open(options.prefix + '/dictionary.dat', 'w'))  # TODO: output dictionary\n",
    "\n",
    "    # Run CorEx on data\n",
    "    if options.verbose:\n",
    "        np.set_printoptions(precision=3, suppress=True)  # For legible output from numpy\n",
    "        print('\\nData summary: X has %d rows and %d columns' % X.shape)\n",
    "        print('Variable names are: ' + ','.join(words))\n",
    "        print('Getting CorEx results')\n",
    "    if options.strategy == 3:\n",
    "        count = 'fraction'\n",
    "    else:\n",
    "        count = 'binarize'  # Strategies 1 and 2 already produce counts <= 1 and are not affected by this choice.\n",
    "    if not options.regraph:\n",
    "        for l, layer in enumerate(layers):\n",
    "            if options.verbose:\n",
    "                print(\"Layer \", l)\n",
    "            if l == 0:\n",
    "                t0 = time()\n",
    "                corexes = [ct.Corex(n_hidden=layer, verbose=options.verbose, count=count).fit(X)]\n",
    "                print('Time for first layer: %0.2f' % (time() - t0))\n",
    "            else:\n",
    "                X_prev = np.matrix(corexes[-1].labels)\n",
    "                corexes.append(ct.Corex(n_hidden=layer, verbose=options.verbose).fit(X_prev))\n",
    "        for l, corex in enumerate(corexes):\n",
    "            # The learned model can be loaded again using ct.Corex().load(filename)\n",
    "            print('TC at layer %d is: %0.3f' % (l, corex.tc))\n",
    "            corex.save(options.output + '/layer_' + str(l) + '.dat')\n",
    "    else:\n",
    "        corexes = [ct.Corex().load(options.output + '/layer_' + str(l) + '.dat') for l in range(len(layers))]\n",
    "\n",
    "\n",
    "    # This line outputs plots showing relationships at the first layer\n",
    "    vis_rep(corexes[0], data=X, column_label=words, prefix=options.output)\n",
    "    # This line outputs a hierarchical networks structure in a .dot file in the \"graphs\" folder\n",
    "    # And it tries to compile the dot file into a pdf using the command line utility sfdp (part of graphviz)\n",
    "    vis_hierarchy(corexes, column_label=words, max_edges=options.max_edges, prefix=options.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight threshold is 0.000000 for graph with max of 200.000000 edges \n",
      "non-isolated nodes,edges 35 36\n",
      "non-isolated nodes,edges 35 34\n",
      "/Users/ryanalcantara/Buff Drive/Biomech_Lit_Up/literature_update/Construct_Models/tests/d3_files/force.html topic-model-example/graphs/force.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'topic-model-example/graphs/graph_prune_200.dot.pdf'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import unicode\n",
    "vis_hierarchy([topic_model, tm_layer2, tm_layer3], column_label=words, max_edges=200, prefix='topic-model-example')\n",
    "\n",
    "from graphviz import Source\n",
    "path = 'topic-model-example/graphs/graph_prune_200.dot'\n",
    "s = Source.from_file(path)\n",
    "s.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchoring for Semi-Supervised Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchored CorEx is an extension of CorEx that allows the \"anchoring\" of words to topics. When anchoring a word to a topic, CorEx is trying to maximize the mutual information between that word and the anchored topic. So, anchoring provides a way to guide the topic model towards specific subsets of words that the user would like to explore.  \n",
    "\n",
    "The anchoring mechanism is flexible, and so there are many possibilities of anchoring. We explored the following types of anchoring in our TACL paper:\n",
    "\n",
    "1. Anchoring a single set of words to a single topic. This can help promote a topic that did not naturally emerge when running an unsupervised instance of the CorEx topic model. For example, one might anchor words like \"snow,\" \"cold,\" and \"avalanche\" to a topic if one suspects there should be a snow avalanche topic within a set of disaster relief articles.\n",
    "\n",
    "2. Anchoring single sets of words to multiple topics. This can help find different aspects of a topic that may be discussed in several different contexts. For example, one might anchor \"protest\" to three topics and \"riot\" to three other topics to understand different framings that arise from tweets about political protests.\n",
    "\n",
    "3. Anchoring different sets of words to multiple topics. This can help enforce topic separability if there appear to be chimera topics. For example, one might anchor \"mountain,\" \"Bernese,\" and \"dog\" to one topic and \"mountain,\" \"rocky,\" and \"colorado\" to another topic to help separate topics that merge discussion of Bernese Mountain Dogs and the Rocky Mountains.\n",
    "\n",
    "\n",
    "We'll demonstrate how to anchor words to the the CorEx topic model and how to develop other anchoring strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAIT/LOCOMOTION                   2547\n",
       "ORTHOPAEDICS/SURGERY              2445\n",
       "SPORT/EXERCISE                    2248\n",
       "ORTHOPAEDICS/SPINE                1829\n",
       "TISSUE/BIOMATERIAL                1693\n",
       "BONE                              1565\n",
       "NEURAL                            1364\n",
       "COMPARATIVE                       1281\n",
       "TENDON/LIGAMENT                   1241\n",
       "METHODS                           1152\n",
       "JOINT/CARTILAGE                   1143\n",
       "DENTAL/ORAL/FACIAL                1077\n",
       "MODELING                           992\n",
       "CELLULAR/SUBCELLULAR               974\n",
       "REHABILITATION                     941\n",
       "CARDIOVASCULAR/CARDIOPULMONARY     931\n",
       "ROBOTICS                           847\n",
       "EVOLUTION/ANTHROPOLOGY             798\n",
       "VISUAL/VESTIBULAR/EYE              622\n",
       "MUSCLE                             564\n",
       "VETERINARY/AGRICULTURAL            562\n",
       "TRAUMA/IMPACTTESTING               552\n",
       "PROSTHETICS/ORTHOTICS              443\n",
       "ERGONOMICS                         402\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() #same distribution as original data\n",
    "#to get a good list of anchor words, need to process data and see what occurs the most often? fdif?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor one word to the first topic\n",
    "anchor_words = ['robot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor the word 'nasa' to the first topic\n",
    "anchored_topic_model = ct.Corex(n_hidden=n_topics, seed=2) #50 topics\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This anchors the single word \"nasa\" to the first topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New topic: cruciate,cruciate ligament,anterior cruciate,anterior cruciate ligament,screw,cruciate ligament reconstruction,fixation,pedicle,screw fixation,pedicle screw\n"
     ]
    }
   ],
   "source": [
    "topic_words,_ = zip(*anchored_topic_model.get_topics(topic=0))\n",
    "print('New topic: ' + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can anchor multiple groups of words to multiple topics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor 'nasa' and 'space' to first topic, 'sports' and 'stadium' to second topic, so on...\n",
    "anchor_words = [['nasa', 'space'], ['sports', 'stadium'], ['politics', 'government'], ['love', 'hope']]\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: cruciate,cruciate ligament,anterior cruciate,anterior cruciate ligament,screw,cruciate ligament reconstruction,fixation,pedicle,screw fixation,pedicle screw\n",
      "1: robot,spinal cord,cord,spinal,walking,control,motor,recovery,randomized,trial\n",
      "2: element,finite,finite element,prosthesis,element analysis,finite element analysis,fractures,analysis,plate,locking\n",
      "3: muscle,lower,lower limb,lower extremity,limb,extremity,cerebral palsy,muscle activity,ground reaction,reaction\n",
      "4: model,rotator cuff,cuff,rotator,element model,finite element model,cuff repair,rotator cuff repair,rabbit model,rabbit\n",
      "5: bone,cortical bone,marrow,bone marrow,healing,trabecular bone,rats,bone loss,bone healing,trabecular\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in the above topic model, topics will no longer be sorted according to descending TC. Instead, the first topic will be the one with \"nasa\" and \"space\" anchored to it, the second topic will be the one with \"sports\" and \"stadium\" anchored to it, and so on.  \n",
    "\n",
    "Observe, the topic with \"love\" and \"hope\" anchored to it is less interpretable than the other three topics. This could be a sign that there is not a good topic around these two words, and one should consider if it is appropriate to anchor around them.\n",
    "\n",
    "We can continue to develop even more involved anchoring strategies. Here we anchor \"nasa\" by itself, as well as in two other topics each with \"politics\" and \"news\" to find different aspects around the word \"nasa\". We also create a fourth anchoring of \"war\" to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor with single words and groups of words\n",
    "anchor_words = ['nasa', ['nasa', 'politics'], ['nasa', 'news'], 'war']\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: nasa,space,orbit,launch,shuttle,moon,earth,lunar,satellite,commercial\n",
      "1: nasa,politics,research,gov,science,scientific,institute,organization,studies,providing\n",
      "2: news,nasa,insisting,edwards,hal,llnl,cso,cfv,nodak,admin\n",
      "3: war,israel,armenians,armenian,israeli,jews,soldiers,military,killed,history\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you do not specify the column labels through `words`, then you can still anchor by specifying the column indices of the features you wish to anchor on. You may also specify anchors using a mix of strings and indices if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing anchor strength:** the anchor strength controls how much weight CorEx puts towards maximizing the mutual information between the anchor words and their respective topics. Anchor strength should always be set at a value *greater than* 1, since setting anchor strength between 0 and 1 only recovers the unsupervised CorEx objective. Empirically, setting anchor strength from 1.5-3 seems to nudge the topic model towards the anchor words. Setting anchor strength greater than 5 is strongly enforcing that the CorEx topic model find a topic associated with the anchor words.\n",
    "\n",
    "We encourage users to experiment with the anchor strength and determine what values are best for their needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`vis_topic`** module provides support for outputting topics and visualizations of the CorEx topic model. The code below creates a results direcory named \"twenty\" in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print topics in text file\n"
     ]
    }
   ],
   "source": [
    "vt.vis_rep(topic_model, column_label=words, prefix='twenty-seven')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TACL paper details the theory of the CorEx topic model, its sparsity optimization, anchoring via the information bottleneck, comparisons to LDA, and anchoring experiments. The two papers from Greg Ver Steeg and Aram Galstyan develop the CorEx theory in general and provide further motivation and details of the underlying CorEx mechanisms. Hodas et al. demonstrated early CorEx topic model results and investigated an application of pointwise total correlations to quantify \"surprising\" documents.\n",
    "\n",
    "1. [Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244), Gallagher et al., TACL 2017.\n",
    "\n",
    "2. [Discovering Structure in High-Dimensional Data Through Correlation Explanation](https://arxiv.org/abs/1406.1222), Ver Steeg and Galstyan, NIPS 2014. \n",
    "\n",
    "3. [Maximally Informative Hierarchical Representions of High-Dimensional Data](https://arxiv.org/abs/1410.7404), Ver Steeg and Galstyan, AISTATS 2015.\n",
    "\n",
    "4. [Disentangling the Lexicons of Disaster Response in Twitter](https://dl.acm.org/citation.cfm?id=2741728), Hodas et al., WWW 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
