{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "classify_papers.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMbO/yZ7Ieb/JYTj1O/nXPk"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJG5shp08CGC"
   },
   "source": [
    "Uses Fine-Tuned BERT network to classify biomechanics papers from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6klLdde-78nI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1608242503750,
     "user_tz": 420,
     "elapsed": 2779,
     "user": {
      "displayName": "Ryan Alcantara",
      "photoUrl": "",
      "userId": "11016774030491622791"
     }
    },
    "outputId": "43978f89-caa3-4bd1-e708-c71db134cb6f"
   },
   "source": [
    "# Install & load libraries\n",
    "try:\n",
    "  from official.nlp import optimization\n",
    "except:\n",
    "  !pip install -q -U tf-models-official\n",
    "  from official.nlp import optimization\n",
    "try:\n",
    "  from Bio import Entrez\n",
    "except:\n",
    "  !pip install -q -U biopython\n",
    "  from Bio import Entrez\n",
    "try:\n",
    "  import tensorflow_text as text\n",
    "except:\n",
    "  !pip install -q -U tensorflow_text\n",
    "  import tensorflow_text as text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_hub as hub\n",
    "from google.colab import drive\n",
    "import datetime as dt\n",
    "today = dt.date.today()\n",
    "yesterday = today - dt.timedelta(days=1)\n",
    "week_ago = yesterday - dt.timedelta(days=7)  # week ago yesterday\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaSczx4IuQ0Y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1608242535661,
     "user_tz": 420,
     "elapsed": 23955,
     "user": {
      "displayName": "Ryan Alcantara",
      "photoUrl": "",
      "userId": "11016774030491622791"
     }
    },
    "outputId": "9016ee42-eff1-4917-e1c7-17af5b272642"
   },
   "source": [
    "\n",
    "# Define Search Criteria ----\n",
    "def search(query):\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='most recent',\n",
    "                            retmax='5000',\n",
    "                            retmode='xml',\n",
    "                            datetype='pdat',\n",
    "                            # reldate=7,  # only within n days from now\n",
    "                            mindate=min_date,\n",
    "                            maxdate=max_date,  # for searching date range\n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Perform Search and Pull Paper Titles ----\n",
    "def fetch_details(ids):\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Make the stop words for string cleaning ----\n",
    "def clean_str(text, stops):\n",
    "    text = BeautifulSoup(text, 'lxml').text\n",
    "    text = text.split()\n",
    "    return ' '.join([word for word in text if word not in stops])\n",
    "\n",
    "stop = list(stopwords.words('english'))\n",
    "stop_c = [string.capwords(word) for word in stop]\n",
    "for word in stop_c:\n",
    "    stop.append(word)\n",
    "\n",
    "new_stop = ['The', 'An', 'A', 'Do', 'Is', 'In', 'StringElement', \n",
    "            'NlmCategory', 'Label', 'attributes', 'INTRODUCTION',\n",
    "            'METHODS', 'BACKGROUND', 'RESULTS', 'CONCLUSIONS']\n",
    "for s in new_stop:\n",
    "    stop.append(s)\n",
    "\n",
    "# Search terms (can test string with Pubmed Advanced Search) ----\n",
    "# search_results = search('(Biomech*[Title/Abstract] OR locomot*[Title/Abstract])')\n",
    "min_date = week_ago.strftime('%m/%d/%Y')\n",
    "max_date = yesterday.strftime('%m/%d/%Y')\n",
    "search_results = search('(biomech*[Title/Abstract] OR locomot*[Title/Abstract] NOT opiod*[Title/Abstract] NOT pharm*[Journal] NOT mice[Title/Abstract] NOT rats[Title/Abstract] NOT elegans[Title/Abstract])')\n",
    "id_list = search_results['IdList']\n",
    "papers = fetch_details(id_list)\n",
    "print(len(papers['PubmedArticle']), 'Papers found')\n",
    "\n",
    "titles, full_titles, keywords, authors, links, journals, abstracts = ([] for i in range(7))\n",
    "\n",
    "for paper in papers['PubmedArticle']:\n",
    "    # clean and store titles, abstracts, and links\n",
    "    t = clean_str(paper['MedlineCitation']['Article']['ArticleTitle'], \n",
    "                  stop).replace('[','').replace(']','').capitalize()  # rm brackets that survived beautifulsoup, sentence case\n",
    "    titles.append(t)\n",
    "    full_titles.append(paper['MedlineCitation']['Article']['ArticleTitle'])\n",
    "    pmid = paper['MedlineCitation']['PMID']\n",
    "    links.append('[URL=\"https://www.ncbi.nlm.nih.gov/pubmed/{0}\"]{1}[/URL]'.format(pmid, t))\n",
    "    try:\n",
    "        abstracts.append(clean_str(paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0], \n",
    "                                    stop).replace('[','').replace(']','').capitalize())  # rm brackets that survived beautifulsoup, sentence case\n",
    "    except:\n",
    "        abstracts.append('')\n",
    "\n",
    "    # clean and store authors\n",
    "    auths = []\n",
    "    try:\n",
    "        for auth in paper['MedlineCitation']['Article']['AuthorList']:\n",
    "            try:  # see if there is a last name and initials\n",
    "                auth_name = [auth['LastName'], auth['Initials'] + ',']\n",
    "                auth_name = ' '.join(auth_name)\n",
    "                auths.append(auth_name)\n",
    "            except:\n",
    "                if 'LastName' in auth.keys():  # maybe they don't have initials\n",
    "                    auths.append(auth['LastName'] + ',')\n",
    "                else:  # no last name\n",
    "                    auths.append('')\n",
    "                    print(paper['MedlineCitation']['Article']['ArticleTitle'],\n",
    "                          'has an issue with an author name:')\n",
    "\n",
    "    except:\n",
    "        auths.append('AUTHOR NAMES ERROR')\n",
    "        print(paper['MedlineCitation']['Article']['ArticleTitle'], 'has no author list?')\n",
    "    # compile authors\n",
    "    authors.append(' '.join(auths).replace('[','').replace(']',''))  # rm brackets in names\n",
    "    # journal names\n",
    "    journals.append(paper['MedlineCitation']['Article']['Journal']['Title'].replace('[','').replace(']',''))  # rm brackets\n",
    "\n",
    "    # store keywords \n",
    "    if paper['MedlineCitation']['KeywordList'] != []:\n",
    "        kwds = []\n",
    "        for kw in paper['MedlineCitation']['KeywordList'][0]:\n",
    "            kwds.append(kw[:])\n",
    "        keywords.append(', '.join(kwds).lower())\n",
    "    else:\n",
    "      keywords.append('')\n",
    "\n",
    "# Put Titles, Abstracts, Authors, Journal, and Keywords into dataframe\n",
    "papers_df = pd.DataFrame({'title': titles,\n",
    "                          'keywords': keywords,\n",
    "                          'abstract': abstracts,\n",
    "                          'authors': authors,\n",
    "                          'journal': journals,\n",
    "                          'links': links,\n",
    "                          'raw_title': full_titles,\n",
    "                          'mindate': min_date,\n",
    "                          'maxdate': max_date})\n",
    "\n",
    "\n",
    "# remove papers with no title or no authors\n",
    "for index, row in papers_df.iterrows():\n",
    "    if row['title'] == '' or row['authors'] == 'AUTHOR NAMES ERROR':\n",
    "        papers_df.drop(index, inplace=True)\n",
    "papers_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# join titles and abstract\n",
    "papers_df['BERT_input'] = pd.DataFrame(papers_df['title'] + ' ' + papers_df['abstract'])\n",
    "\n",
    "# Load Fine-Tuned BERT Network ----\n",
    "model = tf.saved_model.load('/content/gdrive/My Drive/literature_update/Data/BERT32/')\n",
    "print('Loaded model from disk')\n",
    "\n",
    "# Load Label Encoder ----\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load('/content/gdrive/My Drive/literature_update/Data/BERT_label_encoder.npy')\n",
    "print('Loaded Label Encoder')\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "106 Papers found\n",
      "Loaded model from disk\n",
      "Loaded Label Encoder\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "55tVgCM--ktr"
   },
   "source": [
    "# Predict Paper Topic ----\n",
    "predicted_topic = model(papers_df['BERT_input'], training=False)  # will run out of GPU memory (14GB) if predicting more than ~2000 title+abstracts at once"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QvD9JQj7-2_e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1608242623587,
     "user_tz": 420,
     "elapsed": 694,
     "user": {
      "displayName": "Ryan Alcantara",
      "photoUrl": "",
      "userId": "11016774030491622791"
     }
    },
    "outputId": "5d3eec7e-997d-4bc5-fdf1-d50b2efab43a"
   },
   "source": [
    "# Determine Publications that BiomchBERT is unsure about ----\n",
    "topics, pred_val_str = ([] for i in range(2))\n",
    "\n",
    "for pred_prob in predicted_topic:\n",
    "    pred_val = np.max(pred_prob)\n",
    "    if pred_val > 1.5 * np.sort(pred_prob)[-2]:  # Is top confidence score more than 1.5x the second best confidence score?\n",
    "        topics.append(le.inverse_transform([np.argmax(pred_prob)])[0])\n",
    "        top1 = le.inverse_transform([np.argmax(pred_prob)])[0]\n",
    "        top2 = le.inverse_transform([list(pred_prob).index([np.sort(pred_prob)[-2]])])[0]\n",
    "        # pred_val_str.append(pred_val * 100)  # just report top category\n",
    "        pred_val_str.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\n",
    "            np.round(np.sort(pred_prob)[-2] * 100, 1)) + '% ' + str(top2))  # report top 2 categories\n",
    "    else:\n",
    "        topics.append('UNKNOWN')\n",
    "        top1 = le.inverse_transform([np.argmax(pred_prob)])[0]\n",
    "        top2 = le.inverse_transform([list(pred_prob).index([np.sort(pred_prob)[-2]])])[0]\n",
    "        pred_val_str.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\n",
    "            np.round(np.sort(pred_prob)[-2] * 100, 1)) + '% ' + str(top2))\n",
    "        \n",
    "papers_df['topic'] = topics\n",
    "papers_df['pred_val'] = pred_val_str\n",
    "\n",
    "print('BiomchBERT is unsure about {0} papers\\n'.format(len(papers_df[papers_df['topic'] == 'UNKNOWN'])))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "BiomchBERT is unsure about 7 papers\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeacO2b3WHFu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1608243698447,
     "user_tz": 420,
     "elapsed": 215,
     "user": {
      "displayName": "Ryan Alcantara",
      "photoUrl": "",
      "userId": "11016774030491622791"
     }
    },
    "outputId": "258e7862-d4e2-41f2-81c2-9b78e1386c35"
   },
   "source": [
    "# Prompt User to decide for BiomchBERT ----\n",
    "unknown_papers = papers_df[papers_df['topic'] == 'UNKNOWN']\n",
    "for indx, paper in unknown_papers.iterrows():\n",
    "  print(paper['title'])\n",
    "  print(paper['journal'])\n",
    "  print(paper['pred_val'])\n",
    "  print()\n",
    "  options = []\n",
    "  for cls in le.classes_:\n",
    "    if cls in paper['pred_val']:\n",
    "      options.append(cls)\n",
    "  choice = input('(1)st topic, (2)nd topic, or (r)emove paper? ')\n",
    "  print()\n",
    "  if choice == '1':\n",
    "    papers_df.iloc[indx]['topic'] = options[0]\n",
    "  elif choice == '2':\n",
    "    papers_df.iloc[indx]['topic'] = options[1]\n",
    "  elif choice == 'r':\n",
    "    papers_df.iloc[indx]['topic'] = '_REMOVE_'  # not deleted, but withheld from text file output\n",
    "\n",
    "print('Removing {0} papers\\n'.format(len(papers_df[papers_df['topic'] == '_REMOVE_'])))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Removing 2 papers\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Deqf5q7BdTAJ"
   },
   "source": [
    "# Double check that none of these papers were included in past literature updates ----\n",
    "# load prior papers\n",
    "# papers_df.to_csv('/content/gdrive/My Drive/literature_update/Literature_Updates/prior_papers.csv', index=False)  # run ONLY on first week\n",
    "prior_papers = pd.read_csv('/content/gdrive/My Drive/literature_update/Literature_Updates/prior_papers.csv')\n",
    "prior_papers.dropna(subset=['title'], inplace=True)\n",
    "prior_papers.reset_index(drop=True, inplace=True)\n",
    "match = papers_df['title'].isin(prior_papers['title'])  # boolean for matching titles between this week and prior papers\n",
    "papers_df.drop(papers_df[match].index, inplace=True)\n",
    "papers_df.reset_index(drop=True, inplace=True)\n",
    "updated_prior_papers = pd.concat([prior_papers, papers_df], axis=0)\n",
    "updated_prior_papers.reset_index(drop=True, inplace=True)\n",
    "updated_prior_papers.to_csv('/content/gdrive/My Drive/literature_update/Literature_Updates/prior_papers.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fQKyxoBF36B",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1608244653845,
     "user_tz": 420,
     "elapsed": 345,
     "user": {
      "displayName": "Ryan Alcantara",
      "photoUrl": "",
      "userId": "11016774030491622791"
     }
    },
    "outputId": "967cd454-f60e-4e4c-b452-d8306a38caf1"
   },
   "source": [
    "# Create Text File for Biomch-L ----\n",
    "# Compile papers grouped by topic\n",
    "txtname = '/content/gdrive/My Drive/literature_update/Literature_Updates/' + today.strftime(\"%Y-%m-%d\") + '-litupdate.txt'\n",
    "txt = open(txtname, 'w', encoding='utf-8')\n",
    "txt.write('[SIZE=16px][B]LITERATURE UPDATE[/B][/SIZE]\\n')\n",
    "txt.write(week_ago.strftime(\"%b %d\") + ' - '+ yesterday.strftime(\"%b %d, %Y\")+'\\n')  # a week ago from yesterday.\n",
    "txt.write(\n",
    "    \"\"\"\n",
    "Literature search terms: biomech* & locomot*\n",
    "\n",
    "Publications are classified by [URL=\"https://www.github.com/alcantarar/literature_update\"]BiomchBERT[/URL], a neural network trained on past Biomch-L Literature Updates. BiomchBERT is managed by [URL=\"https://www.twitter.com/Ryan_Alcantara_\"]Ryan Alcantara[/URL], a PhD Candidate at the University of Colorado Boulder. Each publication has a score (out of 100%) reflecting how confident BiomchBERT is that the publication belongs in a particular category (top 2 shown). If something doesn't look right, [URL=\"https://www.github.com/alcantarar/literature_update/issues/new\"]submit an issue[/URL].\n",
    "\n",
    "[URL=\"https://www.ryan-alcantara.com\"]www.ryan-alcantara.com[/URL]. \n",
    "\n",
    "*********************NOTE*********************\n",
    "- Not all articles have a DOI.\n",
    "- Some DOI links may not yet be available online.\n",
    "- Articles with no volume, issue or page numbers indicate that the article has not been published in paper form yet, but may be available in electronic form through the publisher\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# Write papers to text file grouped by topic ----\n",
    "topic_list = np.unique(papers_df.sort_values('topic')['topic'])\n",
    "\n",
    "for topic in topic_list:\n",
    "    papers_subset = pd.DataFrame(papers_df[papers_df.topic == topic].reset_index(drop=True))\n",
    "    txt.write('\\n')\n",
    "    # TOPIC NAME (with some cleaning)\n",
    "    if topic == '_REMOVE_':\n",
    "      continue\n",
    "    elif topic == 'UNKNOWN':\n",
    "        txt.write('[SIZE=16px][B]*Papers BiomchBERT is unsure how to classify*[/B][/SIZE]\\n')\n",
    "    elif topic == 'CARDIOVASCULAR/CARDIOPULMONARY':\n",
    "      topic = 'CARDIOVASCULAR/PULMONARY'\n",
    "      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n",
    "    elif topic == 'CELLULAR/SUBCELLULAR':\n",
    "      topic = 'CELLULAR'\n",
    "      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n",
    "    elif topic == 'ORTHOPAEDICS/SURGERY':\n",
    "      topic = 'ORTHOPAEDICS (SURGERY)'\n",
    "      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n",
    "    elif topic == 'ORTHOPAEDICS/SPINE':\n",
    "      topic = 'ORTHOPAEDICS (SPINE)'\n",
    "      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n",
    "    else:\n",
    "        txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n",
    "    # HYPERLINKED PAPERS, AUTHORS, JOURNAL NAME\n",
    "    for i, paper in enumerate(papers_subset['links']):\n",
    "        txt.write('[B]%s[/B] ' % paper)\n",
    "        txt.write('%s ' % papers_subset['authors'][i])\n",
    "        txt.write('[I]%s[/I]. ' % papers_subset['journal'][i])\n",
    "        # CONFIDENCE SCORE (BERT softmax categorical crossentropy)\n",
    "        try:\n",
    "            txt.write('(%.1f%%) \\n\\n' % papers_subset['pred_val'][i])\n",
    "        except:\n",
    "            txt.write('(%s)\\n\\n' % papers_subset['pred_val'][i]) \n",
    "\n",
    "txt.close()\n",
    "print('Literature Update Exported for Biomch-L')\n",
    "print('Location:', txtname)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Literature Update Exported for Biomch-L\n",
      "Location: /content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/2020-12-17-litupdate.txt\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}