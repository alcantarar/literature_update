{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classify_papers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLdrEhnYOJq8/JDWVC4fOK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XJG5shp08CGC"},"source":["Uses Fine-Tuned BERT network to classify biomechanics papers from PubMed"]},{"cell_type":"code","metadata":{"id":"6klLdde-78nI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608191436285,"user_tz":420,"elapsed":219,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"072e1ca0-5211-4e16-dbbb-590a328f5ced"},"source":["try:\r\n","  from official.nlp import optimization\r\n","except:\r\n","  !pip install -q -U tf-models-official\r\n","  from official.nlp import optimization\r\n","try:\r\n","  from Bio import Entrez\r\n","except:\r\n","  !pip install -q -U biopython\r\n","  from Bio import Entrez\r\n","try:\r\n","  import tensorflow_text as text\r\n","except:\r\n","  !pip install -q -U tensorflow_text\r\n","  import tensorflow_text as text\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import nltk\r\n","nltk.download('stopwords')\r\n","from nltk.corpus import stopwords\r\n","import tensorflow as tf\r\n","import string\r\n","import datetime\r\n","from bs4 import BeautifulSoup\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from tensorflow.keras.models import load_model\r\n","import tensorflow_hub as hub\r\n","from google.colab import drive\r\n","import datetime as dt\r\n","today = dt.date.today()\r\n","week_ago = today - dt.timedelta(days=7)\r\n","\r\n","drive.mount('/content/gdrive')"],"execution_count":99,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXsmSj-u9F9F","executionInfo":{"status":"ok","timestamp":1608191868675,"user_tz":420,"elapsed":20829,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"0a03c39e-cac9-4356-8c21-5a4754e6e62c"},"source":["\r\n","# Define Search Criteria ----\r\n","def search(query):\r\n","    Entrez.email = 'your.email@example.com'\r\n","    handle = Entrez.esearch(db='pubmed',\r\n","                            sort='most recent',\r\n","                            retmax='5000',\r\n","                            retmode='xml',\r\n","                            datetype='pdat',\r\n","                            reldate=7,  # only within n days from now\r\n","                            # mindate='2019/03/25',\r\n","                            # maxdate='2019/03/27',  # for searching date range\r\n","                            term=query)\r\n","    results = Entrez.read(handle)\r\n","    return results\r\n","\r\n","\r\n","# search terms (can test string with Pubmed Advanced Search)\r\n","# search_results = search('(Biomech*[Title/Abstract] OR locomot*[Title/Abstract])')\r\n","search_results = search('(biomech*[Title/Abstract] OR locomot*[Title/Abstract] NOT opiod*[Title/Abstract] NOT pharm*[Journal] NOT mice[Title/Abstract] NOT rats[Title/Abstract])')\r\n","\r\n","# Perform Search and Save Paper Titles ----\r\n","def fetch_details(ids):\r\n","    Entrez.email = 'your.email@example.com'\r\n","    handle = Entrez.efetch(db='pubmed',\r\n","                           retmode='xml',\r\n","                           id=ids)\r\n","    results = Entrez.read(handle)\r\n","    return results\r\n","\r\n","\r\n","id_list = search_results['IdList']\r\n","papers = fetch_details(id_list)\r\n","print(\"\")\r\n","\r\n","# Definitely could change these loops for speed.\r\n","papers_length = len(papers['PubmedArticle'])\r\n","titles = [None] * papers_length\r\n","full_titles = [None] * papers_length\r\n","keywords = [None] * papers_length\r\n","authors = [None] * papers_length\r\n","links = [None] * papers_length\r\n","journals = [None] * papers_length\r\n","abstracts = [None] * papers_length\r\n","\r\n","\r\n","def clean_str(text, stops):\r\n","    text = BeautifulSoup(text, 'lxml').text\r\n","    text = text.split()\r\n","    return ' '.join([word for word in text if word not in stops])\r\n","\r\n","\r\n","# Make the Stop Words for string cleaning\r\n","stop = list(stopwords.words('english'))\r\n","stop_c = [string.capwords(word) for word in stop]\r\n","for word in stop_c:\r\n","    stop.append(word)\r\n","stop.append('The')\r\n","stop.append('An')\r\n","stop.append('A')\r\n","stop.append('Do')\r\n","stop.append('Is')\r\n","stop.append('In')\r\n","new_stop = ['StringElement', 'NlmCategory', 'Label', 'attributes', 'INTRODUCTION',\r\n","            'METHODS', 'BACKGROUND', 'RESULTS', 'CONCLUSIONS']\r\n","for s in new_stop:\r\n","    stop.append(s)\r\n","\r\n","for i, paper in enumerate(papers['PubmedArticle']):\r\n","    titles[i] = clean_str(papers['PubmedArticle'][i]['MedlineCitation']['Article']['ArticleTitle'], stop)\r\n","    full_titles[i] = papers['PubmedArticle'][i]['MedlineCitation']['Article']['ArticleTitle']\r\n","    try:\r\n","        abstracts[i] = \\\r\n","            clean_str(papers['PubmedArticle'][i]['MedlineCitation']['Article']['Abstract']['AbstractText'][0], stop)\r\n","    except:\r\n","        abstracts[i] = ''\r\n","print(np.size(titles), 'Papers found')\r\n","\r\n","# Pull information from PubMed Results ----\r\n","# Format title, journal, authors in markdown friendly manner\r\n","\r\n","for i, paper in enumerate(papers['PubmedArticle']):\r\n","    if paper['MedlineCitation']['Article']['ArticleTitle'] == '':\r\n","        continue\r\n","    if paper['MedlineCitation']['Article']['ArticleTitle'][0] == '[':\r\n","        links[i] = '[URL=\"https://www.ncbi.nlm.nih.gov/pubmed/%s\"]%s[/URL]' % \\\r\n","                   (paper['MedlineCitation']['PMID'],\r\n","                    BeautifulSoup(paper['MedlineCitation']['Article']['ArticleTitle'][1:-1], 'lxml').text\r\n","                    )\r\n","    else:\r\n","        links[i] = '[URL=\"https://www.ncbi.nlm.nih.gov/pubmed/%s\"]%s[/URL]' % \\\r\n","                   (paper['MedlineCitation']['PMID'],\r\n","                    BeautifulSoup(paper['MedlineCitation']['Article']['ArticleTitle'], 'lxml').text\r\n","                   )\r\n","\r\n","    auths = []\r\n","    try:\r\n","        for auth in paper['MedlineCitation']['Article']['AuthorList']:\r\n","            try:\r\n","                auth_name = [auth['LastName'], auth['Initials'] + ',']\r\n","                auth_name = ' '.join(auth_name)\r\n","                auths.append(auth_name)\r\n","            except:\r\n","                auths.append('')\r\n","                print(paper['MedlineCitation']['Article']['ArticleTitle'],\r\n","                      'has an issue with an author name')\r\n","    except:\r\n","        auths.append('AUTHOR NAMES ERROR')\r\n","        print(paper['MedlineCitation']['Article']['ArticleTitle'], 'has no author list?')\r\n","    authors[i] = ' '.join(auths)\r\n","    journals[i] = '%s' % (paper['MedlineCitation']['Article']['Journal']['Title'])\r\n","    # store keywords \r\n","    if paper['MedlineCitation']['KeywordList'] != []:\r\n","        kwds = []\r\n","        for kw in paper['MedlineCitation']['KeywordList'][0]:\r\n","            kwds.append(kw[:])\r\n","        keywords[i] = ' '.join(kwds)\r\n","\r\n","# Clean up title and word strings ----\r\n","titles = [t.lower() for t in titles]  # same case\r\n","titles = [t.replace('<sub>', ' ').replace('</sub>', '') for t in titles]  # subscript\r\n","titles = [t.replace('<i>', ' ').replace('</i>', '') for t in titles]  # italics\r\n","titles = [t.replace('[', '').replace(']', '') for t in titles]  # remove brackets from html parser\r\n","# clean up keywords\r\n","keywords2 = []\r\n","for k in keywords:\r\n","    if k is None:\r\n","        keywords2.append('')\r\n","    else:\r\n","        keywords2.append(k.lower())\r\n","keywords = keywords2\r\n","# keywords = [k.lower() for k in keywords] #same case\r\n","\r\n","\r\n","# Loading the Network ----\r\n","# Load Fine-Tuned BERT model\r\n","model = tf.saved_model.load('/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Data/BERT32/')\r\n","print('Loaded model from disk')\r\n","\r\n","# Load Label Encoder\r\n","le = LabelEncoder()\r\n","le.classes_ = np.load('/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Data/BERT_label_encoder.npy')\r\n","print('Loaded Label Encoder')\r\n","\r\n","# get titles for this week's literature update\r\n","papers_df = pd.DataFrame({'title': titles,\r\n","                          'keywords': keywords,\r\n","                          'abstract': abstracts,\r\n","                          'author': authors,\r\n","                          'journal': journals})\r\n","\r\n","for index, row in papers_df.iterrows():\r\n","    if row['title'] == '' or row['author'] == 'AUTHOR NAMES ERROR':\r\n","    # if row['abstract'] == '' or row['author'] == 'AUTHOR NAMES ERROR' or row['title'] == '':\r\n","\r\n","        papers_df.drop(index, inplace=True)\r\n","\r\n","# join titles and abstract\r\n","papers_df['everything'] = pd.DataFrame(papers_df['title'].astype(str) + papers_df['abstract'].astype(str))\r\n"],"execution_count":107,"outputs":[{"output_type":"stream","text":["\n","80 Papers found\n","Biomechanical properties of a novel locking compression plate to stabilize oblique tibial osteotomies in buffaloes. has an issue with an author name\n","WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fbc61531378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fbc61531488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fbc63fcd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","Loaded model from disk\n","Loaded Label Encoder\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"55tVgCM--ktr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608191869735,"user_tz":420,"elapsed":20849,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"e85d6029-bd79-4fe9-a0d9-1da5ecf76576"},"source":["predicted_topic = model(papers_df['everything'], training=False)  # will run out of GPU memory if predicting more than ~2000 title+abstracts"],"execution_count":108,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fbc6246f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QvD9JQj7-2_e","executionInfo":{"status":"ok","timestamp":1608191870872,"user_tz":420,"elapsed":21611,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}}},"source":["topics = []\r\n","pred_val = []\r\n","pred_val_vec = []\r\n","title_temp = []\r\n","indx = []\r\n","\r\n","for k, top_val in enumerate(predicted_topic):\r\n","    if k in papers_df.index:\r\n","        pred_val = np.max(top_val)\r\n","        if pred_val > 1.5 * np.sort(top_val)[-2]:\r\n","            indx.append(k)\r\n","            topics.append(le.inverse_transform([np.argmax(top_val)])[0])\r\n","            title_temp.append(papers_df['title'][k])\r\n","            top1 = le.inverse_transform([np.argmax(top_val)])[0]\r\n","            top2 = le.inverse_transform([list(top_val).index([np.sort(top_val)[-2]])])[0]\r\n","            # pred_val_vec.append(pred_val * 100)  # just report top category\r\n","            pred_val_vec.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\r\n","                np.round(np.sort(top_val)[-2] * 100, 1)) + '% ' + str(top2))  # report top 2 categories\r\n","        else:\r\n","            indx.append(k)\r\n","            topics.append('unknown')\r\n","            title_temp.append(papers_df['title'][k])\r\n","            top1 = le.inverse_transform([np.argmax(top_val)])[0]\r\n","            top2 = le.inverse_transform([list(top_val).index([np.sort(top_val)[-2]])])[0]\r\n","            pred_val_vec.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\r\n","                np.round(np.sort(top_val)[-2] * 100, 1)) + '% ' + str(top2))\r\n","    else:\r\n","        print('Skipping prediction of paper #: ' + str(k))\r\n","papers_df = pd.DataFrame(data={'title': title_temp,\r\n","                               'topic': topics,\r\n","                               'pred_val': pred_val_vec})\r\n"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhK2S6wJ_DOU","executionInfo":{"status":"ok","timestamp":1608191870873,"user_tz":420,"elapsed":19743,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"564bd6f9-da4b-4f7b-f471-70f8c6fcd772"},"source":["# Save Titles and Topics ----\r\n","\r\n","# add info for github markdown format\r\n","papers_df['title'] = [title if title[1] is not '[' else title[1:-1] for title in papers_df['title']]\r\n","papers_df['authors'] = [authors[k] if authors[k][1] is not '[' else authors[1:-1] for k in indx]\r\n","papers_df['journal'] = [journals[k] for k in indx]\r\n","papers_df['links'] = [links[k] for k in indx]\r\n","papers_df['full_title'] = [full_titles[k] for k in indx]\r\n","# generate filename\r\n","now = datetime.datetime.now()\r\n","strings = [str(now.year), str(now.month), str(now.day), 'litupdate.csv']\r\n","fname = '/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/' + '-'.join(strings)\r\n","strings = [str(now.year), str(now.month), str(now.day), 'litupdate.md']\r\n","mdname = '/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/' + '-'.join(strings)\r\n","strings = [str(now.year), str(now.month), str(now.day), 'litupdate']\r\n","urlname = '-'.join(strings)\r\n","\r\n","print('Filename: ', mdname)"],"execution_count":110,"outputs":[{"output_type":"stream","text":["Filename:  /content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/2020-12-17-litupdate.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fQKyxoBF36B","executionInfo":{"status":"ok","timestamp":1608191870873,"user_tz":420,"elapsed":19257,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"a28e08bc-7830-440f-8425-2696ab993c30"},"source":["# Create Text File for Biomch-L ----\r\n","# Compile papers grouped by topic\r\n","txtname = '/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/' + '-'.join([str(now.year), str(now.month), str(now.day), 'litupdate.txt'])\r\n","txt = open(txtname, 'w', encoding='utf-8')\r\n","txt.write('LITERATURE UPDATE\\n')\r\n","txt.write(week_ago.strftime(\"%b %d\") + ' - '+ today.strftime(\"%b %d, %Y\")+'\\n')\r\n","txt.write(\r\n","    \"\"\"\r\n","Literature search terms: biomech* & locomot*\r\n","\r\n","Publications are classified by [URL=\"https://www.github.com/alcantarar/literature_update\"]BiomchBERT[/URL], a neural network trained on past Biomch-L Literature Updates. BiomchBERT is managed by [URL=\"https://www.twitter.com/Ryan_Alcantara_\"]Ryan Alcantara[/URL], a PhD Candidate at the University of Colorado Boulder. Each publication has a score (out of 100%) reflecting how confident BiomchBERT is that the publication belongs in a particular category (top 2 shown). If something doesn't look right, email [EMAIL=\"biomchBERT@gmail.com\"]biomchBERT@gmail.com[/EMAIL].\r\n","\r\n","[URL=\"https://www.ryan-alcantara.com\"]www.ryan-alcantara.com[/URL]. \r\n","\r\n","*********************NOTE*********************\r\n","- Not all articles have a DOI.\r\n","- Some DOI links may not yet be available online.\r\n","- Articles with no volume, issue or page numbers indicate that the article has not been published in paper form yet, but may be available in electronic form through the publisher\r\n","\r\n","\r\n","    \"\"\"\r\n","    )\r\n","\r\n","# Write papers to text file grouped by topic ----\r\n","topic_list = np.unique(papers_df.sort_values('topic')['topic'])\r\n","\r\n","for topic in topic_list:\r\n","    papers_subset = pd.DataFrame(papers_df[papers_df.topic == topic].reset_index(drop=True))\r\n","    txt.write('\\n')\r\n","    # TOPIC NAME (with some cleaning)\r\n","    if topic == '_REMOVE_':\r\n","      continue\r\n","    elif topic == 'unknown':\r\n","        txt.write('[SIZE=16px][B]*Papers BiomchBERT is unsure how to classify*[/B][/SIZE]\\n')\r\n","    elif topic == 'CARDIOVASCULAR/CARDIOPULMONARY':\r\n","      topic = 'CARDIOVASCULAR/PULMONARY'\r\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\r\n","    elif topic == 'CELLULAR/SUBCELLULAR':\r\n","      topic = 'CELLULAR'\r\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\r\n","    elif topic == 'ORTHOPAEDICS/SURGERY':\r\n","      topic = 'ORTHOPAEDICS (SURGERY)'\r\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\r\n","    elif topic == 'ORTHOPAEDICS/SPINE':\r\n","      topic = 'ORTHOPAEDICS (SPINE)'\r\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\r\n","    else:\r\n","        txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\r\n","    # HYPERLINKED PAPERS, AUTHORS, JOURNAL NAME\r\n","    for i, paper in enumerate(papers_subset['links']):\r\n","        txt.write('[B]%s[/B] ' % paper)\r\n","        txt.write('%s ' % papers_subset['authors'][i])\r\n","        txt.write('[I]%s[/I]. ' % papers_subset['journal'][i])\r\n","        # CONFIDENCE SCORE (BERT softmax categorical crossentropy)\r\n","        try:\r\n","            txt.write('(%.1f%%) \\n\\n' % papers_subset['pred_val'][i])\r\n","        except:\r\n","            txt.write('(%s)\\n\\n' % papers_subset['pred_val'][i]) \r\n","\r\n","txt.close()\r\n","print('Literature Update Exported for Biomch-L')\r\n","print('Location:', txtname)"],"execution_count":111,"outputs":[{"output_type":"stream","text":["Literature Update Exported for Biomch-L\n","Location: /content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/2020-12-17-litupdate.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKb0-y2P_Pl9","executionInfo":{"status":"ok","timestamp":1608164298476,"user_tz":420,"elapsed":276,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"17a9dd9a-fe7a-44ed-a4d3-1e4b29615d05"},"source":["# # Create Markdown for ryan-alcantara.com ----\r\n","# # Compile papers grouped by topic\r\n","# md_file = open(mdname, 'w', encoding='utf-8')\r\n","# md_file.write('---\\n')\r\n","# md_file.write('layout: single\\n')\r\n","# md_file.write('title: Biomechanics Literature Update\\n')\r\n","# md_file.write('collection: literature\\n')\r\n","# md_file.write('permalink: /literature/%s\\n' % urlname)\r\n","# md_file.write('excerpt: <br>\\n')\r\n","# md_file.write('toc: true\\n')\r\n","# md_file.write('toc_sticky: true\\n')\r\n","# md_file.write('toc_label: Topics\\n')\r\n","# md_file.write('---\\n')\r\n","\r\n","# # tidy up topic strings\r\n","# topic_list = np.unique(papers_df.sort_values('topic')['topic'])\r\n","# ss = [s for s in topic_list if 'UNIQUE' in s]\r\n","# for i, t in enumerate(topic_list):\r\n","#     if 'UNIQUE' in t:\r\n","#         topic_list[i] = 'UNIQUE TOPIC'\r\n","#         print('Assigned unique topic: ' + str(i))\r\n","#     if 'IMPACT' in t:\r\n","#         topic_list[i] = 'TRAUMA/IMPACT'\r\n","\r\n","\r\n","# # Make Markdown File ----\r\n","# st = '### Created by: [Ryan Alcantara](https://twitter.com/Ryan_Alcantara_)'\r\n","# st = st + ' & [Gary Bruening](https://twitter.com/garebearbru) -'\r\n","# st = st + ' University of Colorado Boulder\\n\\n'\r\n","# md_file.write(st)\r\n","# for topic in topic_list:\r\n","#     papers_subset = pd.DataFrame(papers_df[papers_df.topic == topic].reset_index(drop=True))\r\n","#     md_file.write('----\\n')\r\n","#     if topic == 'unknown':\r\n","#         md_file.write('# %s: Num=%i\\n' % (topic, len(papers_subset)))\r\n","#     else:\r\n","#         md_file.write('# %s\\n' % topic)\r\n","#     md_file.write('----\\n')\r\n","#     md_file.write('\\n')\r\n","#     md_file.write('[Back to top](#created-by-ryan-alcantara--gary-bruening---university-of-colorado-boulder)')\r\n","#     md_file.write('\\n')\r\n","#     for i, paper in enumerate(papers_subset['links']):\r\n","#         md_file.write('%s\\n' % paper)\r\n","#         md_file.write('%s\\n' % papers_subset['authors'][i])\r\n","#         md_file.write('%s.  \\n' % papers_subset['journal'][i])\r\n","#         try:\r\n","#             md_file.write('(%.1f%%) \\n' % papers_subset['pred_val'][i])\r\n","#         except:\r\n","#             md_file.write('%s\\n' % papers_subset['pred_val'][i])\r\n","#         md_file.write('\\n')\r\n","\r\n","# md_file.close()\r\n","# print('Literature Update Exported as Markdown')\r\n","# print('Location:', mdname)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Literature Update Exported as Markdown\n","Location: /content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/2020-12-17-litupdate.md\n"],"name":"stdout"}]}]}