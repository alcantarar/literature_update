{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classify_papers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMl71e90ZMGDY/FejPx37l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XJG5shp08CGC"},"source":["Uses Fine-Tuned BERT network to classify biomechanics papers from PubMed"]},{"cell_type":"code","metadata":{"id":"6klLdde-78nI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608095772495,"user_tz":420,"elapsed":33690,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"4a40961f-8dde-4e7f-bc08-3ed51ff673d0"},"source":["try:\r\n","  from official.nlp import optimization\r\n","except:\r\n","  !pip install -q -U tf-models-official\r\n","  from official.nlp import optimization\r\n","try:\r\n","  from Bio import Entrez\r\n","except:\r\n","  !pip install -q -U biopython\r\n","  from Bio import Entrez\r\n","try:\r\n","  import tensorflow_text as text\r\n","except:\r\n","  !pip install -q -U tensorflow_text\r\n","  import tensorflow_text as text\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import nltk\r\n","nltk.download('stopwords')\r\n","from nltk.corpus import stopwords\r\n","import tensorflow as tf\r\n","import string\r\n","import datetime\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from tensorflow.keras.models import load_model\r\n","import tensorflow_hub as hub\r\n","from google.colab import drive\r\n","\r\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 849kB 18.3MB/s \n","\u001b[K     |████████████████████████████████| 358kB 39.7MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 52.6MB/s \n","\u001b[K     |████████████████████████████████| 36.7MB 86kB/s \n","\u001b[K     |████████████████████████████████| 102kB 15.1MB/s \n","\u001b[K     |████████████████████████████████| 174kB 53.8MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 2.3MB 18.5MB/s \n","\u001b[K     |████████████████████████████████| 2.6MB 16.0MB/s \n","\u001b[?25h[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXsmSj-u9F9F","executionInfo":{"status":"ok","timestamp":1608095818075,"user_tz":420,"elapsed":38327,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"a0b874f5-c6ea-4fa1-ac22-fcf9771495f7"},"source":["\r\n","# Define Search Criteria ----\r\n","def search(query):\r\n","    Entrez.email = 'your.email@example.com'\r\n","    handle = Entrez.esearch(db='pubmed',\r\n","                            sort='most recent',\r\n","                            retmax='5000',\r\n","                            retmode='xml',\r\n","                            datetype='pdat',\r\n","                            reldate=7,  # only within n days from now\r\n","                            # mindate='2019/03/25',\r\n","                            # maxdate='2019/03/27',  # for searching date range\r\n","                            term=query)\r\n","    results = Entrez.read(handle)\r\n","    return results\r\n","\r\n","\r\n","# search terms (can test string with Pubmed Advanced Search)\r\n","search_results = search('(Biomech*[Title/Abstract] OR locomot*[Title/Abstract])')\r\n","\r\n","\r\n","# Perform Search and Save Paper Titles ----\r\n","def fetch_details(ids):\r\n","    Entrez.email = 'your.email@example.com'\r\n","    handle = Entrez.efetch(db='pubmed',\r\n","                           retmode='xml',\r\n","                           id=ids)\r\n","    results = Entrez.read(handle)\r\n","    return results\r\n","\r\n","\r\n","id_list = search_results['IdList']\r\n","papers = fetch_details(id_list)\r\n","print(\"\")\r\n","\r\n","# Definitely could change these loops for speed.\r\n","papers_length = len(papers['PubmedArticle'])\r\n","titles = [None] * papers_length\r\n","full_titles = [None] * papers_length\r\n","keywords = [None] * papers_length\r\n","authors = [None] * papers_length\r\n","links = [None] * papers_length\r\n","journals = [None] * papers_length\r\n","abstracts = [None] * papers_length\r\n","\r\n","\r\n","def clean_str(text, stops):\r\n","    text = text.split()\r\n","    return ' '.join([word for word in text if word not in stops])\r\n","\r\n","\r\n","# Make the Stop Words for string cleaning\r\n","stop = list(stopwords.words('english'))\r\n","stop_c = [string.capwords(word) for word in stop]\r\n","for word in stop_c:\r\n","    stop.append(word)\r\n","stop.append('The')\r\n","stop.append('An')\r\n","stop.append('A')\r\n","stop.append('Do')\r\n","stop.append('Is')\r\n","stop.append('In')\r\n","new_stop = ['StringElement', 'NlmCategory', 'Label', 'attributes', 'INTRODUCTION',\r\n","            'METHODS', 'BACKGROUND', 'RESULTS', 'CONCLUSIONS']\r\n","for s in new_stop:\r\n","    stop.append(s)\r\n","\r\n","for i, paper in enumerate(papers['PubmedArticle']):\r\n","    titles[i] = clean_str(papers['PubmedArticle'][i]['MedlineCitation']['Article']['ArticleTitle'], stop)\r\n","    full_titles[i] = papers['PubmedArticle'][i]['MedlineCitation']['Article']['ArticleTitle']\r\n","    try:\r\n","        abstracts[i] = \\\r\n","            clean_str(papers['PubmedArticle'][i]['MedlineCitation']['Article']['Abstract']['AbstractText'][0], stop)\r\n","    except:\r\n","        abstracts[i] = ''\r\n","print(np.size(titles), 'Papers found')\r\n","\r\n","# Pull information from PubMed Results ----\r\n","# Format title, journal, authors in markdown friendly manner\r\n","\r\n","for i, paper in enumerate(papers['PubmedArticle']):\r\n","    if paper['MedlineCitation']['Article']['ArticleTitle'] == '':\r\n","        continue\r\n","    if paper['MedlineCitation']['Article']['ArticleTitle'][0] == '[':\r\n","        links[i] = \"* [%s](https://www.ncbi.nlm.nih.gov/pubmed/%s)\" % \\\r\n","                   (paper['MedlineCitation']['Article']['ArticleTitle'][1:-1],\r\n","                    paper['MedlineCitation']['PMID'])\r\n","    else:\r\n","        links[i] = \"* [%s](https://www.ncbi.nlm.nih.gov/pubmed/%s)\" % \\\r\n","                   (paper['MedlineCitation']['Article']['ArticleTitle'],\r\n","                    paper['MedlineCitation']['PMID'])\r\n","\r\n","    auths = []\r\n","    try:\r\n","        for auth in paper['MedlineCitation']['Article']['AuthorList']:\r\n","            try:\r\n","                auth_name = [auth['LastName'], auth['Initials'] + ',']\r\n","                auth_name = ' '.join(auth_name)\r\n","                auths.append(auth_name)\r\n","            except:\r\n","                auths.append('')\r\n","                print(paper['MedlineCitation']['Article']['ArticleTitle'],\r\n","                      'has an issue with an author name')\r\n","    except:\r\n","        auths.append('AUTHOR NAMES ERROR')\r\n","        print(paper['MedlineCitation']['Article']['ArticleTitle'], 'has no author list?')\r\n","    authors[i] = ' '.join(auths)\r\n","    journals[i] = '*%s*' % (paper['MedlineCitation']['Article']['Journal']['Title'])\r\n","    # store keywords \r\n","    if paper['MedlineCitation']['KeywordList'] != []:\r\n","        kwds = []\r\n","        for kw in paper['MedlineCitation']['KeywordList'][0]:\r\n","            kwds.append(kw[:])\r\n","        keywords[i] = ' '.join(kwds)\r\n","\r\n","# Clean up title and word strings ----\r\n","titles = [t.lower() for t in titles]  # same case\r\n","titles = [t.replace('<sub>', ' ').replace('</sub>', '') for t in titles]  # subscript\r\n","titles = [t.replace('<i>', ' ').replace('</i>', '') for t in titles]  # italics\r\n","titles = [t.replace('[', '').replace(']', '') for t in titles]  # remove brackets from html parser\r\n","# clean up keywords\r\n","keywords2 = []\r\n","for k in keywords:\r\n","    if k is None:\r\n","        keywords2.append('')\r\n","    else:\r\n","        keywords2.append(k.lower())\r\n","keywords = keywords2\r\n","# keywords = [k.lower() for k in keywords] #same case\r\n","\r\n","\r\n","# Loading the Network ----\r\n","# Load Fine-Tuned BERT model\r\n","model = tf.saved_model.load('/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Data/BERT32/')\r\n","print('Loaded model from disk')\r\n","\r\n","# Load Label Encoder\r\n","le = LabelEncoder()\r\n","le.classes_ = np.load('/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Data/BERT_label_encoder.npy')\r\n","print('Loaded Label Encoder')\r\n","\r\n","# get titles for this week's literature update\r\n","papers_df = pd.DataFrame({'title': titles,\r\n","                          'keywords': keywords,\r\n","                          'abstract': abstracts,\r\n","                          'author': authors,\r\n","                          'journal': journals})\r\n","\r\n","for index, row in papers_df.iterrows():\r\n","    if row['title'] == '':\r\n","    # if row['abstract'] == '' or row['author'] == 'AUTHOR NAMES ERROR' or row['title'] == '':\r\n","\r\n","        papers_df.drop(index, inplace=True)\r\n","\r\n","# join titles and abstract\r\n","papers_df['everything'] = pd.DataFrame(papers_df['title'].astype(str) + papers_df['abstract'].astype(str))\r\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\n","84 Papers found\n","\n","Loaded model from disk\n","\n","Loaded Label Encoder\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"55tVgCM--ktr","executionInfo":{"status":"ok","timestamp":1608095851603,"user_tz":420,"elapsed":1376,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}}},"source":["predicted_topic = model(papers_df['everything'], training=False)\r\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvD9JQj7-2_e","executionInfo":{"status":"ok","timestamp":1608095882192,"user_tz":420,"elapsed":324,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}}},"source":["topics = []\r\n","pred_val = []\r\n","pred_val_vec = []\r\n","title_temp = []\r\n","indx = []\r\n","\r\n","for k, top_val in enumerate(predicted_topic):\r\n","    if k in papers_df.index:\r\n","        pred_val = np.max(top_val)\r\n","        if pred_val > 1.5 * np.sort(top_val)[-2]:\r\n","            indx.append(k)\r\n","            topics.append(le.inverse_transform([np.argmax(top_val)])[0])\r\n","            title_temp.append(papers_df['title'][k])\r\n","            pred_val_vec.append(pred_val * 100)\r\n","        else:\r\n","            indx.append(k)\r\n","            topics.append('unknown')\r\n","            title_temp.append(papers_df['title'][k])\r\n","            top1 = le.inverse_transform([np.argmax(top_val)])[0]\r\n","            top2 = le.inverse_transform([list(top_val).index([np.sort(top_val)[-2]])])[0]\r\n","            pred_val_vec.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\r\n","                np.round(np.sort(top_val)[-2] * 100, 1)) + '% ' + str(top2))\r\n","    else:\r\n","        print('Skipping prediction of paper #: ' + str(k))\r\n","papers_df = pd.DataFrame(data={'title': title_temp,\r\n","                               'topic': topics,\r\n","                               'pred_val': pred_val_vec})\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhK2S6wJ_DOU","executionInfo":{"status":"ok","timestamp":1608096021901,"user_tz":420,"elapsed":186,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"c6aac2f1-fb7e-40f6-a935-618aca371c57"},"source":["# Save Titles and Topics ----\r\n","\r\n","# add info for github markdown format\r\n","papers_df['title'] = [title if title[1] is not '[' else title[1:-1] for title in papers_df['title']]\r\n","papers_df['authors'] = [authors[k] if authors[k][1] is not '[' else authors[1:-1] for k in indx]\r\n","papers_df['journal'] = [journals[k] for k in indx]\r\n","papers_df['links'] = [links[k] for k in indx]\r\n","papers_df['full_title'] = [full_titles[k] for k in indx]\r\n","# generate filename\r\n","now = datetime.datetime.now()\r\n","strings = [str(now.year), str(now.month), str(now.day), 'litupdate.csv']\r\n","fname = '/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/' + '-'.join(strings)\r\n","strings = [str(now.year), str(now.month), str(now.day), 'litupdate.md']\r\n","mdname = '/content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/' + '-'.join(strings)\r\n","strings = [str(now.year), str(now.month), str(now.day), 'litupdate']\r\n","urlname = '-'.join(strings)\r\n","\r\n","print('Filename: ', fname)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Filename:  /content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/2020-12-16-litupdate.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKb0-y2P_Pl9","executionInfo":{"status":"ok","timestamp":1608096023980,"user_tz":420,"elapsed":184,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"781a8667-dd0a-46af-adad-c540bdee0f0a"},"source":["\r\n","# Compile papers grouped by topic\r\n","md_file = open(mdname, 'w', encoding='utf-8')\r\n","md_file.write('---\\n')\r\n","md_file.write('layout: single\\n')\r\n","md_file.write('title: Biomechanics Literature Update\\n')\r\n","md_file.write('collection: literature\\n')\r\n","md_file.write('permalink: /literature/%s\\n' % urlname)\r\n","md_file.write('excerpt: <br>\\n')\r\n","md_file.write('toc: true\\n')\r\n","md_file.write('toc_sticky: true\\n')\r\n","md_file.write('toc_label: Topics\\n')\r\n","md_file.write('---\\n')\r\n","\r\n","# tidy up topic strings\r\n","topic_list = np.unique(papers_df.sort_values('topic')['topic'])\r\n","ss = [s for s in topic_list if 'UNIQUE' in s]\r\n","for i, t in enumerate(topic_list):\r\n","    if 'UNIQUE' in t:\r\n","        topic_list[i] = 'UNIQUE TOPIC'\r\n","        print('Assigned unique topic: ' + str(i))\r\n","    if 'IMPACT' in t:\r\n","        topic_list[i] = 'TRAUMA/IMPACT'\r\n","\r\n","\r\n","# Make Markdown File ----\r\n","st = '### Created by: [Ryan Alcantara](https://twitter.com/Ryan_Alcantara_)'\r\n","st = st + ' & [Gary Bruening](https://twitter.com/garebearbru) -'\r\n","st = st + ' University of Colorado Boulder\\n\\n'\r\n","md_file.write(st)\r\n","for topic in topic_list:\r\n","    papers_subset = pd.DataFrame(papers_df[papers_df.topic == topic].reset_index(drop=True))\r\n","    md_file.write('----\\n')\r\n","    if topic == 'unknown':\r\n","        md_file.write('# %s: Num=%i\\n' % (topic, len(papers_subset)))\r\n","    else:\r\n","        md_file.write('# %s\\n' % topic)\r\n","    md_file.write('----\\n')\r\n","    md_file.write('\\n')\r\n","    md_file.write('[Back to top](#created-by-ryan-alcantara--gary-bruening---university-of-colorado-boulder)')\r\n","    md_file.write('\\n')\r\n","    for i, paper in enumerate(papers_subset['links']):\r\n","        md_file.write('%s\\n' % paper)\r\n","        md_file.write('%s\\n' % papers_subset['authors'][i])\r\n","        md_file.write('%s.  \\n' % papers_subset['journal'][i])\r\n","        try:\r\n","            md_file.write('(%.1f%%) \\n' % papers_subset['pred_val'][i])\r\n","        except:\r\n","            md_file.write('%s\\n' % papers_subset['pred_val'][i])\r\n","        md_file.write('\\n')\r\n","\r\n","md_file.close()\r\n","print('Literature Update Exported as Markdown')\r\n","print('Location:', mdname)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Literature Update Exported as Markdown\n","Location: /content/gdrive/My Drive/Biomech_Lit_Up/literature_update/Literature_Updates/2020-12-16-litupdate.md\n"],"name":"stdout"}]}]}