{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wearable_papers.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XJG5shp08CGC"},"source":["Let's find some wearables papers and merge them into the biomchBERT dataset..."]},{"cell_type":"code","metadata":{"id":"6klLdde-78nI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608501132715,"user_tz":420,"elapsed":291,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"7f0a364c-9fcf-4369-c7bb-90cdb1188b27"},"source":["# Install & load libraries\n","\n","try:\n","  from Bio import Entrez\n","except:\n","  !pip install -q -U biopython\n","  from Bio import Entrez\n","import pandas as pd\n","import numpy as np\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","import string\n","from bs4 import BeautifulSoup\n","\n","from google.colab import drive\n","import datetime as dt\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gScJyjTbHjtA","executionInfo":{"status":"ok","timestamp":1608502699498,"user_tz":420,"elapsed":422,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}}},"source":["# read in pubmed search results\r\n","# search term: (sensor[Title/Abstract] OR wearable[Title/Abstract] NOT robot*[Title/Abstract]) AND (biomech*[Title/Abstract] OR locomot*[Title/Abstract])\r\n","\r\n","pubmed_in = pd.read_csv('/content/gdrive/My Drive/literature_update/Data/csv-sensorTitl-set.csv')\r\n","\r\n","id_list = pubmed_in['PMID']"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaSczx4IuQ0Y","executionInfo":{"status":"ok","timestamp":1608502709590,"user_tz":420,"elapsed":8468,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"a9b5f90e-394a-4773-aba7-027854394ab3"},"source":["\n","# Perform Search and Pull Paper Titles ----\n","def fetch_details(ids):\n","    Entrez.email = 'your.email@example.com'\n","    handle = Entrez.efetch(db='pubmed',\n","                           retmode='xml',\n","                           id=ids)\n","    results = Entrez.read(handle)\n","    return results\n","\n","\n","# Make the stop words for string cleaning ----\n","def clean_str(text, stops):\n","    text = BeautifulSoup(text, 'lxml').text\n","    text = text.split()\n","    return ' '.join([word for word in text if word not in stops])\n","\n","stop = list(stopwords.words('english'))\n","stop_c = [string.capwords(word) for word in stop]\n","for word in stop_c:\n","    stop.append(word)\n","\n","new_stop = ['The', 'An', 'A', 'Do', 'Is', 'In', 'StringElement', \n","            'NlmCategory', 'Label', 'attributes', 'INTRODUCTION',\n","            'METHODS', 'BACKGROUND', 'RESULTS', 'CONCLUSIONS']\n","for s in new_stop:\n","    stop.append(s)\n","\n","papers = fetch_details(id_list)\n","print(len(papers['PubmedArticle']), 'Papers found')\n","\n","titles, full_titles, keywords, authors, links, journals, abstracts = ([] for i in range(7))\n","\n","for paper in papers['PubmedArticle']:\n","    # clean and store titles, abstracts, and links\n","    t = paper['MedlineCitation']['Article']['ArticleTitle']\n","    titles.append(t)\n","    full_titles.append(paper['MedlineCitation']['Article']['ArticleTitle'])\n","    pmid = paper['MedlineCitation']['PMID']\n","    links.append('[URL=\"https://www.ncbi.nlm.nih.gov/pubmed/{0}\"]{1}[/URL]'.format(pmid, t))\n","    try:\n","        abstracts.append(paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0])  # rm brackets that survived beautifulsoup, sentence case\n","    except:\n","        abstracts.append('')\n","\n","    # clean and store authors\n","    auths = []\n","    try:\n","        for auth in paper['MedlineCitation']['Article']['AuthorList']:\n","            try:  # see if there is a last name and initials\n","                auth_name = [auth['LastName'], auth['Initials'] + ',']\n","                auth_name = ' '.join(auth_name)\n","                auths.append(auth_name)\n","            except:\n","                if 'LastName' in auth.keys():  # maybe they don't have initials\n","                    auths.append(auth['LastName'] + ',')\n","                else:  # no last name\n","                    auths.append('')\n","                    print(paper['MedlineCitation']['Article']['ArticleTitle'],\n","                          'has an issue with an author name:')\n","        auths[-1] = auths[-1][0:-1]  # remove comma after last author\n","    except:\n","        auths.append('AUTHOR NAMES ERROR')\n","        print(paper['MedlineCitation']['Article']['ArticleTitle'], 'has no author list?')\n","    # compile authors\n","    authors.append(\"['\" + ' '.join(auths).replace('[','').replace(']','') + \"']\")  # rm brackets in names\n","    # journal names\n","    journals.append(paper['MedlineCitation']['Article']['Journal']['Title'].replace('[','').replace(']',''))  # rm brackets\n","\n","    # store keywords \n","    if paper['MedlineCitation']['KeywordList'] != []:\n","        kwds = []\n","        for kw in paper['MedlineCitation']['KeywordList'][0]:\n","            kwds.append(kw[:])\n","        keywords.append(', '.join(kwds).lower())\n","    else:\n","      keywords.append('')\n","\n","# Put Titles, Abstracts, Authors, Journal, and Keywords into dataframe\n","papers_df = pd.DataFrame({'title': titles,\n","                          'keywords': keywords,\n","                          'abstract': abstracts,\n","                          'authors': authors,\n","                          'journal': journals,\n","                          'links': links,\n","                          'raw_title': full_titles,\n","                          'mindate': \"2020/12/12\",\n","                          'maxdate': \"2010/1/1\"})\n","\n","\n","# remove papers with no title or no authors\n","for index, row in papers_df.iterrows():\n","    if row['title'] == '' or row['authors'] == 'AUTHOR NAMES ERROR':\n","        papers_df.drop(index, inplace=True)\n","papers_df.reset_index(drop=True, inplace=True)\n","\n","# join titles and abstract\n","papers_df['BERT_input'] = pd.DataFrame(papers_df['title'] + ' ' + papers_df['abstract'])\n","\n"],"execution_count":62,"outputs":[{"output_type":"stream","text":["1122 Papers found\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dxc6N4oiJXIC","executionInfo":{"status":"ok","timestamp":1608502739197,"user_tz":420,"elapsed":14620,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}}},"source":["# read in old biomch-L papers\r\n","\r\n","## read in data ----\r\n","df = pd.read_csv('/content/gdrive/My Drive/literature_update/Data/Biomch-L_papers.csv', encoding='UTF-8-SIG')\r\n","df.columns = ['X','topic_split', 'topic', 'authors','title','journal','year','vol_issue','doi','abstract']\r\n","\r\n","# keep just topics where there are at least 900 entries:\r\n","df = df.groupby('topic').filter(lambda x: len(x) > 900)\r\n","# remove unique topics\r\n","df.drop(df[df['topic'] == 'UNIQUETOPIC'].index, inplace=True)\r\n","\r\n","# total number of topics\r\n","n_topics = len(df.groupby('topic').size())\r\n","\r\n","\r\n","df['title'] = df['title'].replace(np.nan, '', regex = True)\r\n","df['abstract'] = df['abstract'].replace(np.nan, '', regex = True)\r\n","df.drop(['X', 'topic_split', 'authors', 'journal', 'year', 'vol_issue', 'doi'], axis=1, inplace=True)\r\n","df.head()\r\n","df.sort_index(inplace=True)\r\n","\r\n","## clean up title and abstract text ----\r\n","#set cleaning parameters\r\n","def clean(t):\r\n","    t = t.split()\r\n","    return ' '.join([(i) for (i) in t if i not in stop])\r\n","\r\n","stop = list(stopwords.words('english'))\r\n","stop.append('The')\r\n","stop.append('An')\r\n","stop.append('A')\r\n","stop.append('Do')\r\n","stop.append('Is')\r\n","stop.append('In')\r\n","\r\n","new_stop = ['StringElement','NlmCategory','Label','attributes','INTRODUCTION',\r\n","            'METHODS','BACKGROUND','RESULTS','CONCLUSIONS']\r\n","for item in new_stop:\r\n","    stop.append(item)\r\n","\r\n","#apply cleaning to title and abstract text\r\n","df['title'] = df['title'].apply(clean)\r\n","df['abstract'] = df['abstract'].apply(clean)\r\n","old_papers = df.copy()\r\n","\r\n","wearable_papers = papers_df.copy()\r\n","wearable_papers['title'] = wearable_papers['title'].apply(clean)\r\n","wearable_papers['abstract'] = wearable_papers['abstract'].apply(clean)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYFn43IXJ-DR","executionInfo":{"status":"ok","timestamp":1608502739198,"user_tz":420,"elapsed":12423,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"8cc0235e-3e35-4cbf-b0df-e5bd68db77d5"},"source":["print(wearable_papers.columns)\r\n","print(old_papers.columns)"],"execution_count":64,"outputs":[{"output_type":"stream","text":["Index(['title', 'keywords', 'abstract', 'authors', 'journal', 'links',\n","       'raw_title', 'mindate', 'maxdate', 'BERT_input'],\n","      dtype='object')\n","Index(['topic', 'title', 'abstract'], dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Deqf5q7BdTAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608502739867,"user_tz":420,"elapsed":659,"user":{"displayName":"Ryan Alcantara","photoUrl":"","userId":"11016774030491622791"}},"outputId":"b8722e30-a954-4558-e239-5370fc4ccd50"},"source":["# Double check that none of these papers were included in past literature updates ----\n","old_papers.dropna(subset=['title'], inplace=True)\n","old_papers.reset_index(drop=True, inplace=True)\n","\n","match = wearable_papers['title'].isin(old_papers['title'])  # boolean for matching titles between this week and prior papers\n","\n","wearable_papers.drop(wearable_papers[match].index, inplace=True)\n","wearable_papers.reset_index(drop=True, inplace=True)\n","\n","print(wearable_papers.shape)\n","wearable_papers.to_csv('/content/gdrive/My Drive/literature_update/Data/wearable_papers.csv', index=False)\n","# then the top 700 were taken and filtered by hand. about 500 were kept. "],"execution_count":65,"outputs":[{"output_type":"stream","text":["(1119, 10)\n"],"name":"stdout"}]}]}